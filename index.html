<!DOCTYPE html>
<html lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">

    <!--[if lt IE 9]>
        <style>body {display: none; background: none !important} </style>
        <meta http-equiv="Refresh" Content="0; url=//outdatedbrowser.com/" />
    <![endif]-->

<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
<meta name="format-detection" content="telephone=no">
<meta name="author" content="HanderH">


    
    


<meta name="description" content="学习总结　思考感悟　知识管理">
<meta name="keywords" content="java 大数据　IT">
<meta property="og:type" content="website">
<meta property="og:title" content="hsj的博客">
<meta property="og:url" content="http://www.handerh.top/index.html">
<meta property="og:site_name" content="hsj的博客">
<meta property="og:description" content="学习总结　思考感悟　知识管理">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hsj的博客">
<meta name="twitter:description" content="学习总结　思考感悟　知识管理">

<link rel="apple-touch-icon" href="/apple-touch-icon.png">


    <link rel="alternate" href="/atom.xml" title="hsj的博客" type="application/atom+xml">



    <link rel="shortcut icon" href="/favicon.png">



    <link href="//cdn.bootcss.com/animate.css/3.5.1/animate.min.css" rel="stylesheet">



    <link href="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" rel="stylesheet">



    <script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
    <link href="//cdn.bootcss.com/pace/1.0.2/themes/blue/pace-theme-minimal.css" rel="stylesheet">


<link rel="stylesheet" href="/css/style.css">


    <style> .article { opacity: 0;} </style>


<link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">


<title>hsj的博客</title>

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>
<script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

<script>
    var yiliaConfig = {
        fancybox: true,
        animate: true,
        isHome: true,
        isPost: false,
        isArchive: false,
        isTag: false,
        isCategory: false,
        fancybox_js: "//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js",
        scrollreveal: "//cdn.bootcss.com/scrollReveal.js/3.1.4/scrollreveal.min.js",
        search: true
    }
</script>


    <script> yiliaConfig.jquery_ui = [false]; </script>



    <script> yiliaConfig.rootUrl = "\/";</script>




  
<script type="text/javascript">
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?4f798eed1a766f92447c385d4a7be8eb";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>



</head></html>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        <a href="/" class="profilepic">
            <img src="/img/handerh.jpg" class="animated zoomIn">
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">HanderH</a></h1>
        </hgroup>

        
        <p class="header-subtitle">life and work</p>
        

        
            <form id="search-form">
            <input type="text" id="local-search-input" name="q" placeholder="search..." class="search form-control" autocomplete="off" autocorrect="off" searchonload="false" />
            <i class="fa fa-times" onclick="resetSearch()"></i>
            </form>
            <div id="local-search-result"></div>
            <p class='no-result'>No results found <i class='fa fa-spinner fa-pulse'></i></p>
        


        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                    
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        

        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a href="/">主页</a></li>
                        
                            <li><a href="/archives/">所有文章</a></li>
                        
                            <li><a href="/tags/">标签云</a></li>
                        
                            <li><a href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" href="mailto:husj0423@163.com" title="Email"></a>
                            
                                <a class="fa GitHub" href="https://github.com/HanderH/" title="GitHub"></a>
                            
                                <a class="fa RSS" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" href="https://user.qzone.qq.com/1290449261" title="QQ"></a>
                            
                        </ul>
                    </nav>
                </section>
                
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Flume/">Flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java多线程/">java多线程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java设计模式/">java设计模式</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/jvm/">jvm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/ubuntu-linux/">ubuntu linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据结构与算法/">数据结构与算法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/项目总结/">项目总结</a></li></ul>
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a class="main-nav-link switch-friends-link" href="https://hexo.io">Hexo</a>
                    
                      <a class="main-nav-link switch-friends-link" href="https://pages.github.com/">GitHub</a>
                    
                      <a class="main-nav-link switch-friends-link" href="http://moxfive.xyz/">MOxFIVE</a>
                    
                    </div>
                </section>
                

                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">大数据的时代</div>
                </section>
                
            </div>
        </div>
    </header>                
</div>
    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">HanderH</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                <img src="/img/handerh.jpg" class="animated zoomIn">
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">HanderH</a></h1>
            </hgroup>
            
            <p class="header-subtitle">life and work</p>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/">主页</a></li>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/tags/">标签云</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fa Email" target="_blank" href="mailto:husj0423@163.com" title="Email"></a>
                            
                                <a class="fa GitHub" target="_blank" href="https://github.com/HanderH/" title="GitHub"></a>
                            
                                <a class="fa RSS" target="_blank" href="/atom.xml" title="RSS"></a>
                            
                                <a class="fa QQ" target="_blank" href="https://user.qzone.qq.com/1290449261" title="QQ"></a>
                            
                        </ul>
            </nav>
        </header>                
    </div>
    <link class="menu-list" tags="标签" friends="友情链接" about="关于我"/>
</nav>
      <div class="body-wrap">
  
    <article id="post-kafka学习笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/08/13/kafka学习笔记/" class="article-date">
      <time datetime="2019-08-13T06:35:24.000Z" itemprop="datePublished">2019-08-13</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/13/kafka学习笔记/">kafka学习笔记</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">1.7k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">8分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h3 id="kafka的安装"><a href="#kafka的安装" class="headerlink" title="kafka的安装"></a>kafka的安装</h3>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/kafka/">kafka</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2019/08/13/kafka学习笔记/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-Spark-Streaming" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/08/12/Spark-Streaming/" class="article-date">
      <time datetime="2019-08-12T07:56:09.000Z" itemprop="datePublished">2019-08-12</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/12/Spark-Streaming/">Spark Streaming</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">1.2k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">6分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>Spark Streaming基于流式数据的处理，具有高吞吐量和容错能力能力强的特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等。和Spark Core类似，Spark Streaming也有一种抽象的数据集合，叫做DStream。DStream是随时间推移而收到的数据序列，每个时间区间收到的数据都作为RDD存在。DStream提供了两种操作，一种是转化操作，一种的输出操作</p>
<h3 id="Kafka数据源"><a href="#Kafka数据源" class="headerlink" title="Kafka数据源"></a>Kafka数据源</h3><p>Spark Streaming读取kafka数据源有两种方式</p>
<p>一种是Receiver,这种方法使用一个 Receiver 来接收数据。在该 Receiver 的实现中使用了 Kafka high-level consumer API。Receiver 从 kafka 接收的数据将被存储到 Spark executor 中，随后启动的 job 将处理这些数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Logger.getLogger(&quot;org.apache.spark&quot;).setLevel(Level.ERROR)</span><br><span class="line">    Logger.getLogger(&quot;org.eclipse.jetty.server&quot;).setLevel(Level.OFF)</span><br><span class="line">    val conf = new SparkConf().setAppName(&quot;SparkFlumeNGWordCount&quot;).setMaster(&quot;local[2]&quot;)</span><br><span class="line">    val ssc = new StreamingContext(conf, Seconds(5))</span><br><span class="line">    ssc.checkpoint(&quot;hdfs://hadoop03:9000/checkpoint&quot;)</span><br><span class="line">    //创建kafka对象   生产者 和消费者 </span><br><span class="line">    //模式1 采取的是 receiver 方式  reciver 每次只能读取一条记录</span><br><span class="line">    val topic = Map(&quot;mydemo2&quot; -&gt; 1)</span><br><span class="line">    //直接读取的方式  由于kafka 是分布式消息系统需要依赖Zookeeper</span><br><span class="line">    val data = KafkaUtils.createStream(ssc, &quot;hadoop03:2181&quot;, &quot;mygroup&quot;, topic, StorageLevel.MEMORY_AND_DISK)</span><br><span class="line">    //数据累计计算</span><br><span class="line">    val updateFunc =(curVal:Seq[Int],preVal:Option[Int])=&gt;&#123;</span><br><span class="line">    //进行数据统计当前值加上之前的值</span><br><span class="line">    var total = curVal.sum</span><br><span class="line">    //最初的值应该是0</span><br><span class="line">    var previous = preVal.getOrElse(0)</span><br><span class="line">    //Some 代表最终的返回值</span><br><span class="line">    Some(total+previous)</span><br><span class="line">  &#125;</span><br><span class="line">   val result = data.map(_._2).flatMap(_.split(&quot; &quot;)).map(word=&gt;(word,1)).updateStateByKey(updateFunc).print()</span><br><span class="line">   //启动ssc</span><br><span class="line">   ssc.start()</span><br><span class="line">   ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>一种是不需要 Receiver 的方法。替代了使用 receivers 来接收数据，该方法定期查询每个 topic+partition 的 lastest offset，并据此决定每个 batch 要接收的 offsets 范围。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">val conf = new SparkConf().setAppName(&quot;kafka&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line">  val ssc =  new StreamingContext(conf,Seconds(5))</span><br><span class="line">  //kafka集群</span><br><span class="line">  val brokers = &quot;hadoop03:9092&quot;</span><br><span class="line">  // 订阅主题</span><br><span class="line">  val sourceTopic = &quot;source&quot;</span><br><span class="line">  // 消费者组</span><br><span class="line">  val consumerGroup = &quot;Test&quot;</span><br><span class="line">  // 配置参数</span><br><span class="line">  val kafkaParams = Map[String,String](</span><br><span class="line">    ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -&gt; brokers,</span><br><span class="line">    ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -&gt; &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;,</span><br><span class="line">    ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -&gt; &quot;org.apache.kafka.common.serialization.StringDeserializer&quot;,</span><br><span class="line">    ConsumerConfig.GROUP_ID_CONFIG -&gt; consumerGroup</span><br><span class="line">  )</span><br><span class="line">  //从kafka中读取数据</span><br><span class="line">  val kfaKaStreaming: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String,String,StringDecoder,StringDecoder](ssc,kafkaParams,Set(sourceTopic))</span><br><span class="line">  kfaKaStreaming.print()</span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<h3 id="Dstream有状态转换"><a href="#Dstream有状态转换" class="headerlink" title="Dstream有状态转换"></a>Dstream有状态转换</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">	val conf = new SparkConf().setAppName(&quot;test&quot;).setMaster(&quot;local[*]&quot;)</span><br><span class="line"></span><br><span class="line">    val ssc = new StreamingContext(conf,Milliseconds(5000))</span><br><span class="line">    //设置保存数据的目录</span><br><span class="line">    ssc.checkpoint(&quot;./check&quot;)</span><br><span class="line">    val lineStream: DStream[String] = ssc.textFileStream(&quot;/home/husj/test/&quot;)</span><br><span class="line">    val m = lineStream.flatMap(line=&gt;&#123;line.split(&quot; &quot;)&#125;).map(x =&gt;(x,1))</span><br><span class="line">    val updataFun = (values:Seq[Int],state:Option[Int])=&gt;&#123;</span><br><span class="line">      val v  = state.getOrElse(0)</span><br><span class="line">      val sum = values.sum</span><br><span class="line">      Some(v+sum)</span><br><span class="line">    &#125;</span><br><span class="line">    val res: DStream[(String, Int)] = m.updateStateByKey(updataFun)</span><br><span class="line">//  val res = m.reduceByKey(_+_).print()</span><br><span class="line">    res.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<h3 id="流式DataFrame"><a href="#流式DataFrame" class="headerlink" title="流式DataFrame"></a>流式DataFrame</h3><p>数据源:socket file kafka</p>
<ol>
<li>socket</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">object MassageServer &#123;</span><br><span class="line"></span><br><span class="line">  // 定义随机获取整数的方法</span><br><span class="line">  def index(length: Int) = &#123;</span><br><span class="line">    import java.util.Random</span><br><span class="line">    val rdm = new Random</span><br><span class="line">    rdm.nextInt(length)</span><br><span class="line">  &#125;</span><br><span class="line">  def main(args: Array[String]) &#123;</span><br><span class="line">    println(&quot;模拟数据器启动！！！&quot;)</span><br><span class="line">    // 获取指定文件总的行数</span><br><span class="line">    val filename =&quot;/home/husj/software/spark/examples/src/main/resources/people.txt&quot;</span><br><span class="line">    val lines = Source.fromFile(filename).getLines.toList</span><br><span class="line">    val filerow = lines.length</span><br><span class="line">    // 指定监听某端口，当外部程序请求时建立连接</span><br><span class="line">    val serversocket = new ServerSocket(9999)</span><br><span class="line">    while (true) &#123;</span><br><span class="line">      //监听9999端口，获取socket对象</span><br><span class="line">      val socket = serversocket.accept()</span><br><span class="line">      //      println(socket)</span><br><span class="line">      new Thread() &#123;</span><br><span class="line">        override def run = &#123;</span><br><span class="line">          println(&quot;Got client co nnected from: &quot; + socket.getInetAddress)</span><br><span class="line">          val out = new PrintWriter(socket.getOutputStream(), true)</span><br><span class="line">          while (true) &#123;</span><br><span class="line">            Thread.sleep(1000)</span><br><span class="line">            // 当该端口接受请求时，随机获取某行数据发送给对方</span><br><span class="line">            val content = lines(index(filerow))</span><br><span class="line">            println (content)</span><br><span class="line">            out.write(content + &apos;\n&apos;)</span><br><span class="line">            out.flush()</span><br><span class="line">          &#125;</span><br><span class="line">          socket.close()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.start()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">val spark = 					SparkSession.builder().appName(&quot;socket&quot;).master(&quot;local[*]&quot;).getOrCreate()</span><br><span class="line"></span><br><span class="line">import spark.implicits._</span><br><span class="line"></span><br><span class="line">val df: DataFrame = spark.readStream.format(&quot;socket&quot;).option(&quot;host&quot;,&quot;127.0.0.1&quot;).</span><br><span class="line">  option(&quot;port&quot;,9999).option(&quot;includeTimestamp&quot;,false).load()</span><br><span class="line">val dds: Dataset[(String, Int)] = df.flatMap(row =&gt; row.getAs[String](0).split(&quot; &quot;)).map(x =&gt;(x,1))</span><br><span class="line"></span><br><span class="line">val res: DataFrame = dds.toDF(&quot;word&quot;,&quot;num&quot;).groupBy(&quot;word&quot;).count()</span><br><span class="line">   </span><br><span class="line">val sq =  res.writeStream.outputMode(&quot;Complete&quot;).format(&quot;console&quot;).start()</span><br><span class="line"></span><br><span class="line">sq.awaitTermination()</span><br><span class="line"></span><br><span class="line">spark.close()</span><br></pre></td></tr></table></figure>

<p>参数说明:option(“includeTimestamp”,false),是否加载时间戳 </p>
<p>​        outmode:输出模式,有三种,Append,Complete,Update</p>
<p>​        Append:支持查询但是不支持aggregate运算</p>
<p>​        Complete:支持查询并且支持aggregate运算</p>
<p>​        Update:某些更新的行将会被写出</p>
<ol start="2">
<li><p>file:普通文件没什么好说的,主要是json格式的数据,spark streaming默认不识别json中的结构信息</p>
<p>,需要自己指定结构信息,或者开启自动识别json信息的选项</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> val spark = SparkSession.builder().appName(&quot;file&quot;).master(&quot;local[*]&quot;).config(&quot;spark.sql.streaming.schemaInference&quot;,true)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    import spark.implicits._</span><br><span class="line">//    val df = spark.readStream.format(&quot;text&quot;).option(&quot;path&quot;,&quot;/home/husj/test&quot;).load()</span><br><span class="line">//    val struct = new StructType().add(&quot;properties&quot;,new StructType().add(&quot;ip&quot;,&quot;string&quot;).add(&quot;port&quot;,&quot;int&quot;).add(&quot;httpMethod&quot;,&quot;string&quot;))</span><br><span class="line">    val df = spark.readStream.format(&quot;json&quot;).option(&quot;multiline&quot;,true).option(&quot;path&quot;,&quot;spark-sql/jsonFile&quot;).load()</span><br><span class="line">    val query = df.writeStream.format(&quot;console&quot;).option(&quot;truncate&quot;,false).start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">    spark.close()</span><br></pre></td></tr></table></figure>

<p>​            </p>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/spark/">spark</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-spark-sql" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/08/09/spark-sql/" class="article-date">
      <time datetime="2019-08-09T12:49:49.000Z" itemprop="datePublished">2019-08-09</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/09/spark-sql/">spark sql</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">2k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">9分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="spark-sql的概念"><a href="#spark-sql的概念" class="headerlink" title="spark sql的概念"></a>spark sql的概念</h3><p>spark sql是 是spark用来处理结构化数据的一个模块，提供了DataFrame和DataSet两个新的抽象概念。与RDD类似，DataSet和DataFrame都是分布式数据容器，DataFrame更像穿透数据库二维表格，除了记录数据之外，还记录数据的结构信息。DataFrame是Row对象的集合，而DataSet是DataFrame的一个扩展，DataSet每一个record存储的是一个强类型值而不是一个Row。RDD，DataSet和DataFram的共同点：三者都是分布式的弹性数据集，三者都有惰性机制，都有partition机制，有许多共同函数。三者可以相互转化</p>
<h3 id="DataFrame的创建"><a href="#DataFrame的创建" class="headerlink" title="DataFrame的创建"></a>DataFrame的创建</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">val spark = SparkSession.builder().appName(&quot;test1&quot;).master(&quot;local[*]&quot;).getOrCreate()</span><br><span class="line">//1. 从文件中读取</span><br><span class="line">val df = spark.read.json(&quot;people.json&quot;)</span><br><span class="line"></span><br><span class="line">//2. createDataFrame[A &lt;: Product : TypeTag](data: Seq[A])</span><br><span class="line">import  spark.implicits</span><br><span class="line">// A &lt;: Product  :case class 或者 元组</span><br><span class="line">val df = spark.createDataFrame[Persion](Seq(Persion(&quot;zs&quot;,18),Persion(&quot;lisi&quot;,20)))</span><br><span class="line">// 需要一个样例类</span><br><span class="line">case class Persion(name:String,age:Int)</span><br><span class="line"></span><br><span class="line">//3.createDataFrame(rowRDD: RDD[Row], schema: StructType)</span><br><span class="line">val rdd: RDD[Row] =  sc.parallelize(Seq(Row(&quot;zs&quot;,19),Row(&quot;lisi&quot;,14)))</span><br><span class="line">val schemal = StructType(Seq(StructField(&quot;name&quot;,StringType),StructField(&quot;age&quot;,IntegerType)))</span><br><span class="line">val df = spark.createDataFrame(rdd,schemal)</span><br></pre></td></tr></table></figure>

<h3 id="RDD，DataSet和DatDaFram互相转换"><a href="#RDD，DataSet和DatDaFram互相转换" class="headerlink" title="RDD，DataSet和DatDaFram互相转换"></a>RDD，DataSet和DatDaFram互相转换</h3><ul>
<li>RDD =》DataFrame</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">case class People(var name:String,var age:Int)</span><br><span class="line"></span><br><span class="line">val rdd = spark.sparkContext.textFile(&quot;people.txt&quot;)</span><br><span class="line">val rdd2: RDD[People] = rdd.map(line =&gt; (People(line.split(&quot;,&quot;)(0),line.split(&quot;,&quot;)(1).trim.toInt)))</span><br><span class="line">val rdd3:RDD[(String,Int)] = rdd.map(line =&gt; ((line.split(&quot;,&quot;)(0),line.split(&quot;,&quot;)(1).trim.toInt)))</span><br><span class="line">// rdd2 =&gt; df 样例类</span><br><span class="line">val dataf: DataFrame = rdd2.toDF()</span><br><span class="line">dataf.foreach(row =&gt; println(row.getString(0)+&quot; &quot;+row.getInt(1)))</span><br><span class="line">// rdd3 =&gt; df 元组</span><br><span class="line">val dataf: DataFrame = rdd3.toDF()</span><br><span class="line">dataf.foreach(row =&gt; println(row.getString(0)+&quot; &quot;+row.getInt(1)))</span><br></pre></td></tr></table></figure>

<ul>
<li>RDD =》DataSet</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 样例类 可以看出DataSet是一种强类型</span><br><span class="line">val dataS: Dataset[People] = rdd2.toDS()</span><br><span class="line">dataS.foreach(peo =&gt; println(peo.name +&quot; &quot;+peo.age))</span><br><span class="line">// 元组</span><br><span class="line">val dataS: Dataset[(String, Int)] = rdd3.toDS()</span><br><span class="line">dataS.foreach(line=&gt;(println(line._1+&quot; &quot;+line._2)))</span><br></pre></td></tr></table></figure>

<ul>
<li>DataFrame =》 RDD</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val dfRdd: RDD[Row] =  .rdd</span><br><span class="line">dfRdd.foreach(row =&gt; println(row.getString(0)+&quot;: &quot;+row.getInt(1)))</span><br></pre></td></tr></table></figure>

<ul>
<li>DataFrame =》 DataSet</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 注意：DF转DS的时候，转换到DF的类型需要与DS的类型一致，如果这里的DF是由RDD[People]转来的</span><br><span class="line">// 所以Dataset只能为Dataset[People]</span><br><span class="line">val dfDs: Dataset[People] = dataf.as[People]</span><br><span class="line">dfDs.foreach(peo =&gt; println(peo.name+&quot;: &quot;+peo.age))</span><br></pre></td></tr></table></figure>

<ul>
<li>DataSet =》 RDD</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val dsRdd: RDD[People] = dataS.rdd</span><br><span class="line">dsRdd.foreach(peo =&gt; println(peo.name+&quot;: &quot;+peo.age))</span><br></pre></td></tr></table></figure>

<ul>
<li>DataSet =》 DataFrame</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val dsDF: DataFrame = dataS.toDF()</span><br><span class="line">dsDF.foreach(row =&gt; println(row.getString(0)+&quot; &quot;+row.getInt(1)))</span><br></pre></td></tr></table></figure>

<h3 id="udf与udaf"><a href="#udf与udaf" class="headerlink" title="udf与udaf"></a>udf与udaf</h3><ul>
<li>udf:用户自定义函数，单行函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(&quot;addName&quot;,(x:String)=&gt;(&quot;name:&quot;+x))</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;select addName(name) as name from persion&quot;).show()</span><br></pre></td></tr></table></figure>

<ul>
<li>udaf：用户自定义聚集函数，多行函数，多对一</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">class MyUdaf extends UserDefinedAggregateFunction&#123;</span><br><span class="line">   //输入值的类型</span><br><span class="line">  override def inputSchema: StructType = StructType(Seq(StructField(&quot;input&quot;,DoubleType)))</span><br><span class="line">  // 缓存的值的类型</span><br><span class="line">  override def bufferSchema: StructType = StructType(Seq(StructField(&quot;sum&quot;,DoubleType),StructField(&quot;count&quot;,IntegerType)))</span><br><span class="line">  //返回值的类型</span><br><span class="line">  override def dataType: DataType = DoubleType</span><br><span class="line"></span><br><span class="line">  override def deterministic: Boolean = true</span><br><span class="line">  //初始值</span><br><span class="line">  override def initialize(buffer: MutableAggregationBuffer): Unit = &#123;</span><br><span class="line">    buffer(0) = 0.0</span><br><span class="line">    buffer(1) = 0</span><br><span class="line">  &#125;</span><br><span class="line">  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;</span><br><span class="line">    buffer(0) = buffer.getDouble(0)+input.getDouble(0)</span><br><span class="line">    buffer(1) = buffer.getInt(1)+1</span><br><span class="line">  &#125;</span><br><span class="line">  // 分区合并</span><br><span class="line">  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;</span><br><span class="line">    buffer1(0) = buffer1.getDouble(0)+buffer2.getDouble(0)</span><br><span class="line">    buffer1(1) = buffer1.getInt(1)+buffer2.getInt(1)</span><br><span class="line">  &#125;</span><br><span class="line">  // 结果</span><br><span class="line">  override def evaluate(buffer: Row): Double = &#123;</span><br><span class="line">    buffer.getDouble(0)/buffer.getInt(1)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.udf.register(&quot;mu&quot;,new MyUdaf)</span><br><span class="line">spark.sql(&quot;select mu(age) from persion&quot;).show()</span><br></pre></td></tr></table></figure>

<h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> @Test</span><br><span class="line">  def test1(): Unit =&#123;</span><br><span class="line"></span><br><span class="line">    val spark = SparkSession.builder().appName(&quot;jdbc&quot;).master(&quot;local[*]&quot;).getOrCreate()</span><br><span class="line">    val properties = new Properties()</span><br><span class="line">    properties.setProperty(&quot;user&quot;,&quot;root&quot;)</span><br><span class="line">    properties.setProperty(&quot;password&quot;,&quot;husaijian123&quot;)</span><br><span class="line">    val jdbcDF= spark.read.jdbc(&quot;jdbc:mysql://localhost:3306/interview&quot;,&quot;student&quot;,properties)</span><br><span class="line">    jdbcDF.show()</span><br><span class="line"></span><br><span class="line">    val jdbcDF2 = spark.read.format(&quot;jdbc&quot;).</span><br><span class="line"> option(&quot;url&quot;,&quot;jdbc:mysql://localhost:3306/interview&quot;).option(&quot;dbtable&quot;,&quot;student&quot;).option(&quot;user&quot;,&quot;root&quot;).option(&quot;password&quot;,&quot;husaijian123&quot;).load()</span><br><span class="line">    jdbcDF2.show()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Spark-Sql运行架构"><a href="#Spark-Sql运行架构" class="headerlink" title="Spark Sql运行架构"></a>Spark Sql运行架构</h3><p><img src="https://handerh-1259550163.cos.ap-shanghai.myqcloud.com/java/spark/spark_sql1.png" alt></p>
<ol>
<li>在解析Sql之前会创建SparkSession,SparkSession会将元数据保存到SessionCatelog,包括表名，字段名，字段类型。</li>
<li>使用SparkSqlParser进行解析SQL,构建语法树</li>
<li>使用分析器Analysiser绑定逻辑计划</li>
<li>使用优化器Optimizer优化逻辑计划</li>
<li>私用SparkPlanner生成物理计划</li>
<li>使用QueryExecution执行物理计划</li>
</ol>
<h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><p>窗口函数计算的一组行，被称为Frame。每一个被处理的行都有一个唯一的frame相关联。</p>
<p>窗口函数分为三类，ranking（排名），analystic（分析），aggregate（聚合）</p>
<table>
<thead>
<tr>
<th><strong>Function Type</strong></th>
<th><strong>SQL</strong></th>
<th><strong>DataFrame API</strong></th>
<th><strong>Description</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Ranking</td>
<td>rank</td>
<td>rank</td>
<td>rank值可能是不连续的</td>
</tr>
<tr>
<td>Ranking</td>
<td>dense_rank</td>
<td>denseRank</td>
<td>rank值一定是连续的</td>
</tr>
<tr>
<td>Ranking</td>
<td>percent_rank</td>
<td>percentRank</td>
<td>相同的分组中 (rank -1) / ( count(score) - 1 )</td>
</tr>
<tr>
<td>Ranking</td>
<td>ntile</td>
<td>ntile</td>
<td>将同一组数据循环的往n个桶中放，返回对应的桶的index，index从1开始</td>
</tr>
<tr>
<td>Ranking</td>
<td>row_number</td>
<td>rowNumber</td>
<td>很单纯的行号，类似excel的行号</td>
</tr>
<tr>
<td>Analytic</td>
<td>cume_dist</td>
<td>cumeDist</td>
<td></td>
</tr>
<tr>
<td>Analytic</td>
<td>first_value</td>
<td>firstValue</td>
<td>相同的分组中第一个值</td>
</tr>
<tr>
<td>Analytic</td>
<td>last_value</td>
<td>lastValue</td>
<td>相同的分组中最后一个值</td>
</tr>
<tr>
<td>Analytic</td>
<td>lag</td>
<td>lag</td>
<td>取前n行数据</td>
</tr>
<tr>
<td>Analytic</td>
<td>lead</td>
<td>lead</td>
<td>取后n行数据</td>
</tr>
<tr>
<td>Aggregate</td>
<td>min</td>
<td>min</td>
<td>最小值</td>
</tr>
<tr>
<td>Aggregate</td>
<td>max</td>
<td>max</td>
<td>最大值</td>
</tr>
<tr>
<td>Aggregate</td>
<td>sum</td>
<td>sum</td>
<td>求和</td>
</tr>
<tr>
<td>Aggregate</td>
<td>avg</td>
<td>avg</td>
<td>求平均</td>
</tr>
</tbody></table>
<p>用法：函数名 over(分区 排序 范围)</p>
<p>当一个函数被窗口函数使用时，需要为该窗口函数定义相关的窗口规范,窗口规范包括三个部分</p>
<p>分区（PARTITION BY）</p>
<p>排序（order by)</p>
<p>帧规范：指定哪些行会被当前输入行的帧包括，通过其他行对于当前行的相对位置实现。定义帧规范需要定义帧的类型，开始边界，结束边界。一共有五种边界：UNBOUNDED PRECEDING(分区第一行)，UNBOUNDED FOLLOWING(分区最后一行)，CURRENT ROW，n PRECEDING(当前行的前n行）,n FOLLOWING(当前行后n行)。有两种帧类型：ROW帧和RANGE帧。</p>
<p>ROW帧是基于当前输入行的位置的物理偏移量,如：BETWEEN 1 PRECEDING AND 1 FOLLOWING表示一个包括当前行、当前行之前1行和之后1行的帧</p>
<p>RANGE帧是基于当前行位置的逻辑偏移。逻辑偏移为当前输入行的排序表达式的值和帧边界行的排序表达式的值之差。如：RANGE BETWEEN 2000 PRECEDING AND 1000 FOLLOWING，则边界为[current revenue value - 2000, current revenue value + 1000]。</p>
<ol>
<li>rank,dense_rank,percent_rank，row_number</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">## rank 排名时有重复，排序不连续，但是最大名次不变</span><br><span class="line">select id,name,classId,score, dense_rank() over(partition by classId order by score)rank from scores</span><br><span class="line"></span><br><span class="line">+---+-----+-------+-----+----+</span><br><span class="line">| id| name|classId|score|rank|</span><br><span class="line">+---+-----+-------+-----+----+</span><br><span class="line">|  3|张三3|      1| 79.0|   1|</span><br><span class="line">|  8|张三8|      1| 87.0|   2|</span><br><span class="line">|  1|张三1|      1| 88.0|   3|</span><br><span class="line">|  6|张三6|      1| 89.0|   4|</span><br><span class="line">|  4|张三4|      2| 78.0|   1|</span><br><span class="line">|  5|张三5|      2| 85.0|   2| </span><br><span class="line">|  2|张三2|      2| 85.0|   2|</span><br><span class="line">|  7|张三7|      2| 90.0|   4|</span><br><span class="line">+---+-----+-------+-----+----+</span><br><span class="line"></span><br><span class="line">## dense_rank 排名时有重复，排序连续，最大值减小</span><br><span class="line">select id,name,classId,score, dense_rank() over(partition by classId order by score)rank from scores</span><br><span class="line">+---+-----+-------+-----+----+</span><br><span class="line">| id| name|classId|score|rank|</span><br><span class="line">+---+-----+-------+-----+----+</span><br><span class="line">|  3|张三3|      1| 79.0|   1|</span><br><span class="line">|  8|张三8|      1| 87.0|   2|</span><br><span class="line">|  1|张三1|      1| 88.0|   3|</span><br><span class="line">|  6|张三6|      1| 89.0|   4|</span><br><span class="line">|  4|张三4|      2| 78.0|   1|</span><br><span class="line">|  5|张三5|      2| 85.0|   2|</span><br><span class="line">|  2|张三2|      2| 85.0|   2|</span><br><span class="line">|  7|张三7|      2| 90.0|   3|</span><br><span class="line">+---+-----+-------+-----+----+</span><br><span class="line"></span><br><span class="line">## percent_rank (rank -1) / ( count(score) - 1 )</span><br><span class="line">select id,name,classId,score, percent_rank() over(partition by classId order by score)rank from scores</span><br><span class="line">+---+-----+-------+-----+------------------+</span><br><span class="line">| id| name|classId|score|              rank|</span><br><span class="line">+---+-----+-------+-----+------------------+</span><br><span class="line">|  3|张三3|      1| 79.0|               0.0|</span><br><span class="line">|  8|张三8|      1| 87.0|0.3333333333333333|</span><br><span class="line">|  1|张三1|      1| 88.0|0.6666666666666666|</span><br><span class="line">|  6|张三6|      1| 89.0|               1.0|</span><br><span class="line">|  4|张三4|      2| 78.0|               0.0|</span><br><span class="line">|  5|张三5|      2| 85.0|0.3333333333333333|</span><br><span class="line">|  2|张三2|      2| 85.0|0.3333333333333333|</span><br><span class="line">|  7|张三7|      2| 90.0|               1.0|</span><br><span class="line">+---+-----+-------+-----+------------------+</span><br><span class="line">## row_number 按顺序排序 无重复排名</span><br><span class="line">select id,name,classId,score, row_number() over(partition by classId order by score)rank from scores</span><br><span class="line">---+-----+-------+-----+----+</span><br><span class="line">| id| name|classId|score|rank|</span><br><span class="line">+---+-----+-------+-----+----+</span><br><span class="line">|  3|张三3|      1| 79.0|   1|</span><br><span class="line">|  8|张三8|      1| 87.0|   2|</span><br><span class="line">|  1|张三1|      1| 88.0|   3|</span><br><span class="line">|  6|张三6|      1| 89.0|   4|</span><br><span class="line">|  4|张三4|      2| 78.0|   1|</span><br><span class="line">|  5|张三5|      2| 85.0|   2|</span><br><span class="line">|  2|张三2|      2| 85.0|   3|</span><br><span class="line">|  7|张三7|      2| 90.0|   4|</span><br><span class="line">+---+-----+-------+-----+----+</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>aggregate函数统计的结果在排序后的结果是不一样的</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">avg(score) over(partition by `) avg # 每个分组加起来求平均值</span><br><span class="line">avg(score) over(partition by classId order by score) # r1=r1 r2=(r1+r2)/2 ...</span><br><span class="line">avg(score) over(partition by classId  rows between 1 preceding and 1 following) avg_row  #当前行的前一行+当前行+后一行的平均值</span><br><span class="line">avg(score) over(partition by classId order by score range between 1 preceding and 1 following # score+(是否存在score+或-1)的平均值</span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/spark/">spark</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-spark之RDD操作" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/08/07/spark之RDD操作/" class="article-date">
      <time datetime="2019-08-07T07:44:39.000Z" itemprop="datePublished">2019-08-07</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/07/spark之RDD操作/">spark之RDD操作</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">2.5k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">12分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="1-RDD的基本概念"><a href="#1-RDD的基本概念" class="headerlink" title="1. RDD的基本概念"></a>1. RDD的基本概念</h3><p>RDD是弹性分布式数据集,是spark中最基本的数据抽象，它代表一个可变，可分区的数据集合。在spark中对数据的所有操作包括创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。</p>
<h4 id="RDD的属性"><a href="#RDD的属性" class="headerlink" title="RDD的属性"></a>RDD的属性</h4><ol>
<li>Partitions:一组分片，即数据集的基本组成单位.每个分片会被一个任务运行</li>
<li>Compute：分区计算函数</li>
<li>Dependencies:RDD的每次转换操作都会生成依赖关系，在部分分区数据丢失时，spark可以根据依赖关系恢复数据</li>
<li>Partitioner:RDD的分区方式，默认的是HashPartitoner,可以实现Partitioner自定义分区方式</li>
<li>preferred location:存储每个Partition位置的列表。</li>
</ol>
<h4 id="RDD的弹性"><a href="#RDD的弹性" class="headerlink" title="RDD的弹性"></a>RDD的弹性</h4><ol>
<li><p>自动进行内存和磁盘数据存储的切换</p>
<p>Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换</p>
</li>
<li><p>基于血统的高效容错机制</p>
<p>在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</p>
</li>
<li><p>Task失败进行特定次数的重试</p>
<p>RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</p>
</li>
<li><p>Stage如果失败会自动进行特定次数的重试</p>
<p>  如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次</p>
</li>
<li><p>Checkpoint和Persist可主动或被动触发</p>
</li>
</ol>
<h3 id="２-RDD编程"><a href="#２-RDD编程" class="headerlink" title="２.RDD编程"></a>２.RDD编程</h3><h4 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h4><ol>
<li>从集合中创建</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))</span><br><span class="line">这种方式默认分区个数使用的是　taskScheduler.defaultParallelism</span><br><span class="line">val rdd1 = sc.ｍakeRDD(Array(1,2,3,4,5,6,7,8))　// 调用的是上面的函数</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>从文件中创建</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> val rdd1  = sc.textFile(&quot;files/test&quot;)//math.min(defaultParallelism, 2)默认分区数</span><br><span class="line">// 获取的是每个文件的内容 key为文件名 value为文件内容</span><br><span class="line"> val rdd2: RDD[(String, String)] = sc.wholeTextFiles(&quot;files/test&quot;)</span><br></pre></td></tr></table></figure>

<h4 id="RDD的转换"><a href="#RDD的转换" class="headerlink" title="RDD的转换"></a>RDD的转换</h4><ol>
<li>mapPartitions:类似map,但是是基于分区执行的，有几个分区执行几次</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;))</span><br><span class="line">// mapPartition需要接受一个函数：f: Iterator[(String, String)] =&gt; Iterator[U]</span><br><span class="line">res10.mapPartitions(iter =&gt;Iterator(iter.mkString(&quot;|&quot;))).collect</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>glom：将每个分区中中的数据形成一个数组，形成一个数组类型的RDD</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd1 = sc.parallelize(1 to 10,4)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.glom.collect</span><br><span class="line">res0: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4, 5), Array(6, 7), Array(8, 9, 10))</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.glom</span><br><span class="line">res1: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[2] at glom at &lt;console&gt;:26</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>mapPartitionsWithIndex:比mapPartitions多了一个分区号</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">rdd1.mapPartitionsWithIndex((x,y)=&gt;Iterator(x+&quot;:&quot;+y.mkString(&quot;|&quot;))）</span><br><span class="line">res4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at mapPartitionsWithIndex at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; res4.collect</span><br><span class="line">res5: Array[String] = Array(0:(kpop,female), 1:(zorro,male)|(mobin,male))</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>sample(withReplacement: Boolean, fraction: Double,seed: Long = Utils.random.nextLong)</p>
<p>withReplacement表示抽出数据是否返回，fraction比例，seed随机的种子</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; rdd.sample(true,0.1,4).collect</span><br><span class="line">res10: Array[Int] = Array(4, 6)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sample(true,0.2,4).collect</span><br><span class="line">res11: Array[Int] = Array(3, 5, 6)</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>partitionBy:分区，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;),(4,&quot;ddd&quot;)),4)</span><br><span class="line">scala&gt; rdd.partitionBy(new org.apache.spark.HashPartitioner(2))</span><br><span class="line">res0: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[1] at partitionBy at &lt;console&gt;:26</span><br><span class="line"></span><br><span class="line">scala&gt; res0.collect</span><br><span class="line">res1: Array[(Int, String)] = Array((2,bbb), (4,ddd), (1,aaa), (3,ccc))</span><br><span class="line"></span><br><span class="line">可以看到产生了ShuffledRDD，并且有四个分区变为两个</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>coalesce：根据分区数，重新进行分区，默认不进行shuffle</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(1 to 16,4)</span><br><span class="line">rdd.coalesce(3)</span><br><span class="line"></span><br><span class="line">scala&gt; res3.partitions.size  </span><br><span class="line">res6: Int = 3 </span><br><span class="line">分区个数变为三</span><br></pre></td></tr></table></figure>

<ol start="7">
<li>repartition:底层调用coalesce(numPartitions, shuffle = true)，重分区使用shuffle</li>
<li>sortBy:先用函数对数据进行处理，再排序</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scala&gt;  val rdd = sc.parallelize(List(1,2,3,4))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.sortBy(x=&gt;x%2)collect</span><br><span class="line">warning: there was one feature warning; re-run with -feature for details</span><br><span class="line">res0: Array[Int] = Array(2, 4, 1, 3)</span><br></pre></td></tr></table></figure>

<ol start="9">
<li>join:在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.join(rdd1).collect()</span><br><span class="line">res13: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6)))</span><br></pre></td></tr></table></figure>

<ol start="10">
<li>cogroup:在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable<v>,Iterable<w>))类型的RDD</w></v></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cogroup(rdd1).collect()</span><br><span class="line">res14: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6))))</span><br></pre></td></tr></table></figure>

<ol start="11">
<li>reduceByKey</li>
</ol>
<p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(List((&quot;female&quot;,1),(&quot;male&quot;,5),(&quot;female&quot;,5),(&quot;male&quot;,2)))</span><br><span class="line">rdd.reduceByKey(_+_).collect</span><br><span class="line">res6: Array[(String, Int)] = Array((female,6), (male,7))</span><br></pre></td></tr></table></figure>

<p>12.groupByKey</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.groupByKey() // 生成一个RDD[k,Iterator[t]]</span><br></pre></td></tr></table></figure>

<ol start="13">
<li><p>combineByKey( createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C,  mergeCombiners: (C, C) =&gt; C) createCombiner:分区内第一次碰到key，创建key对应C的初始值</p>
<p>mergeValue：第二次碰到key，将当前值与C中的值进行合并，分区内合并</p>
<p>mergeCombiners：由于每个分区是独立的，需要合并每个分区</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val scores: Array[(String, Int)] = Array((&quot;Fred&quot;,88), (&quot;Fred&quot;,95), (&quot;Fred&quot;,91), (&quot;Wilma&quot;,93), (&quot;Wilma&quot;,95), (&quot;Wilma&quot;,98))</span><br><span class="line">val input = sc.parallelize(scores)</span><br><span class="line">input.combineByKey(v=&gt;(v,1),(a:(Int,Int),b)=&gt;(a._1+b,a._2+1),(x:(Int,Int),y:(Int,Int))=&gt;(x._1+y._1,x._2+y._2))</span><br><span class="line"></span><br><span class="line">结果为 Array((Wilma,(286,3)), (Fred,(274,3)))</span><br></pre></td></tr></table></figure>

<ol start="13">
<li><p>aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U) </p>
<p>zeroValue：分区内的每个key对应的初始值U</p>
<p>seqOp：分区内的U的合并操作</p>
<p>combOp：每个分区的合并操作</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val scores: Array[(String, Int)] = Array((&quot;Fred&quot;,88), (&quot;Fred&quot;,95), (&quot;Fred&quot;,91), (&quot;Wilma&quot;,93), (&quot;Wilma&quot;,95), (&quot;Wilma&quot;,98))</span><br><span class="line">val input = sc.parallelize(scores)</span><br><span class="line">input.aggregateByKey((0,0))((a:(Int,Int),v:Int)=&gt;(a._1+v,a._2+1),(x,y)=&gt;(x._1+y._1,x._2+x._2))</span><br></pre></td></tr></table></figure>

<ol start="14">
<li>foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)] </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)</span><br><span class="line">dd.foldByKey(0)(_+_)</span><br><span class="line">// 结果</span><br><span class="line">res0.collect</span><br><span class="line">res1: Array[(Int, Int)] = Array((3,14), (1,9), (2,3))</span><br></pre></td></tr></table></figure>

<ol start="15">
<li>sortByKey:在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd = sc.parallelize(Array((3,&quot;aa&quot;),(6,&quot;cc&quot;),(2,&quot;bb&quot;),(1,&quot;dd&quot;)))</span><br><span class="line">rdd.sortByKey()</span><br></pre></td></tr></table></figure>

<ol start="16">
<li>mapValues:针对于(K,V)形式的类型只对V进行操作 </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))</span><br><span class="line">rdd3.mapValues(a =&gt; a*2).collect</span><br><span class="line">结果： Array((1,aa), (1,dd), (2,bb), (3,cc))</span><br></pre></td></tr></table></figure>

<h4 id="RDD执行操作"><a href="#RDD执行操作" class="headerlink" title="RDD执行操作"></a>RDD执行操作</h4><ol>
<li>reduce（func)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1.reduce(_+_)</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>collect():在驱动程序中，以数组的形式返回数据集的所有元素</li>
<li>count():返回RDD的元素个数</li>
<li>first():返回RDD的第一个元素（类似于take(1)）</li>
<li>take(n):返回一个由数据集的前n个元素组成的数组</li>
<li>takeOrdered(n):返回前几个的排序</li>
<li>aggregate(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(1)(</span><br><span class="line">     | &#123;(x : Int,y : Int) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : Int,b : Int) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res56: Int = 58</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(1)(</span><br><span class="line">     | &#123;(x : Int,y : Int) =&gt; x * y&#125;,</span><br><span class="line">     | &#123;(a : Int,b : Int) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res57: Int = 30361</span><br></pre></td></tr></table></figure>

<ol start="8">
<li>folder(num)(func):折叠操作，aggregate的简化操作，seqop和combop一样。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; var rdd1 = sc.makeRDD(1 to 4,2)</span><br><span class="line">rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[90] at makeRDD at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.aggregate(1)(</span><br><span class="line">     | &#123;(x : Int,y : Int) =&gt; x + y&#125;,</span><br><span class="line">     | &#123;(a : Int,b : Int) =&gt; a + b&#125;</span><br><span class="line">     | )</span><br><span class="line">res59: Int = 13</span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.fold(1)(_+_)</span><br><span class="line">res60: Int = 13</span><br></pre></td></tr></table></figure>

<ol start="9">
<li><p>saveAsTextFile(path)</p>
<p>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</p>
</li>
<li><p>saveAsSequenceFile(path)</p>
<p>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统</p>
</li>
<li><p>saveAsObjectFile(path)</p>
<p>用于将RDD中的元素序列化成对象，存储到文件中。</p>
</li>
<li><p>countByKey:针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)</span><br><span class="line">rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at parallelize at &lt;console&gt;:24</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.countByKey()</span><br><span class="line">res63: scala.collection.Map[Int,Long] = Map(3 -&gt; 2, 1 -&gt; 3, 2 -&gt; 1)</span><br></pre></td></tr></table></figure>

<ol start="13">
<li>foreach</li>
</ol>
<h4 id="RDD的文件操作"><a href="#RDD的文件操作" class="headerlink" title="RDD的文件操作"></a>RDD的文件操作</h4><ul>
<li>saveAsObject File:底层调用其实是saveAsSequenceFile</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val rdd =  sc.parallelize(Array((&quot;zs&quot;,18),(&quot;lisi&quot;,20),(&quot;ww&quot;,17),(&quot;zl&quot;,30)))</span><br><span class="line"></span><br><span class="line">rdd.saveAsObjectFile(&quot;write_dile&quot;)</span><br><span class="line"></span><br><span class="line">val rdd2: RDD[(String, Int)] = sc.objectFile[(String,Int)](&quot;write_dile&quot;)</span><br></pre></td></tr></table></figure>

<ul>
<li>saveAsSequenceFile</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">al rdd =  sc.parallelize(Array((&quot;zs&quot;,18),(&quot;lisi&quot;,20),(&quot;ww&quot;,17),(&quot;zl&quot;,30)))</span><br><span class="line"></span><br><span class="line">rdd.saveAsSequenceFile(&quot;write_dile&quot;)</span><br><span class="line"></span><br><span class="line">val rdd2: RDD[(String, Int)] = sc.sequenceFile[String,Int](&quot;write_dile&quot;,classOf[String],classOf[Int])</span><br></pre></td></tr></table></figure>

<ul>
<li>hdfs</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">val rdd =  sc.parallelize(Array((&quot;zs&quot;,18),(&quot;lisi&quot;,20),(&quot;ww&quot;,17),(&quot;zl&quot;,30)))</span><br><span class="line">  rdd.saveAsNewAPIHadoopFile(&quot;write_dile&quot;,classOf[Text],classOf[Text],classOf[TextOutputFormat[Text,Text]])</span><br><span class="line">  </span><br><span class="line">val rdd2 = sc.newAPIHadoopFile[Text,Text,KeyValueTextInputFormat](&quot;write_dile&quot;)</span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/spark/">spark</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-spark基本架构与运行原理" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/08/05/spark基本架构与运行原理/" class="article-date">
      <time datetime="2019-08-05T06:36:50.000Z" itemprop="datePublished">2019-08-05</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/05/spark基本架构与运行原理/">spark基本架构与运行原理</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">1.1k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">4分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="spark运行架构图"><a href="#spark运行架构图" class="headerlink" title="spark运行架构图"></a>spark运行架构图</h3><p><img src="https://handerh-1259550163.cos.ap-shanghai.myqcloud.com/java/spark/spark1.png" alt></p>
<ul>
<li>Cluster Manager：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为ResourceManager</li>
<li>Worker:从节点，负责控制计算节点，启动executor。在yarn集群中为nodemanager</li>
<li>Driver:运行Application的main函数并且创建SparkContext.</li>
<li>Executor:执行器,Application运行在worker节点上的一个进程，负责启动线程池运行任务(Task)</li>
<li>RDD DAG:RDD组成的有向无环图，反应RDD之间的依赖关系。当RDD遇到遇到Action算子的时候，将之前所有算子形成一个有向无环图</li>
<li>DAG Scheduler：根据job构造基于stage的DAG并将stage提交给TaskScheduler</li>
<li>TaskScheduler:将task任务分发给Executor执行</li>
<li>SparkEnv:线程级别的上下文，存储运行时中要组件的引用</li>
</ul>
<p><img src="https://handerh-1259550163.cos.ap-shanghai.myqcloud.com/java/spark/rdd.jpg" alt></p>
<h3 id="Spark作业基本概念"><a href="#Spark作业基本概念" class="headerlink" title="Spark作业基本概念"></a>Spark作业基本概念</h3><ul>
<li>Application:用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码</li>
<li>Driver Program:运行Application的main函数并且创建SparkContext</li>
<li>Job:一个RDD Graph 触发的作业，包含多个Task组成的并行计算</li>
<li>Stage:每个job会根据RDD的宽依赖关系切分成多个stage,每个stage包含一组相同的Task,也叫TaskSet</li>
<li>Task：被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责</li>
</ul>
<h3 id="Spark运行机制"><a href="#Spark运行机制" class="headerlink" title="Spark运行机制"></a>Spark运行机制</h3><h4 id="spark运行流程"><a href="#spark运行流程" class="headerlink" title="spark运行流程"></a>spark运行流程</h4><p><img src="https://handerh-1259550163.cos.ap-shanghai.myqcloud.com/java/spark/spark3.png" alt></p>
<ol>
<li>用户在客户端将Application提交到ClusterManager</li>
<li>ClusterManager收到Application后，找一个worker启动Driver,初始化一个SparkContext.</li>
<li>SparkContext向资源管理器申请Executors,并启动StandaloneExecutorBackend</li>
<li>Executors向SparkContext申请Task,SparkContext中的DAG Scheduler会根据依赖关系拆分成多个stage,每个stage提交给一个Task Scheduler，Task Scheduler会将这些Task任务分发到Executor</li>
<li>Executor启动多线程，执行Task。</li>
<li>所有Task完成后，SparkContext向Master注销</li>
</ol>
<h3 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h3><ul>
<li>Standalone:Standalone模式使用Spark自带的资源调度框架,采用Master/Slaves的典型架构</li>
</ul>
<ol>
<li>用户在客户端将Application提交到ClusterManager</li>
<li>ClusterManager收到Application后，找一个worker启动Driver,初始化一个SparkContext.</li>
<li>SparkContext初始化时创建DAG Scheduler和Task Scheduler,向资源管理器申请Executors,并启动StandaloneExecutorBackend</li>
<li>Executors向SparkContext申请Task,SparkContext中的每一个DAG Scheduler会根据依赖关系拆分成多个stage,每个stage提交给一个Task Scheduler，Task Scheduler会将这些Task任务分发到Executor</li>
<li>Executor启动多线程，执行Task。</li>
<li>所有Task完成后，SparkContext向Master注销</li>
</ol>
<ul>
<li>Yarn Client:</li>
</ul>
<ol>
<li>Driver在本地提交任务的机器运行,Driver启动后向ResourceManager申请ApplicationMaster</li>
<li>ResourceManager找到一个NodeManager分配conainer,并且创建ApplicationMaster</li>
<li>AM创建后,向ResourceManager申请启动Executor,Executor启动后向Driver注册.</li>
<li>Driver注册完成后,Driver开始执行main方法,执行算子时,执行一个job,每个job会根据宽依赖划分为多个stage,每个stage,每个stage对应多个TaskSet,经过TaskSchedule分发到Executor执行.</li>
</ol>
<ul>
<li>Yarn Cluster</li>
</ul>
<ol>
<li>任务提交后,向ResourceManager申请ApplcationMaster</li>
<li>ResourceManager找到一个NodeManager分配conainer,并且创建ApplicationMaster,AM创建后,启动Driver,此时的AM就是一个Driver</li>
<li>AM向ResourceManager申请启动Executor,Executor启动后向Driver注册</li>
<li>Driver注册完成后,Driver开始执行main方法,执行算子时,执行一个job,每个job会根据宽依赖划分为多个stage,每个stage,每个stage对应多个TaskSet,经过TaskSchedule分发到Executor执行.</li>
</ol>
<h3 id="Spark-Shuffle阶段"><a href="#Spark-Shuffle阶段" class="headerlink" title="Spark Shuffle阶段"></a>Spark Shuffle阶段</h3><p>Spark Shuffle分为Map和Reduce阶段,它与stage阶段的划分有关,在一个job中如果碰到宽依赖,会被划分为多个stage,最终每个stage都会被划分为ResultStage和ShuffleMapStage,对应ReduceTask和ShuffleMapTask.ShuffleMapTask会根据partitioner对数据进行分组,并且进行持久化.</p>
<ul>
<li><p>未优化HashShuffle</p>
<p>HashShuffle在没有优化之前,每一个ShuffleMapTask会为每一个ReduceTask创建一个bucket(经过partitioner后对用的bucket),并且为每一个bucket创建一个文件.ReduceTask会去对应bucket中获取数据.但是这样有很大缺点就是生成的文件数量太多,这对于磁盘IO影响很大</p>
</li>
</ul>
<p><img src alt></p>
<ul>
<li><p>优化后的HashShuffle:</p>
<p>设置参数 spark.shuffle.consolidateFiles=true,在中间过程生成的文件数为cpu核数*reduceTask个数</p>
</li>
<li><p>Sorted-Based Shuffle:</p>
<ol>
<li><p>SortShuffleWriter运行机制:类似hadoop的shuffle过程,在ShuffleMapTask阶段,每个Task将所有数据写到一个文件中,并且生成一个索引文件.然后在由ReduceTask去文件中拉取对应分区的数据.</p>
</li>
<li><p>bypass运行机制:用于处理数据量小不需要排序和聚合的Shuffle操作</p>
<p>触发条件:1. shuffle map task的数量小于200(spark.shuffle.sort.bypassMergeThreshold )</p>
<p>​        2. 不是聚合类的shuffle算子</p>
</li>
</ol>
</li>
</ul>

      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/spark/">spark</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-hadoop优化配置" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/08/04/hadoop优化配置/" class="article-date">
      <time datetime="2019-08-04T07:08:41.000Z" itemprop="datePublished">2019-08-04</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/04/hadoop优化配置/">hadoop优化配置</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">864字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">4分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        <h3 id="1-历史服务器的配置"><a href="#1-历史服务器的配置" class="headerlink" title="1. 历史服务器的配置"></a>1. 历史服务器的配置</h3><p>为了查看程序的历史运行情况，需要配置一下历史服务器。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> vi mapred-site.xml</span><br><span class="line"> &lt;!-- 历史服务器端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">&lt;value&gt;hadoop01:10020&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 历史服务器web端地址 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop01:19888&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>启动历史服务器</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<p>查看JobHistory</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://hadoop01:19888/jobhistory</span><br></pre></td></tr></table></figure>

<h3 id="2-日志聚集"><a href="#2-日志聚集" class="headerlink" title="2.日志聚集"></a>2.日志聚集</h3><p>应用运行完成以后，将程序运行日志信息上传到HDFS系统上，可以方便的查看到程序运行详情，方便开发调试。开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vi yarn-site.xml</span><br><span class="line"></span><br><span class="line">&lt;!-- 日志聚集功能使能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">&lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 日志保留时间设置7天 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">&lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<h3 id="3-默认的配置文件"><a href="#3-默认的配置文件" class="headerlink" title="3.默认的配置文件"></a>3.默认的配置文件</h3><table>
<thead>
<tr>
<th>要获取的默认文件</th>
<th>文件存放在Hadoop的jar包中的位置</th>
</tr>
</thead>
<tbody><tr>
<td>[core-default.xml]</td>
<td>hadoop-common-2.7.2.jar/ core-default.xml</td>
</tr>
<tr>
<td>[hdfs-default.xml]</td>
<td>hadoop-hdfs-2.7.2.jar/ hdfs-default.xml</td>
</tr>
<tr>
<td>[yarn-default.xml]</td>
<td>hadoop-yarn-common-2.7.2.jar/ yarn-default.xml</td>
</tr>
<tr>
<td>[mapred-default.xml]</td>
<td>hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml</td>
</tr>
</tbody></table>
<h3 id="4-CheckPoint时间设置"><a href="#4-CheckPoint时间设置" class="headerlink" title="4.CheckPoint时间设置"></a>4.CheckPoint时间设置</h3><p>通常情况下，SecondaryNameNode每隔一小时执行一次。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hdfs-default.xml]</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3600&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p>一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1000000&lt;/value&gt;</span><br><span class="line">&lt;description&gt;操作动作次数&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;60&lt;/value&gt;</span><br><span class="line">&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;</span><br><span class="line">&lt;/property &gt;</span><br></pre></td></tr></table></figure>

<h3 id="5-namenode故障处理"><a href="#5-namenode故障处理" class="headerlink" title="5.namenode故障处理"></a>5.namenode故障处理</h3><p> 将SecondaryNameNode中数据拷贝到NameNode存储数据的目录；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// 1. 删除NameNode存储的数据</span><br><span class="line">rm -rf /opt/module/hadoop/data/tmp/dfs/name/*</span><br><span class="line">// 2. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录</span><br><span class="line">scp -r atguigu@hadoop104:/opt/module/hadoop/data/tmp/dfs/namesecondary/* ./name/</span><br></pre></td></tr></table></figure>

<h3 id="6-服役新节点"><a href="#6-服役新节点" class="headerlink" title="6.服役新节点"></a>6.服役新节点</h3><ol>
<li>克隆一台主机，配置ip和hostname</li>
<li>删除克隆后的主机的一些数据</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opt/module/hadoop/data和log）</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>启动datanode</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">yarn-daemin.sh start nodemanager</span><br><span class="line"></span><br><span class="line"># 如果数据不平衡 可以使用命令实现集群的再平衡</span><br><span class="line">start-balancer.sh</span><br></pre></td></tr></table></figure>

<h3 id="7-配置白名单"><a href="#7-配置白名单" class="headerlink" title="7.配置白名单"></a>7.配置白名单</h3><ol>
<li>在namenode的hadoop/etc/hadoop目录下新建dfs.hosts文件,添加主机名,添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop01</span><br><span class="line">hadoop02</span><br><span class="line">hadoop03</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>在hdfs-site.xml配置文件中添加dfs.hosts属性</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/module/hadoop/etc/hadoop/dfs.hosts&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>刷新NameNode</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>更新ResourceManager节点</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<h3 id="8-配置黑名单"><a href="#8-配置黑名单" class="headerlink" title="8.配置黑名单"></a>8.配置黑名单</h3><ol>
<li>在namenode的hadoop/etc/hadoop目录下新建dfs.hosts.exclude文件，添加要退役主机名称</li>
<li>在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">	&lt;name&gt;dfs.hosts.exclude&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>刷新namenode,ResourceManager</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>等到退役节点的数据块成功复制到其它节点，停止该节点</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh stop datanode</span><br><span class="line">yarn-daemon.sh stop nodemanager</span><br></pre></td></tr></table></figure>


      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/hadoop/">hadoop</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/">hadoop</a></li></ul>
    </div>

      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-异常笔记" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/08/02/异常笔记/" class="article-date">
      <time datetime="2019-08-02T06:28:17.000Z" itemprop="datePublished">2019-08-02</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/08/02/异常笔记/">异常笔记</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">0字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">1分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
        
      
    </div>
    
    <div class="article-info article-info-index">
      
      

      
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-mysql基础" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/07/29/mysql基础/" class="article-date">
      <time datetime="2019-07-29T11:12:14.000Z" itemprop="datePublished">2019-07-29</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/29/mysql基础/">mysql</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">2.6k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">11分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h2 id="1-mysql常见函数"><a href="#1-mysql常见函数" class="headerlink" title="1.mysql常见函数"></a>1.mysql常见函数</h2><h3 id="字符函数"><a href="#字符函数" class="headerlink" title="字符函数"></a>字符函数</h3>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/mysql/">mysql</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mysql/">mysql</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2019/07/29/mysql基础/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-redis" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/07/28/redis/" class="article-date">
      <time datetime="2019-07-28T10:51:31.000Z" itemprop="datePublished">2019-07-28</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/28/redis/">redis</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">2.4k字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">9分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <h2 id="一-概述"><a href="#一-概述" class="headerlink" title="一.概述"></a>一.概述</h2><p>redis基于内存持久化，高性能的NoSql的key-value数据库.redis支持数据的持久化，可以将数据保存在磁盘中，重启时可以加载使用。redis持久化策略包括RDB,AOF。redis提供key-value类型的数据，同时还提供list,set,zset.hash类型的数据，redis单个key可存储512M大小，默认端口6379</p>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/redis/">redis</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/redis/">redis</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2019/07/28/redis/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
    <article id="post-通信分析项目总结" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      
<a href="/2019/07/24/通信分析项目总结/" class="article-date">
      <time datetime="2019-07-24T14:10:43.000Z" itemprop="datePublished">2019-07-24</time>
</a>


    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
       
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/07/24/通信分析项目总结/">通信分析项目总结</a>
    </h1>
  

          <!--
            
                 <div style="margin-top:10px;">
    <span class="post-time">
      <span class="post-meta-item-icon">
        <i class="fa fa-keyboard-o"></i>
        <span class="post-meta-item-text">  字数统计: </span>
        <span class="post-count">848字</span>
      </span>
    </span>

    <span class="post-time">
      &nbsp; | &nbsp;
      <span class="post-meta-item-icon">
        <i class="fa fa-hourglass-half"></i>
        <span class="post-meta-item-text">  阅读时长: </span>
        <span class="post-count">3分</span>
      </span>
    </span>
</div>

            
           -->
       </header>
        
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
              <p>该项目是基于通话日志的分析，当前每时每刻都有很多人打电话，那么产生的通话记录也是一个非常大数据量，需求就是根据这个通话日志，统计出每天，每月，每年用户的通话次数以及通话时长。</p>
          
      
    </div>
    
    <div class="article-info article-info-index">
      
      
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/项目总结/">项目总结</a>
    </div>


      
    <div class="article-tag tagcloud">
        <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/项目总结/">项目总结</a></li></ul>
    </div>

      
        <p class="article-more-link">
          <a href="/2019/07/24/通信分析项目总结/#more">more >></a>
        </p>
      
      <div class="clearfix"></div>
    </div>
    
  </div>
  
</article>









  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">Next &raquo;</a>
    </nav>
  
</div>
      <footer id="footer">
    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                <i class="fa fa-copyright"></i> 
                2019 HanderH
            </div>
            <div class="footer-right">
                <a href="http://hexo.io/" target="_blank" title="快速、简洁且高效的博客框架">Hexo</a>  Theme <a href="https://github.com/MOxFIVE/hexo-theme-yelee" target="_blank" title="简而不减 Hexo 双栏博客主题  v3.5">Yelee</a> by MOxFIVE <i class="fa fa-heart animated infinite pulse"></i>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" title="本站到访数"><i class="fa fa-user" aria-hidden="true"></i><span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>| </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit"  title="本页阅读量"><i class="fa fa-eye animated infinite pulse" aria-hidden="true"></i><span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
<div>
<span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("02/14/2018 12:49:00");//此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>
</footer>
    </div>
    
<script data-main="/js/main.js" src="//cdn.bootcss.com/require.js/2.2.0/require.min.js"></script>

    <script>
        $(document).ready(function() {
            var iPad = window.navigator.userAgent.indexOf('iPad');
            if (iPad > -1 || $(".left-col").css("display") === "none") {
                var bgColorList = ["#9db3f4", "#414141", "#e5a859", "#f5dfc6", "#c084a0", "#847e72", "#cd8390", "#996731"];
                var bgColor = Math.ceil(Math.random() * (bgColorList.length - 1));
                $("body").css({"background-color": bgColorList[bgColor], "background-size": "cover"});
            }
            else {
                var backgroundnum = 5;
                var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
                $("body").css({"background": backgroundimg, "background-attachment": "fixed", "background-size": "cover"});
            }
        })
    </script>


    

     




<div class="scroll" id="scroll">
    <a href="#" title="返回顶部"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments" onclick="load$hide();" title="查看评论"><i class="fa fa-comments-o"></i></a>
    <a href="#footer" title="转到底部"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    // Open in New Window
    
        var oOpenInNew = {
            
            
            
            
            
            
             archives: ".archive-article-title", 
             miniArchives: "a.post-list-link", 
            
             friends: "#js-friends a", 
             socail: ".social a" 
        }
        for (var x in oOpenInNew) {
            $(oOpenInNew[x]).attr("target", "_blank");
        }
    
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
  </div>
</body>
</html>