<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spark之RDD操作]]></title>
    <url>%2F2019%2F08%2F07%2Fspark%E4%B9%8BRDD%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. RDD的基本概念RDD是弹性分布式数据集,是spark中最基本的数据抽象，它代表一个可变，可分区的数据集合。在spark中对数据的所有操作包括创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。 RDD的属性 Partitions:一组分片，即数据集的基本组成单位.每个分片会被一个任务运行 Compute：分区计算函数 Dependencies:RDD的每次转换操作都会生成依赖关系，在部分分区数据丢失时，spark可以根据依赖关系恢复数据 Partitioner:RDD的分区方式，默认的是HashPartitoner,可以实现Partitioner自定义分区方式 preferred location:存储每个Partition位置的列表。 RDD的弹性 自动进行内存和磁盘数据存储的切换 Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换 基于血统的高效容错机制 在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。 Task失败进行特定次数的重试 RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。 Stage如果失败会自动进行特定次数的重试 如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次 Checkpoint和Persist可主动或被动触发 ２.RDD编程RDD创建 从集合中创建 123val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8))这种方式默认分区个数使用的是 taskScheduler.defaultParallelismval rdd1 = sc.ｍakeRDD(Array(1,2,3,4,5,6,7,8)) // 调用的是上面的函数 从文件中创建 123 val rdd1 = sc.textFile(&quot;files/test&quot;)//math.min(defaultParallelism, 2)默认分区数// 获取的是每个文件的内容 key为文件名 value为文件内容 val rdd2: RDD[(String, String)] = sc.wholeTextFiles(&quot;files/test&quot;) RDD的转换 mapPartitions:类似map,但是是基于分区执行的，有几个分区执行几次 123sc.parallelize(List((&quot;kpop&quot;,&quot;female&quot;),(&quot;zorro&quot;,&quot;male&quot;),(&quot;mobin&quot;,&quot;male&quot;))// mapPartition需要接受一个函数：f: Iterator[(String, String)] =&gt; Iterator[U]res10.mapPartitions(iter =&gt;Iterator(iter.mkString(&quot;|&quot;))).collect glom：将每个分区中中的数据形成一个数组，形成一个数组类型的RDD 12345678scala&gt; val rdd1 = sc.parallelize(1 to 10,4)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; rdd1.glom.collectres0: Array[Array[Int]] = Array(Array(1, 2), Array(3, 4, 5), Array(6, 7), Array(8, 9, 10))scala&gt; rdd1.glomres1: org.apache.spark.rdd.RDD[Array[Int]] = MapPartitionsRDD[2] at glom at &lt;console&gt;:26 mapPartitionsWithIndex:比mapPartitions多了一个分区号 12345rdd1.mapPartitionsWithIndex((x,y)=&gt;Iterator(x+&quot;:&quot;+y.mkString(&quot;|&quot;))）res4: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[1] at mapPartitionsWithIndex at &lt;console&gt;:26scala&gt; res4.collectres5: Array[String] = Array(0:(kpop,female), 1:(zorro,male)|(mobin,male)) sample(withReplacement: Boolean, fraction: Double,seed: Long = Utils.random.nextLong) withReplacement表示抽出数据是否返回，fraction比例，seed随机的种子 12345scala&gt; rdd.sample(true,0.1,4).collectres10: Array[Int] = Array(4, 6)scala&gt; rdd.sample(true,0.2,4).collectres11: Array[Int] = Array(3, 5, 6) partitionBy:分区，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。 12345678val rdd = sc.parallelize(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;),(4,&quot;ddd&quot;)),4)scala&gt; rdd.partitionBy(new org.apache.spark.HashPartitioner(2))res0: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[1] at partitionBy at &lt;console&gt;:26scala&gt; res0.collectres1: Array[(Int, String)] = Array((2,bbb), (4,ddd), (1,aaa), (3,ccc))可以看到产生了ShuffledRDD，并且有四个分区变为两个 coalesce：根据分区数，重新进行分区，默认不进行shuffle 123456val rdd = sc.parallelize(1 to 16,4)rdd.coalesce(3)scala&gt; res3.partitions.size res6: Int = 3 分区个数变为三 repartition:底层调用coalesce(numPartitions, shuffle = true)，重分区使用shuffle sortBy:先用函数对数据进行处理，再排序 123456scala&gt; val rdd = sc.parallelize(List(1,2,3,4))rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:24scala&gt; rdd.sortBy(x=&gt;x%2)collectwarning: there was one feature warning; re-run with -feature for detailsres0: Array[Int] = Array(2, 4, 1, 3) join:在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD 12345678scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[32] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[33] at parallelize at &lt;console&gt;:24scala&gt; rdd.join(rdd1).collect()res13: Array[(Int, (String, Int))] = Array((1,(a,4)), (2,(b,5)), (3,(c,6))) cogroup:在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD 12345678scala&gt; val rdd = sc.parallelize(Array((1,&quot;a&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))rdd: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[37] at parallelize at &lt;console&gt;:24scala&gt; val rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))rdd1: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[38] at parallelize at &lt;console&gt;:24scala&gt; rdd.cogroup(rdd1).collect()res14: Array[(Int, (Iterable[String], Iterable[Int]))] = Array((1,(CompactBuffer(a),CompactBuffer(4))), (2,(CompactBuffer(b),CompactBuffer(5))), (3,(CompactBuffer(c),CompactBuffer(6)))) reduceByKey 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。 123val rdd = sc.parallelize(List((&quot;female&quot;,1),(&quot;male&quot;,5),(&quot;female&quot;,5),(&quot;male&quot;,2)))rdd.reduceByKey(_+_).collectres6: Array[(String, Int)] = Array((female,6), (male,7)) 12.groupByKey 1rdd.groupByKey() // 生成一个RDD[k,Iterator[t]] combineByKey( createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C) createCombiner:分区内第一次碰到key，创建key对应C的初始值 mergeValue：第二次碰到key，将当前值与C中的值进行合并，分区内合并 mergeCombiners：由于每个分区是独立的，需要合并每个分区 12345val scores: Array[(String, Int)] = Array((&quot;Fred&quot;,88), (&quot;Fred&quot;,95), (&quot;Fred&quot;,91), (&quot;Wilma&quot;,93), (&quot;Wilma&quot;,95), (&quot;Wilma&quot;,98))val input = sc.parallelize(scores)input.combineByKey(v=&gt;(v,1),(a:(Int,Int),b)=&gt;(a._1+b,a._2+1),(x:(Int,Int),y:(Int,Int))=&gt;(x._1+y._1,x._2+y._2))结果为 Array((Wilma,(286,3)), (Fred,(274,3))) aggregateByKey(zeroValue:U,[partitioner: Partitioner]) (seqOp: (U, V) =&gt; U,combOp: (U, U) =&gt; U) zeroValue：分区内的每个key对应的初始值U seqOp：分区内的U的合并操作 combOp：每个分区的合并操作 123val scores: Array[(String, Int)] = Array((&quot;Fred&quot;,88), (&quot;Fred&quot;,95), (&quot;Fred&quot;,91), (&quot;Wilma&quot;,93), (&quot;Wilma&quot;,95), (&quot;Wilma&quot;,98))val input = sc.parallelize(scores)input.aggregateByKey((0,0))((a:(Int,Int),v:Int)=&gt;(a._1+v,a._2+1),(x,y)=&gt;(x._1+y._1,x._2+x._2)) foldByKey(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)] 12345val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)dd.foldByKey(0)(_+_)// 结果res0.collectres1: Array[(Int, Int)] = Array((3,14), (1,9), (2,3)) sortByKey:在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD 12val rdd = sc.parallelize(Array((3,&quot;aa&quot;),(6,&quot;cc&quot;),(2,&quot;bb&quot;),(1,&quot;dd&quot;)))rdd.sortByKey() mapValues:针对于(K,V)形式的类型只对V进行操作 123val rdd3 = sc.parallelize(Array((1,&quot;a&quot;),(1,&quot;d&quot;),(2,&quot;b&quot;),(3,&quot;c&quot;)))rdd3.mapValues(a =&gt; a*2).collect结果： Array((1,aa), (1,dd), (2,bb), (3,cc)) RDD执行操作 reduce（func) 12val rdd1 = sc.makeRDD(1 to 10,2)rdd1.reduce(_+_) collect():在驱动程序中，以数组的形式返回数据集的所有元素 count():返回RDD的元素个数 first():返回RDD的第一个元素（类似于take(1)） take(n):返回一个由数据集的前n个元素组成的数组 takeOrdered(n):返回前几个的排序 aggregate(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U) 1234567891011121314scala&gt; var rdd1 = sc.makeRDD(1 to 10,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x + y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res56: Int = 58scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x * y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res57: Int = 30361 folder(num)(func):折叠操作，aggregate的简化操作，seqop和combop一样。 1234567891011scala&gt; var rdd1 = sc.makeRDD(1 to 4,2)rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[90] at makeRDD at &lt;console&gt;:24scala&gt; rdd1.aggregate(1)( | &#123;(x : Int,y : Int) =&gt; x + y&#125;, | &#123;(a : Int,b : Int) =&gt; a + b&#125; | )res59: Int = 13scala&gt; rdd1.fold(1)(_+_)res60: Int = 13 saveAsTextFile(path) 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 saveAsSequenceFile(path) 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统 saveAsObjectFile(path) 用于将RDD中的元素序列化成对象，存储到文件中。 countByKey:针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 12345scala&gt; val rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)rdd: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[95] at parallelize at &lt;console&gt;:24scala&gt; rdd.countByKey()res63: scala.collection.Map[Int,Long] = Map(3 -&gt; 2, 1 -&gt; 3, 2 -&gt; 1) foreach]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spark基本架构与运行原理]]></title>
    <url>%2F2019%2F08%2F05%2Fspark%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[spark运行架构图 Cluster Manager：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为ResourceManager Worker:从节点，负责控制计算节点，启动executor和Driver。在yarn集群中为nodemanager Driver:运行Application的main函数并且创建SparkContext. Executor:执行器,Application运行在worker节点上的一个进程，负责启动线程池运行任务(Task) RDD DAG:RDD组成的有向无环图，反应RDD之间的依赖关系。当RDD遇到遇到Action算子的时候，将之前所有算子形成一个有向无环图 DAG Scheduler：根据job构造基于stage的DAG并将stage提交给TaskScheduler TaskScheduler:将task任务分发给Executor执行 SparkEnv:线程级别的上下文，存储运行时中要组件的引用 Spark作业基本概念 Application:用户编写的Spark应用程序，其中包括一个Driver功能的代码和分布在集群中多个节点上运行的Executor代码 Driver Program:运行Application的main函数并且创建SparkContext Job:一个RDD Graph 触发的作业，包含多个Task组成的并行计算 Stage:每个job会根据RDD的宽依赖关系切分成多个stage,每个stage包含一组相同的Task,也叫TaskSet Task：被送到某个Executor上的工作单元，但hadoopMR中的MapTask和ReduceTask概念一样，是运行Application的基本单位，多个Task组成一个Stage，而Task的调度和管理等是由TaskScheduler负责 Spark运行机制spark运行流程 用户在客户端将Application提交到ClusterManager ClusterManager收到Application后，找一个worker启动Driver,初始化一个SparkContext. SparkContext初始化时创建DAG Scheduler和Task Scheduler,向资源管理器申请Executors,并启动StandaloneExecutorBackend Executors向SparkContext申请Task,SparkContext中的每一个DAG Scheduler会根据依赖关系拆分成多个stage,每个stage提交给一个Task Scheduler，Task Scheduler会将这些Task任务分发到Executor Executor启动多线程，执行Task。 所有Task完成后，SparkContext向Master注销 Spark运行模式 Standalone:Standalone模式使用Spark自带的资源调度框架,采用Master/Slaves的典型架构 用户在客户端将Application提交到ClusterManager ClusterManager收到Application后，找一个worker启动Driver,初始化一个SparkContext. SparkContext初始化时创建DAG Scheduler和Task Scheduler,向资源管理器申请Executors,并启动StandaloneExecutorBackend Executors向SparkContext申请Task,SparkContext中的每一个DAG Scheduler会根据依赖关系拆分成多个stage,每个stage提交给一个Task Scheduler，Task Scheduler会将这些Task任务分发到Executor Executor启动多线程，执行Task。 所有Task完成后，SparkContext向Master注销 YARN Client Driver在客户端本地运行，这种模式可以使得Spark Application和客户端进行交互，此时driver负责RDD生成、task生成和分发，向AM申请资源。 YARN Cluster：client将application提交到spark集群中就与spark集群断开联系，此时client将不会发挥其他任何作用，仅仅负责提交，此时可以将driver与Application Master是一个东西]]></content>
      <categories>
        <category>spark</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop优化配置]]></title>
    <url>%2F2019%2F08%2F04%2Fhadoop%E4%BC%98%E5%8C%96%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1. 历史服务器的配置为了查看程序的历史运行情况，需要配置一下历史服务器。 1234567891011 vi mapred-site.xml &lt;!-- 历史服务器端地址 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;&lt;value&gt;hadoop01:10020&lt;/value&gt;&lt;/property&gt;&lt;!-- 历史服务器web端地址 --&gt;&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;hadoop01:19888&lt;/value&gt;&lt;/property&gt; 启动历史服务器 1mr-jobhistory-daemon.sh start historyserver 查看JobHistory 1http://hadoop01:19888/jobhistory 2.日志聚集应用运行完成以后，将程序运行日志信息上传到HDFS系统上，可以方便的查看到程序运行详情，方便开发调试。开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager。 12345678910111213vi yarn-site.xml&lt;!-- 日志聚集功能使能 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 日志保留时间设置7天 --&gt;&lt;property&gt;&lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;&lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 3.默认的配置文件 要获取的默认文件 文件存放在Hadoop的jar包中的位置 [core-default.xml] hadoop-common-2.7.2.jar/ core-default.xml [hdfs-default.xml] hadoop-hdfs-2.7.2.jar/ hdfs-default.xml [yarn-default.xml] hadoop-yarn-common-2.7.2.jar/ yarn-default.xml [mapred-default.xml] hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml 4.CheckPoint时间设置通常情况下，SecondaryNameNode每隔一小时执行一次。 12345[hdfs-default.xml]&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.period&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; 一分钟检查一次操作次数，当操作次数达到1百万时，SecondaryNameNode执行一次。 1234567891011&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.txns&lt;/name&gt; &lt;value&gt;1000000&lt;/value&gt;&lt;description&gt;操作动作次数&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.check.period&lt;/name&gt; &lt;value&gt;60&lt;/value&gt;&lt;description&gt; 1分钟检查一次操作次数&lt;/description&gt;&lt;/property &gt; 5.namenode故障处理 将SecondaryNameNode中数据拷贝到NameNode存储数据的目录； 1234// 1. 删除NameNode存储的数据rm -rf /opt/module/hadoop/data/tmp/dfs/name/*// 2. 拷贝SecondaryNameNode中数据到原NameNode存储数据目录scp -r atguigu@hadoop104:/opt/module/hadoop/data/tmp/dfs/namesecondary/* ./name/ 6.服役新节点 克隆一台主机，配置ip和hostname 删除克隆后的主机的一些数据 1opt/module/hadoop/data和log） 启动datanode 12345hadoop-daemon.sh start datanodeyarn-daemin.sh start nodemanager# 如果数据不平衡 可以使用命令实现集群的再平衡start-balancer.sh 7.配置白名单 在namenode的hadoop/etc/hadoop目录下新建dfs.hosts文件,添加主机名,添加到白名单的主机节点，都允许访问NameNode，不在白名单的主机节点，都会被退出。 123hadoop01hadoop02hadoop03 在hdfs-site.xml配置文件中添加dfs.hosts属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/opt/module/hadoop/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 刷新NameNode 1hdfs dfsadmin -refreshNodes 更新ResourceManager节点 1yarn rmadmin -refreshNodes 8.配置黑名单 在namenode的hadoop/etc/hadoop目录下新建dfs.hosts.exclude文件，添加要退役主机名称 在NameNode的hdfs-site.xml配置文件中增加dfs.hosts.exclude属性 1234&lt;property&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/opt/module/hadoop-2.7.2/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property&gt; 刷新namenode,ResourceManager 12hdfs dfsadmin -refreshNodesyarn rmadmin -refreshNodes 等到退役节点的数据块成功复制到其它节点，停止该节点 12hadoop-daemon.sh stop datanodeyarn-daemon.sh stop nodemanager]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[异常笔记]]></title>
    <url>%2F2019%2F08%2F02%2F%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[mysql]]></title>
    <url>%2F2019%2F07%2F29%2Fmysql%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[1.mysql常见函数字符函数 12345678910select length(str) # 获取长度select concat(str1,str2) ＃连接字符串select upper(str) select lower(str)select substring(str,start,len) # 截取字符串 下标从1开始select instr(pstr,cstr) # 返回子串第一次出现的索引select trim(&apos;a&apos; from &apos;aaaaaaaaaafaafaaa&apos;)# 指定去除两边的字符select lpad(str,len,s) # 用指定的字符实现左填充 len：总长度 str+s的长度select rpad(str,len,s) # 用指定的字符实现右填充 len：总长度 str+s的长度select replace(source,sourcestr,desstr) # 替换掉source中的sourcestr 数学函数12345select round(1.65,1) # 四舍五入select ceil(1.3) # 2 向上取整 &gt;=1.3的最小整数select floor(9.9) # 向下取整 &lt;=9.9的最大整数select truncate(1.6899,2) # 截断select mod(a,b) # a-a/b*b 被除数为负，结果为负 日期函数12345678select NOW() #返回当前系统日期加时间select curdate()# 返回日期select curtime() # 返回时间select year(now())select month(now())select day(now())select str_to_date(&apos;2019-07-29&apos;,&apos;%Y-%c-%d&apos;) #将日期格式的字符串转换为日期select date_format() # 将日期转换为字符串 select date_format(now(),&apos;%y-%m-%d&apos;); 其它1234select version()select database()seelct user()show variables like &apos;%char%&apos; 流程控制函数12345678910111213141516171819# ifselect if(expr,true.false)# case函数的使用#1. switch caseselect case 要判断的字段或表达式when 常量1 then 要显示的值或语句when 常量2 then 要显示的值或语句else 要显示的值或语句endfrom table#2.多重ifselect case when expr then 要显示的值或语句when expr then 要显示的值或语句else 要显示的值或语句endfrom table limit:分页1limit offset,size # offset 要显示的条目的起始索引，从０开始，size要显示的条目个数 2.DDL库的管理1234567# 库的创建create database if not exists dbname ;# 库的修改rename database oldname to newname;alter database dbname character set gbk# 库的删除drop database if exists dbname 表的管理12345678910111213141516171819202122232425262728# 表的创建create table if not exists tname( 列名 列的类型 约束 列名 列的类型 约束 ....)# 表的修改#1. 修改列名alter table tname change column oldcol newcol dataType#2.修改列的类型或约束alter table tname modify column cloname newdataType#3.添加新列alter table tname add column newcol dataType;#4.删除列alter table tname drop column cloname#5.修改表名alter table tname rename to newtname#表的删除drop table if exists tname;#表的复制# 复制结构create table copy like tname# 结构+数据create table copy2select * from tname 常见约束 NOT NULL:字段不能为空 DEFAULT:用于保证该字段有默认值 PRIMARY KEY:主键约束，唯一不为空 UNIQUE:唯一约束，可以为空 CHECK:检查约束 [mysql中不支持] FOREIGN KEY :外键，用于限制两个表的关系 列级约束： ​ 六大约束语法都支持，但外键约束没效果 表级约束： ​ 除了非空，默认，都支持 123456789101112131415161718192021222324252627282930313233343536373839404142# 添加表时添加约束create table stu( id int primary key, name varchar(20) not null, gender char(1) check(gender=&apos;男&apos; or gender=&apos;女&apos;), seat int unique , majorid int foreign key references major(id));create table major( id int primary key, majorName varchar(20));# 添加表级约束drop table if exists stu;create table stu( id int , name varchar, gender char(1) , seat int , majorid int, constraint pk primary key(id) constraint uq unique(seat) constraint ck check(gender=&apos;男&apos; or gender=&apos;女&apos;) constraint fk_stu_major froegin key(mojorid) references major(id));# 修改表时添加约束 1alter table tname modify column colname dataType constraint# 修改表时添加约束 2alter table tname add [constraint name] 约束类型(字段名)# 修改表时删除约束# 非空约束和默认约束alter table tname modify column colname dataType；# 主键约束alter table tname drop primary key;# 唯一约束alter table tname drop index colname# 外键约束alter table tname drop foreign key colnames 3.视图一种虚拟存在的表，只保存sql逻辑，不保存查询结果，当多个地方用到同样的查询结果方便使用 123456789101112131415161718192021222324# 视图的创建create view viewNameas查询语句create view ct_view as select * from ct_call;# 视图的修改 # 1create or replace view viewNameas查询语句# 2alter view viewNameas查询语句# 视图的删除drop view viewName ...# 查看视图desc viewName;show create view viewName 4.变量系统变量：系统定义 123456789101112# 查看所有系统变量show global | [session] variables;# 查看满足条件的系统变量show global | [session] variables like &apos;%char%&apos;;# 查看指定的某个系统变量的值select @@gloable|[session].系统变量名select @@tx_isolation;# 为某个系统变量复制set gloable|[session] 系统变量名 = valueset @@gloable|[session].系统变量名 = value如果是全局级别需要加global,如果是session，可以不加，默认session 自定义变量 12345678910111213141516171819202122232425262728# 用户变量 （针对当前会话有效）# 1.声明并初始化 = 或 :=set @用户变量名=值set @用户变量名:=值 select @用户变量名:=值# 2. 赋值方式一： set @用户变量名 =值 set @用户变量名:=值 select @用户变量名:=值方式二: select 字段 into @用户变量名 from table# 3.使用（查看)select @用户变量名; #局部变量 （仅在begin end中有效)# 1.声明declare 变量名 类型;declare 变量名 类型 default value;# 2.赋值方式一： set 局部变量名 =值 set 局部变量名:=值 select @局部变量名:=值方式二: select 字段 into 局部变量名 from table# 3.使用（查看)select @用户变量名; 5.存储过程和函数存储过程一组预先编译好的sql语句集，经编译后存储在数据库，用户只需要传递给定的参数和存储过程名，就可以调用。减少了编译次数并且减少了数据库服务的连接次数，提高了效率。 12345678910111213141516171819202122232425# 创建语法create procedure 存储过程名(参数列表)begin 一组合法的sql语句end参数列表： 参数模式 参数模式 参数类型 in stuname varchar(20) 参数模式： in:该参数作为输入 out ：该参数作为返回值 inout：既可以作为输入又可以作为输出如果存储过程体只有一句话，begin end 可以省略,存储过程每天sql必须加;存储过程的结尾可以使用 delimiter 重新设置etc. delimiter 结束标志# 调用call 存储过程名(实参)# 删除drop procedure pname;&apos;# 查看show create procedure p1 123456789101112131415161718192021222324252627282930313233343536# 例子delimitet $# 无参的存储过程create procedure p1()beginselect * from ct_call;end$call p1()$#increate procedure p2(in cid int)beginselect * from ct_call where id = cid;end$call p2(1)$#outcreate procedure p3(in cid int,out sum int)beginselect sumCall into sum from ct_call where id = cid;end$call p3(1,@sum)$select @sum$#inoutcreate procedure p4(inout a int,inout b int)beginset a=a*2;set b=b*2;end$set @a = 10$set @b = 20$p4(@a,@b)$call(@a,@b)select @a$select @b$ 函数存储过程没有返回值，或者多个返回值，函数只有一个返回值 12345678910111213141516171819# 创建create function 函数名(参数列表) returns 返回值类型begin 函数体end参数列表： 参数名 参数类型函数体:肯定会有return语句，如果没有则会报错如果return没有放在函数体的最后也不报错，但不建议# 调用 select 函数名(参数列表)# 查看show create function fname;# 删除 drop function fname; 12345678910111213141516# 例子 create Function f1() returns int begin declare c int default 0; select count(1) into c from ct_call; return c; end $ select f1() create Function f2(cid int) returns int begin declare sum int default 0; select sumCal into sum from ct_call where id = cid; return sum; end $ select f2(1) 流程控制结构 分支结构 1234567891011121314151617181920212223# ifif (表达式1，表达式2，表达式3) # 如果表达式1成立，返回表达式2的值，否则返回表达式3的值if 条件1 then 语句1；elseif 条件2 then 语句2;...end if;# casecase 变量|表达式|字段when 要判断的值 then 返回值1或语句；when 要判断的值 then 返回值2或语句;...else 返回值nend case;case when 要判断的条件1 then 返回值1或语句；when 要判断的条件2 then 返回值2或语句;...else 返回值nend case; 1234567891011121314151617# 例子#ifcreate function f1(score int) returns charbegin if score&gt;80 then return &apos;A&apos;; elseif score&lt;=80 then return &apos;B&apos;; end if;end$# casecreate procedure p1(in score int)begin case when score&gt;80 then select &apos;A&apos;; else select &apos;B&apos;; end case;end $ 循环结构 三种循环while,loop,repeat， iterate类似continue,leave 类似break 123456789101112131415# while【标签：】while 循环条件 do 循环体end while 【标签】# loop【标签：】loop 循环体end loop 【标签】# repeat【标签：】repeat 循环体until 结束循环的条件end repeat【标签】 123456789101112131415161718192021222324252627282930313233# 例子# whilecreate procedure p_w(in num int)begin declare i int default 1; while i&lt;num do select * from ct_call where id = i; set i = i+1; end while;end $# loopcreate procedure p_l(in num int)begin declare i int default 1; l:loop select * from ct_call where id = i; set i = i+1; if i&gt;10 then leave l; end if; end loop l;end$# repeatcreate procedure p_p(in num int)begin declare i int default 1; repeat select * from ct_call where id = i; set i = i+1; until i&gt;10 end repeat;end$ 6.行转列与列转行12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061CREATE TABLE `TEST_TB_GRADE` ( `ID` int(10) NOT NULL AUTO_INCREMENT, `USER_NAME` varchar(20) DEFAULT NULL, `COURSE` varchar(20) DEFAULT NULL, `SCORE` float DEFAULT &apos;0&apos;, PRIMARY KEY (`ID`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;insert into TEST_TB_GRADE(USER_NAME, COURSE, SCORE) values(&quot;张三&quot;, &quot;数学&quot;, 34),(&quot;张三&quot;, &quot;语文&quot;, 58),(&quot;张三&quot;, &quot;英语&quot;, 58),(&quot;李四&quot;, &quot;数学&quot;, 45),(&quot;李四&quot;, &quot;语文&quot;, 87),(&quot;李四&quot;, &quot;英语&quot;, 45),(&quot;王五&quot;, &quot;数学&quot;, 76),(&quot;王五&quot;, &quot;语文&quot;, 34),(&quot;王五&quot;, &quot;英语&quot;, 89);+----+-----------+--------+-------+| ID | USER_NAME | COURSE | SCORE |+----+-----------+--------+-------+| 1 | 张三 | 数学 | 34 || 2 | 张三 | 语文 | 58 || 3 | 张三 | 英语 | 58 || 4 | 李四 | 数学 | 45 || 5 | 李四 | 语文 | 87 || 6 | 李四 | 英语 | 45 || 7 | 王五 | 数学 | 76 || 8 | 王五 | 语文 | 34 || 9 | 王五 | 英语 | 89 |+----+-----------+--------+-------+行转列:select USER_NAME,max(case course when &apos;语文&apos; then score else 0 end) &apos;语文&apos;,max(case course when &apos;数学&apos; then score else 0 end) &apos;数学&apos;,max(case course when &apos;英语&apos; then score else 0 end) &apos;英语&apos;from TEST_TB_GRADEgroup by USER_NAME;+-----------+--------+--------+--------+| USER_NAME | 语文 | 数学 | 英语 |+-----------+--------+--------+--------+| 张三 | 58 | 34 | 58 || 李四 | 87 | 45 | 45 || 王五 | 34 | 76 | 89 |+-----------+--------+--------+--------+列转行:create table test_tb_grade2 as(select USER_NAME, max(case course when &apos;语文&apos; then score else 0 end) &apos;语文&apos;, max(case course when &apos;数学&apos; then score else 0 end) &apos;数学&apos;, max(case course when &apos;英语&apos; then score else 0 end) &apos;英语&apos;from TEST_TB_GRADEgroup by USER_NAME);select user_name,&apos;语文&apos;course,语文 as course from test_tb_grade2union select user_name,&apos;数学&apos;course,数学 as course from test_tb_grade2union select user_name,&apos;英语&apos;course,英语 as course from test_tb_grade2;]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis]]></title>
    <url>%2F2019%2F07%2F28%2Fredis%2F</url>
    <content type="text"><![CDATA[一.概述redis基于内存持久化，高性能的NoSql的key-value数据库.redis支持数据的持久化，可以将数据保存在磁盘中，重启时可以加载使用。redis持久化策略包括RDB,AOF。redis提供key-value类型的数据，同时还提供list,set,zset.hash类型的数据，redis单个key可存储512M大小，默认端口6379 二.配置文件redis的配置文件为redis.conf 123456789101112131415daemonize yes # 以守护进程启动port 6379 # 指定端口bind 127.0.0.1 #绑定本机，只有本机可以访问databases 16 # 默认的数据库的个数# redis RDB持久化策略的持久化操作save 900 1 每900秒有一次更新操作，做持久化操作save 300 10save 60 10000dbfilename dump.rdb # RDB持久化的镜像文件slaveof &lt;masterip&gt; &lt;masterport&gt; #主从复制requirepass foobared #设置密码maxclients 10000 # 允许客户端连接的最大线程数maxmemory &lt;bytes&gt; ＃指定redis最大内存限制appendonly no #指定是否在每次更新操作后进行日志记录，redis默认情况是异步的将数据写入磁盘 appendfilename &quot;appendonly.aof&quot;：# 指定更新日志文件名 三.数据类型 客户端启动和关闭 12redis-cli -h host -p port -a passwdredis-cli shutdown string类型 123456789101112131415set key val # 设置key和valexists key # 判断key是否存在expire key seceonds # 设置key的过期时间ttl key # 查看key的过期时间persist key # 移除过期时间rename key newkey # 修改key的名字move key db # 将key移动到指定数据库type key # 查看key的数据类型strlen key # 查看key的长度setnx key val # 如果key不存在，则创建getrange key start end # 获取key的指定字符串incr key# 自增incrby key num # 自增numdecr key # 自减decrby key num # 自减num 应用场景:计数，微博数，粉丝数 hash类型:string类型的field和value映射表，可以用来存储java对象,内部实际是一个hashmap，每个 hash 可以存储 232 -1 键值对 1234567891011hset key field value # 设置key valuehget key field # 获取 valuehmset key f1 v1 f2 v2 ＃ 设置多个值hgetall key ＃ 获取hash表中key的所有字段hkeys key 获取hash表中key的所有字段hlen key 获取hash表中key的字段数hdel key field1 field2 ＃ 删除key中的属性hsetnx key field value # 如果key不存在，则创建hincrby key field num # 自增num hincrbyfloat key field num # 自增num hexists key field # 判断key中的属性是否存在 应用场景：存储对象 list类型：列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边），list是一个双向链表。列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿) 123456789101112lpush key v1 v1 v3... #将一个或多个值插入列表头部rpush key v1 v2 v3... #将一个或多个值插入列表尾部lpushx key value #将一个值插入到已存在的列表头部，如果列表不存在，操作无效rpushx key value #将一个值插入到已存在的列表尾部，如果列表不存在，操作无效llen key //获取列表长度lindex key index //通过索引获取列表中的元素lrange key start stop //获取列表指定范围的元素lpop key 从左删除rpop key 从右删除blpop key timeout 移除并且获取列表第一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出数据为止brpop key timeout 移除并且获取列表最后一个元素，如果列表没有元素会阻塞列表直到等待超时或发现可弹出数据为止ltrim start end 保留指定下标的数据 应用场景：twitter的关注列表，粉丝列表，消息队列 set:集合是通过hashtable实现的，概念和数学中的集合基本类似，可以交集，并集，差集等等，set中的元素是没有顺序的。所以添加，删除，查找的复杂度都是O(1)。 12345678910111213sadd key v1 v2 ...scard key 获取集合的成员数smembers key 返回集合中的所有成员sismember key member 判断member是不是集合key的成员srandmember key [count] 返回集合中一个或多个随机数srem key member1 member2... 移除集合一个或多个 成员smove source dest member将member元素从source移动到destnation集合sdiff k1 k2 返回给定集合的差集sdiffstore k12 k1 k2 将k1 k2的差集存储在k12sinter k1 k2 交集sinterstore k12 k1 k2 将k1 k2的交集存储在k12sunion k1 k2 并集sunionstore k12 k1 k2 将k1 k2的并集存储在k12 应用场景:存储一个列表数据，不希望有重复值。set具有求交集，差集，并集的功能，方便实现共同关注，共同好友 sorted set(zset):有序集合,存储的也是string类型元素的集合，不允许有重复的成员，每个元素都会关联一个double类型的分数，redis通过分数为集合中的成员进行从小到大的排序,有序集合的成员是唯一的，但是分数可以重复，集合是通过hash来实现的，添加查找删除的时间复杂度都是1.集合中最大成员数为2的32次方减1 123456789zadd key score1 member1 score2 member2 添加多个成员zcard key 获取有序集合的成员数zcount key min max 返回有序集合指定期间分数的成员数zrank key member 返回有序集合中指定成员的索引zrange key start stop [withscores] 通过索引区间返回有序集合指定区间内的成员zrevrange key start stop [withscores] 通过索引区间返回有序集合指定区间内的成员，分数有高到低、zrem key member [member] 移除集合中一个或多个成员zremrangebyrank key start stop 移除有序集合给定的排名区间所有成员zremrangebyrange key min max 移除有序集合中给定的分数区间的所有成员 应用场景：排行榜，带权重的消息队列 flush 12flushdb 清除当前数据库所有的keyflushall 清除整个redis的数据库所有key 四.发布订阅12345subscribe channel [channel] 订阅某个频道psubscribe pattern [pattern] 订阅一个或多个符合指定模式的频道publish channel messageunsubscribe channel [channel] 退订频道punsubscribe pattern [pattern] 退订所有给定模式的频道 应用场景：构建实时消息系统，比如即时聊天，群聊，博客，公众号 五事物redis批量操作在exex之前，被放入到队列缓存，收到exec命令后进入事物执行。如果事物中有任意命令执行失败，其它命令依然被执行.redis会将一个事物中的所有命令序列化，然后按顺序执行，执行过程中不会被其他命令插入. 12345multi:标记一个事物的开始exec:执行命令discard 取消事物，放弃执行事物块内的所有命令watch:监视一个或多个key watch key [key]如果在事物执行之前这个key被其它命令所改动，那么事物将被打断unwatch:取消watch命令对key的监视 如果执行的某个命令报错，则只有报错的命令不会被执行，其它的命令会被执行回滚：当队列中的某个命令出现报告错误，执行时整个队列的命令都被取消 六.redis持久化redis数据存放在内存(高效，断电数据会丢失)当内存不足时，redis会根据配置的缓存策略淘汰部分key以保证写入成功，当无淘汰策略时或没有找到合适的key时，redis直接返回out of memory 淘汰策略 123456789在redis中，允许用户设置最大使用内存大小 maxmemory 512Gvolatile-lru:从已设置过期时间的数据中挑选最近最少使用的数据淘汰volatile-lfu:从已设置过期时间的数据中，删除一段时间内使用次数最少volatile-ttl：从已设置过期时间的数据中,挑选最近要过期的volatile-random:从已设置过期时间的数据中随机淘汰一些数据allkeys-lru:从数据集中挑选最近最少使用的数据淘汰allkeys-lfu:从所有keys中，删除一段时间内最少使用的数据allkeys-random:从数据集中随机选择数据淘汰no-enviction(驱逐):禁止驱逐数据，不采用任何淘汰算法。默认的设置 redis持久化机制 rdb:是redis的默认持久化机制，rdb相当于快照，保存的是一种状态.将内存中的数据一快照的方式写入到二进制文件中去，默认为dump.rdb 优点：快照保存数据极快，还原数据极快，适用于灾难备份 AOF(Append-only file):由于快照方式是在一定时间间隔，所有如果redis以为down掉的话，就会丢失最后一次快照后的所有修改，如果要求不能丢失任何修改的化，可以采用aof持久化方式.aof持久化方式:将每一个写命令都通过write函数追加到文件（appendonly.aof),当redis重启时会通过执行文件中的保存的写命令在内存中重建整个数据库. 1234appendonly yes # 开启AOF# appendfsync always 收到写命令立即写入到磁盘，最慢，但是保证完全的持久化 appendfsync everysec 每秒钟写入到磁盘一次# appendfsync no 完全依赖os，性能好，持久化没保证]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通信分析项目总结]]></title>
    <url>%2F2019%2F07%2F24%2F%E9%80%9A%E4%BF%A1%E5%88%86%E6%9E%90%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[该项目是基于通话日志的分析，当前每时每刻都有很多人打电话，那么产生的通话记录也是一个非常大数据量，需求就是根据这个通话日志，统计出每天，每月，每年用户的通话次数以及通话时长。 项目架构 项目中遇到的困难 数据格式转化有点麻烦 随机产生通话日志，要求数据遵循的格式一致，用到了一个新的api DecimalFormat，DecimalFormat主要是将数字格式化，它拥有两种占位符0 # 1234567891011// 0 的用法//１．情况一 如果0的位数比要格式化的数字位数多 会在不足的位置用０补充new DecimalFormat(“00.00”).format(3.14) //结果：03.14 //２．情况二 如果0的位数比要格式化的数字位数少 整数位不变，小数部分四舍五入new DecimalFormat(“0.00”).format(13.146) //结果：13.15 // # 的用法//１．情况一 如果#的位数比要格式化的数字位数多 不变new DecimalFormat(“##.###”).format(3.14) //结果：3.14 //２．情况二 如果＃的位数比要格式化的数字位数少 整数位不变，小数部分四舍五入new DecimalFormat(“#.##”).format(13.146) //结果：13.15 hbase预分区以及rowkey的设计 hbase建表时默认只有一个分区，所有数据都会写入到该分区，当数据越来越多时，region会进行split,分成两个reigon,之后，数据写入时会一直往一个region中写入数据，会造成数据热点问题，还有region进行split会消耗集群的io资源，所以需要进行预分区，控制数据的写入，在这个项目中，我总共分了6个分区，分区键为 0|, 1|, 2|, 3|, 4|, 5|.这样，在我往hbase插入数据的时候，就能有效的控制分区了。 为了能够将数据随机插入到给个分区，rowKey设计至关重要，基于分区键的设计，rowkey需要以05这样的数字开头，所以为了可以能够随机的将数据插入到各个分区，在这里，使用了tel和date的hashcode的值，在对分区数进行取模，得到的是05的数字，这样就能够保证数据可以有效的插入到各个分区。 1234567891011/** * rowKey的设计 * 1.长度原则：最大 64kb 推荐10-100byte,最好是8的倍数 * rowKey如果太长，影响性能 * 原因：rowKey如果太长，会占用MemStore,减少缓存的数据 * 2.唯一原则: * 3.散列原则： * 盐值散列：不能使用时间戳，在时间戳前面增加一些随机数 * 字符串反转（时间戳，电话号码） * 计算分区号 */ 3.mysql表的设计 设计了三张表，用户表，日期表，通话日志表 用户表记录用户名 用户电话 日期表记录日期,包括xxxx年 xxxx年xx月 xxxx年xx月xx日，三种类型的日期，方便进行统计 通话日志表有两个外键，一个外键指向用户表，主要是用于获取电话号码，一个外键指向日期表，用于获取具体的日期。 当时是用一张表来解决的，将电话号码和日期都存在通话日志表中，后来看表中数据的时候感觉冗余数据太多，考虑将表拆分。]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>项目总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala]]></title>
    <url>%2F2019%2F07%2F22%2Fscala%2F</url>
    <content type="text"><![CDATA[一.变量scala的数据类型 Any:根类型，所有类的父类，它有两个直接子类，AnyValue,AnyRef AnyRef:所有Scala中引用类（reference class)的父类 Null:所有引用类的子类，类似与java中的null Nothing:所有类的子类，可以将Nothing类型的值返回给任意变量或者函数 数据类型列表 变量声明1234567规则: val/var vname:type = valuevar i //error 变量声明必须初始化var x = _ //error 需要明确具体的类型val y = _ //error val不可变，需要确定的值var i:int = 10val j:Char = &apos;a&apos; var和val的区别： var修饰的变量是可变的，val修饰的变量是不可变的，它底层其实是使用final关键字修饰 键盘输入语句12345println(&quot;请输入你的姓名&quot;)var name = StdIn.readLine()println(&quot;请输入你的证件号&quot;)var card = StdIn.readLine(); 二.程序流程控制1. if-else与java中的if-else 差不多 2.for循环控制Scala为for循环这一常见的控制结果提供了非常多的特性，这些特性被成为for推导式或or表达式 范围数据循环方式 12345678910111213//两边都是闭合的 for(i&lt;-start to end)&#123; &#125;//左闭右开for(i&lt;-start until end)&#123; &#125;//对集合进行循环遍历var list = List(&quot;hello&quot;,&quot;world&quot;,&quot;good&quot;)for (i&lt;-list)&#123; println(i)&#125; 循环守卫 1234 //条件为true的进入循环体内部 为false的则不进入 for (i&lt;-1 to 10 if i%2!=0 )&#123; println(i)&#125; 引入变量 123 for (i&lt;-1 to 10;j = 10-i )&#123; println(i+&quot;:&quot;+j)&#125; 嵌套循环 1234 //嵌套循环 for (i&lt;-1 to 3;j&lt;-1 to 3)&#123; println(&quot;xxx&quot;)&#125; for 推倒 yield将结果收集 123//y 是一个Vector集合 Vector(1, 2, 3)val y = for (i&lt;-1 to 3) yield i println(y) 使用花括号 1234567for &#123; i&lt;-1 to 3 j&lt;-1 to 3 &#125; &#123; println(&quot;xxx&quot;) &#125; 控制步长 12345678//使用range控制for循环的步长 var range = new Range(1,10,2) for (i&lt;- range)&#123; println(i) &#125; for (i&lt;-1 to 10 if i % 2 == 1)&#123; println(i) &#125; 3.while循环控制12345678while()&#123; &#125;do&#123; &#125;while()两者区别:while先进行判断再执行 do while 先执行再判断 循环中断 1234567891011 var i =1; breakable&#123; while (i&lt;20)&#123; println(i) if(i%4==0)&#123; break; &#125; &#125; &#125; //需要导包 import util.control.Breaks._ 4.异常处理 在scala中只有一个catch,但是有多个case，每个case可以用来匹配一种异常 123456789101112try &#123; val i = 10/0; &#125;catch&#123; case exception: ArithmeticException =&gt;&#123; println(&quot;出现除0异常&quot;) &#125; case exception: NullPointerException =&gt;&#123; println(&quot;空指针异常&quot;) &#125; &#125;finally &#123; println(&quot;最后要被执行的代码&quot;) &#125; scala中没有checked异常，都是在运行进行捕捉的 123def test4():Unit=&#123; throw new ArithmeticException();&#125; 三.方法的定义基本语法123def methodName(arg1:type,arg2:type2)[:type]=&#123;&#125; 注意事项: 当函数的返回值是unit是，＝可以省略 12345678 def test1(str:String):Unit=&#123; println(str)&#125; def test1(str:String)&#123; println(str)&#125; 返回值类型可以自动推断 123456789// return可以省略def add(a:Int,b:Int):Int=&#123; a+b;&#125;// 返回值类型可以省略 =不可以省略，否则返回类型为Unitdef add(a:Int,b:Int)=&#123; a+b;&#125; 递归一定要声明返回值类型 12345678910def fac(n:Int):Int=&#123; if(n == 1)&#123; return 1 &#125; if (n==2)&#123; return 2 &#125; return fac(n-1)+fac(n-2) &#125; 可以指定默认值 1234//如果指定实参会将默认值覆盖def test2(str:String=&quot;test2&quot;)=&#123; println(str) &#125; 可变参数 123def test3(str:String,str2:Any*)=&#123; &#125; 过程返回值为Unit,忽略= 1234def test1(str:String)&#123; println(str) &#125; 惰性函数当函数的返回值被声明为lazy时，函数的执行将延迟，直到正在去取值的时候，函数才会执行 123456lazy val lazyVal = add(10,20);def add(a:Int,b:Int)=&#123; a+b&#125; 注意:不能修饰var变量 ​ lazy还可以用来修饰变量 1lazy val i = 1 //变量值的分配将会延迟 四.面向对象编程1.类与对象 类的定义 编写一个类，用scalac将器编译，再通过 javap -p 反编译查看生成的java代码 123456789//scala 类class Cat &#123; var name:String = _ var age:Int = _ var gender:String = _&#125; javap -p Cat.class 123456789101112131415public class com.hsj.scala.day1.day2.Cat &#123; //属性 private java.lang.String name; private int age; private java.lang.String gender; //get set方法 public java.lang.String name(); //name的get public void name_$eq(java.lang.String);//name的set public int age(); //age get public void age_$eq(int);//age set public java.lang.String gender();//gender get public void gender_$eq(java.lang.String);gendet set //构造器 public com.hsj.scala.day1.day2.Cat();//默认构造器&#125; 可以看到 属性默认是private,如果使用的是var，对应的会生成get set 方法 如果使用val,则只会生成get方法 创建对象 1234def main(args: Array[String]): Unit = &#123; val cat:Cat = new Cat var cat2:Cat = new Cat &#125; 推荐使用val,因为创建对象一般不会改变它的引用，只是改变它的属性值 2.构造器scala的构造器与java的构造器有很多相同的地方，scala中的构造器可以定义多个，支持重载，这一点和java一样，但是scala也有自己的一些新特性，scala的构造器分为主构造器，辅助构造器，主构造器在类上声明。 123456789101112class Cat(nameIn:String, ageIn:Int, gen:String)&#123; //主构造器 var name:String = _ var age:Int = _ var gender:String = _ def this()&#123; //辅助构造器 this(&quot;jack&quot;,18,&quot;male&quot;) &#125;&#125; 注意： ​ 在辅助构造器中一定要在第一行调用主构造器 主构造器中的参数 如果主构造器中的参数没有加修饰符修饰，默认为局部变量 主构造器中的参数使用var修饰，这个参数会升级成为成员变量，并且提供get set方法 主构造器中的参数使用val修饰，这个参数会升级成为成员变量，只提供get 方法 get set方法12345678//get val cat:Cat = new Cat println(cat.name) println(cat.age) println(cat.gender)//set cat.name = &quot;mary&quot; println(cat.name) 如果想要使用java风格的get set需要在属性字段上添加@BeanProperty 123@BeanProperty var name:String = _println(cat.getName) 对象的创建流程 new 关键字创建对象 识别new后面的类是否被加载，如果没有被加载，先加载类 在内存为该对象分配空间 调用父类构造器进行初始化 调用构造器进行初始化 将该对象的地址赋值给一个变量 3.packagescala中的包与java中的包作用一样 区分相同名字的类 控制访问范围 类很多时，方便管理类 scala默认导入三个包，java.lang ,scala._ ,scala.predef 包的使用12345678910package com.hsj.packageTestpackage test&#123; class Test &#123; &#125; package scala&#123; &#125;&#125; 注意事项： package test{} :表示创建了包com.hsj.packageTest.test 在scala中，可以在一个文件中创建多个package,并且可以在包中创建类，特质，和对象 scala中子包可以访问父包中的内容，｛ ｝体现了作用范围，当子包拥有与父包相同的类时，采用就近原则，如果希望指定哪个类，带上包名即可 如果想要在父包中使用子包的一些内容，需要在父包中import 包中不能定义变量和方法 包对象由于包中不能定义变量和方法，使用包对象补足 每一个包都可以拥有一个包对象，名字必须相同，在包对象中定义的变量和方法可以在包中使用 12345678910111213141516package object scala&#123; var name:String =&quot;scala&quot;; def print():Unit=&#123; println(name) &#125;&#125;package scala&#123; object test&#123; def main(args: Array[String]): Unit = &#123; print() &#125; &#125;&#125; scala中包的访问权限 属性的访问权限为默认时，属性使用的是private修饰 方法的访问权限为默认时，属性使用的是public修饰 private只有在类的内部和伴生对象中可用 protected 只有在子类可以访问，同包无法访问，这点和java不同 scala添加了包访问权限，表示属于那个包 123456789101112package scala&#123; object test&#123; def main(args: Array[String]): Unit = &#123; var per:Persion = new Persion println(per.name) &#125; &#125; class Persion&#123; private [scala] var name:String = &quot;jack&quot; &#125;&#125; 4.伴生类与伴生对象介绍在scala中，类用class表示，object表示的一个静态的对象，如果class的名称与object的名称相同，该类被称为半生类，object被称为伴生对象。伴生对象的内容都是静态的，可以通过类名直接调用，类和其伴生对象可以互相访问私有属性，但必须存在同一个源文件中 1234567891011object User&#123; var name:String = &quot;jack&quot; def main(args: Array[String]): Unit = &#123; println(User.name) &#125;&#125;class User( var name:String)&#123; &#125; apply方法使用apply(装配器，将属性装配到对象)方法，实现类名(参数)的方式创建对象 12345678910111213141516171819202122object User&#123; def apply():User=new User(); def apply(name:String): User = new User(name)&#125;class User()&#123; var name:String = _; def this(name:String)&#123; this this.name = name; &#125;&#125;object test&#123; def main(args: Array[String]): Unit = &#123; val u1 = User(&quot;link&quot;) println(u1.name) &#125;&#125; 5.trait(特质)特质的声明123trait 特质名&#123; &#125; 特质的使用1class 类名 extends 父类 with trait1 with trait2 with trait3... 特质中可以拥有抽象方法和具体方法 12345678trait Persion &#123; def eat() def sleep(): Unit =&#123; println(&quot;sleep~~~~&quot;) &#125;&#125; 动态混入除了可以在类声明的时候继承特质，还可以在构建对象是动态的混入特质，达到扩展功能的效果 12345678910111213141516171819202122232425262728293031323334trait Persion &#123; def eat() def sleep(): Unit =&#123; println(&quot;sleep~~~~&quot;) &#125;&#125;trait Animal&#123; def fright(): Unit =&#123; println(&quot;fright~~~&quot;) &#125; def run();&#125;class P1 extends Persion&#123; override def eat(): Unit = &#123; println(&quot;eat~~~~~&quot;) &#125;&#125;object P1&#123; def main(args: Array[String]): Unit = &#123; val p1 = new P1 with Animal&#123; override def run(): Unit = &#123; &#125; &#125; p1.eat() p1.fright() &#125;&#125; 叠加特质:构建对象时混入多个特质，称为叠加特质 叠加特质的声明是从左到右，方法的执行是从右到左 123456789101112131415161718192021222324252627282930313233343536object TestTrait &#123; def main(args: Array[String]): Unit = &#123; //声明从左到右，执行从右到左 类似栈 val abc = new ABC with A1 with A2 abc.test //执行的是A2中的test方法 &#125;&#125;class ABC&#123;&#125;trait Total&#123; def test;&#125;trait A extends Total &#123; override def test: Unit =&#123; println(&quot;A test~~&quot;) &#125; println(&quot;A~~&quot;)&#125;trait A1 extends A&#123; override def test: Unit =&#123; println(&quot;A1 test~~&quot;) super.test &#125; println(&quot;A1~~&quot;)&#125;trait A2 extends A&#123; override def test: Unit =&#123; println(&quot;A2 test~~&quot;) super.test &#125; println(&quot;A2~~&quot;)&#125; 叠加特质一定要注意方法的执行顺序如上述代码中，先执行A2中的test，执行到 super.test的时候，执行的不一定是父特质的方法，它会根据混入的特质的顺序执行，执行的是它左边的特质中的test方法，如果左边没有特质则执行父特质的方法。 上述代码执行顺序： A2.test–&gt;A1.test–&gt;A.test 注意:A1和A2继承的是同一个父特质，如果不是同一个父特质，那么A2中的super执行的是父特质test 12345678910111213141516171819202122232425262728293031323334353637383940object TestTrait &#123; def main(args: Array[String]): Unit = &#123; val abc = new ABC with A1 with A2 abc.test &#125;&#125;class ABC&#123;&#125;trait Total&#123; def test;&#125;trait A extends Total &#123; override def test: Unit =&#123; println(&quot;A test~~&quot;) &#125; println(&quot;A~~&quot;)&#125;trait B extends Total&#123; override def test: Unit =&#123; println(&quot;B test~~&quot;) &#125; println(&quot;B~~&quot;)&#125;trait A1 extends A&#123; override def test: Unit =&#123; println(&quot;A1 test~~&quot;) super.test &#125; println(&quot;A1~~&quot;)&#125;trait A2 extends B&#123; override def test: Unit =&#123; println(&quot;A2 test~~&quot;) super.test &#125; println(&quot;A2~~&quot;)&#125; 执行顺序:A2.test—&gt;B.test 富接口富接口：该特质中既有抽象方法又有非抽象方法 123456trait Persion &#123; def eat() def sleep(): Unit =&#123; println(&quot;sleep~~~~&quot;) &#125;&#125; trait中的字段trait中可以定义具体的字段和抽象的字段，一个对象如果混入该特质，就拥有了特质中的字段 12345678910111213object TestTrait2 extends App &#123; val param = new T with Param&#123; override val age: Int = 18 &#125; println(param.name)&#125;trait Param&#123; val name:String = &quot;jack&quot; val age:Int;&#125;class T&#123;&#125; trait的构造顺序创建对象时混入特质 调用当前类的超类的构造器 调用第一个trait的的超类构造器 调用第一个trait的构造器 调用第二个trait的超类构造器 调用第二个trait的构造器 调用当前类的构造 trait的扩展类trait可以继承类，如果有那个类(A)混入了该trait,那么，这个类（A)会成为trait的超类的子类， 如果这个类（Ａ）还继承了别的类，继承的这个类一定要是trait的超类的子类，否则会发生多继承的错误 123456789101112131415object TestTrait3 &#123; def main(args: Array[String]): Unit = &#123; val a = new A println(a.getMessage) &#125;&#125;trait TraitException extends Exception&#123; override def getMessage: String = &#123; return &quot;TraitException&quot; &#125;&#125;class A extends Exception with TraitException&#123;&#125; 解决该情况在idea中编译时不报错的方式 1234567// 自身类型trait TraitException extends Exception&#123; this:Exception =&gt; override def getMessage: String = &#123; return &quot;TraitException&quot; &#125;&#125; 6.重写 在Scala中重写一个非抽象方法必须使用override修饰符。 1class week extends month&#123; override def firstday = &#123;...&#125; &#125; .重写包括字段和方法，但参数不同的方法可以不重写 123456class month&#123; def secondday（m:String）=&#123;...&#125;&#125; class week extends month&#123; def secondday =&#123;...&#125;&#125; 在Scala中调用超类的方法和Java一样，使用super关键字 只有主构造器可以调用超类的主构造器; 1class Employee(name:String,age:Int,val salary:Double) extends Person(name:String,age:Int) 重写规则 1234567重写 def 用val ：利用val能重写超类用没有参数的方法(getter) 用def：子类的方法与超类方法重名 重写val 用val：子类的一个私有字段与超类的字段重名，getter方法重写超类的getter方法 重写var 用var：且当超类的var是抽象的才能被重写，否则超类的var都会被继承 7.嵌套类内部类1234567891011121314151617181920class OuterClass &#123; //别名 outerClass =&gt; var name:String = &quot;jack&quot; class InnerClass&#123; def inner: Unit =&#123; //访问外部类的属性 1 println(OuterClass.this.name) //访问外部类的属性 2 别名 println(outerClass.name) &#125; &#125; def tets: Unit =&#123; //创建内部类的对象 val outerClass = new OuterClass val innerClass:outerClass.InnerClass = new outerClass.InnerClass &#125;&#125; 静态内部类123456789101112class OuterClass &#123; def tets: Unit =&#123; //创建静态内部类对象 val staticInnerClass = new OuterClass.StaticInnerClass &#125;&#125;//伴生对象object OuterClass&#123; class StaticInnerClass&#123; &#125;&#125; 类型投影创建内部类时，创建的内部类会与外部类实例产生关联 1234567891011121314151617181920212223class OuterClass2 &#123; class InnerClass&#123; def test(innerClass: InnerClass): Unit =&#123; println(innerClass) &#125; &#125;&#125;object OuterClassDemo&#123; def main(args: Array[String]): Unit = &#123; val outer1 = new OuterClass2 val outer2 = new OuterClass2 val inner1 = new outer1.InnerClass val inner2 = new outer2.InnerClass inner1.test(inner1); inner2.test(inner2)//报错 由于inner1已经和外部类实例outer1关联，inner1调用test方法传入的参数只能是outer1.InnerClass// inner1.test(inner2) &#125;&#125; 为了解决上述问题，只需要在方法声明上添加 外部类#内部类,表示忽略外部类实例与内部类实例的关系 123def test(innerClass: OuterClass2#InnerClass): Unit =&#123; println(innerClass)&#125; ## 五.隐式转换和隐式值 1.隐式转换解决数据类型的转换123456789101112object ImpliciteTest&#123; def main(args: Array[String]): Unit = &#123; //在底层生成f1$1(double d)的方法，将double强转为int implicit def f1(d:Double):Int=&#123; d.toInt &#125; //底层调用f1$1（2.5)的方法 val i = 2.5; &#125;&#125; 2.隐式值隐式值也叫隐式变量，当一个方法在省略隐式参数时，会自动搜索作用域的隐式参数作为缺省参数 123456789 def main(args: Array[String]): Unit = &#123; implicit val name:String = &quot;lix&quot; def test(implicit name:String): Unit =&#123; println(name) &#125; test&#125; 3.隐式值，默认值，传值的优先级传值&gt;隐式值&gt;默认值 12345678910111213141516171819 def main(args: Array[String]): Unit = &#123; implicit val name:String = &quot;lix&quot; def test(implicit name:String=&quot;jack&quot;): Unit =&#123; println(name) &#125; test //结果为lix&#125; def main(args: Array[String]): Unit = &#123; implicit val name:String = &quot;lix&quot; def test(implicit name:String=&quot;jack&quot;): Unit =&#123; println(name) &#125; test(&quot;xxx&quot;) //结果为xxx&#125; 4.隐式类隐式类必须定义在类，伴生对象，包对象中，不能作为一个顶级类存在，并且隐式类有且只有一个构造函数 123456789101112131415object ImpliciteTest&#123; def main(args: Array[String]): Unit = &#123; // implicit 修饰的是构造器 implicit class A(b: B)&#123; def test: Unit =&#123; println(&quot;隐式类的调用&quot;) &#125; &#125; val b = new B b.test &#125;&#125;class B&#123;&#125; 总结： 隐式类的转换时机 方法中的参数与它需要的参数类型不一致，或者是在赋值的时候 当一个对象调用本类不存在的方法或参数时 六.数据结构1.集合关系scala集合包括可变集合和不可变集合,scala集合有三大类.序列seq,集合set，映射map 可变集合：scala.collection.mutable,集合本身不能动态改变，java中的数组 不可变集合：scala.collection.immutable，集合本身可以动态改变，java中的ArrayList 不可变集合预览图 可变集合预览图 2.数组不可变数组123456789101112131415// 定义方式一 val a1 = new Array[String](5) // 定义方式二 val a2 = Array(&quot;hello&quot;,&quot;scala&quot;) //遍历方式一 for (elem &lt;- a2) &#123; println(elem) &#125; //遍历方式二 for (i&lt;-0 to a2.length)&#123; println(a2(i)) &#125; //遍历方式三 a2.foreach(println) 可变数组123456789101112131415161718192021222324252627282930313233343536373839val ab1 = ArrayBuffer[String](&quot;hello&quot;,&quot;scala&quot;) val ab2 = ArrayBuffer[String](&quot;hello2&quot;,&quot;scala2&quot;) //append 在原数组后面追加数据 ab1.append(&quot;ab1&quot;,&quot;append&quot;) //remove 删除数组指定下标的元素并将其返回 val res1: String = ab1.remove(0) //从数组下标为0的位置开始，删除两个元素 val res2 = ab1.remove(0,2); //drop 删除指定个数的元素 返回删除的元素，形成一个新的数组 原数组不变 val res3:ArrayBuffer[String] = ab1.drop(2) /dropRight 从右边删除指定个数的元素 返回删除的元素，形成一个新的数组 val ab2 = ab.dropRight(3) //可变数组转化为不可变数组 原数组不发生改变 val newArr: Array[String] = ab1.toArray //将不可变数组转化为可变数组，原数组不发生改变 val newBuf = newArr.toBuffer //在ab1后面追加集合ab2 ab1 ++= ab2; //在ab1前面追加集合ab2 ab1.++=:(ab2) // ab2 ++=: ab1 //在集合ab1后面追加集合ab2并返回一个新的集合 val ab3 = ab1.++(ab2) //在集合ab1前面追加集合ab2并返回一个新的集合 val ab4 = ab1.++:(ab2) ab1.+=(&quot;hello+=&quot;) ab1.+=:(&quot;hello+=:&quot;) val ab5 = ab1.+(&quot;hello+&quot;); val ab6 = ab1.+(&quot;hello+:&quot;) 二维数组12345678 val arr: Array[Array[String]] = Array.ofDim[String](5,4) for (item&lt;-arr)&#123; for (res&lt;-item)&#123; print(res+&quot;\t&quot;) &#125; println&#125; 3.元组元组可以存放各种数据类型不同的数据，元组将多个无关的数据封装成为一个整体，元组最多只能有22个元素 123456789101112// 元组的定义 val t1: Tuple1[String] = new Tuple1(&quot;1&quot;) val t3: (Int, Int, String) = new Tuple3(1,2,&quot;str&quot;) val t4 = (&quot;1&quot;,2.1,&quot;str&quot;,&quot;hello&quot;) //元组的访问 println(t4._1) println(t4._2) //元组的遍历 for (i&lt;-t3.productIterator)&#123; println(i) &#125; 4.List当我们定义一个List时在scala中是不可变的，如果需要使用可变的，需要使用ListBuffer 不可变集合List12345678910//不可变集合 val l1 = List(1,2,&quot;abc&quot;) val list = List(1,2,3,&quot;str&quot;) // :: 集合对象一定要放在右边 如果不加Nil会形成一个集合，如果加Nil,会形多个集合的集合 val list2 = 4::5::list //List(4, 5, 1, 2, 3, str) val list3 = 4::5::list::Nil //List(4, 5, List(1, 2, 3, str)) //::: 将集合中的每一个元素添加到集合中去 注意 ::: 左右两边都要是集合 val list4 = 4::5::immutable.Nil:::list println(list4) 可变集合ListBuffer123456789101112131415161718192021222324252627282930313233343536373839//可变集合 val lb = ListBuffer(11,22,&quot;abc2&quot;) println(lb.length) //;list元素的访问 println(lb(1)) //遍历 for (i&lt;-lb)&#123; println(i) &#125; // 获取除第一个元素外的其它元素 val tail = lb.tail println(tail) // 从右边开始获取n个元素 val tr = lb.takeRight(2) println(tr) // 获取n个元素 val t = lb.take(2) println(t) //获取符合条件的元素 val tw = lb.takeWhile(i=&gt;i==null) println(tw) println(lb) println(&quot;===========================&quot;) //删除n个元素,将删除后的原集合剩下的元素返回 val ld = lb.drop(2) println(ld) //从右边开始删除n个元素,将删除后的原集合剩下的元素返回 val dr = lb.dropRight(2) println(dr) //删除满足条件的元素，将删除后的原集合剩下的元素返回 val dw = lb.dropWhile(elem =&gt; elem!=null) println(dw) 映射Mapscala中的集合默认是不可变的，如果需要使用可变的，可以使用scala.collection.mutabl下的类 123456789101112131415161718192021222324252627//定义 val map1 = Map((1,&quot;li&quot;),(2,&quot;xi&quot;),(3,&quot;jiafeng&quot;)) map1.+((4,&quot;meng&quot;)) val map2 = mutable.Map((1,&quot;lo2&quot;),(2,&quot;xo2&quot;))//取值 // map1(4) 如果不存在 NoSuchElementException // 如果存在 返回Some 不存在返回None println(map1.get(3)) // 如果值存在 返回值 不存在 返回 默认值 println(map1.getOrElse(4,&quot;NULL&quot;))//添加｜修改 如果不存在则添加，存在则覆盖 map2(1) = &quot;lolo&quot; println(map2) map2.+=((1,&quot;xxx&quot;)); println(map2) //遍历方式 for ((k,v)&lt;- map2)&#123; println(k+&quot; &quot;+v) &#125; for (m&lt;- map2)&#123; println(m._1+&quot; &quot;+m._2) &#125; for (k &lt;- map2.keys)&#123; println(k+&quot; &quot;+map2(k)) &#125; Set不重复元素的集合 123456789101112131415161718//定义 //不可变集合 val set1 = Set(6,1,2,3,1) //不可变集合 val set2 = mutable.Set(6,1,2,3,1) println(set1)//增 set2.+=(4) set2.add(5)//删 set2.-=(4) set2.remove(5) println(set2)//遍历 for (i&lt;- set2)&#123; println(i) &#125; 集合元素的操作 将List(1,2,3)中的所有元素*2 123val list = List(1,2,3,4,5)val res1 = list.map(i =&gt; i*2)println(res1) 将List(“hello”,”world”,”scala”)中的所有单词，全部转为大写 12val res2 = list2.map(word=&gt;word.toUpperCase) println(res2) flatMap 扁平化 12345val res3 = list2.flatMap(s=&gt;s.toUpperCase)println(res3) // List(H, E, L, L, O, W, O, R, L, D, S, C, A, L, A)val list3 = List(List(1,2,3),List(4,5,6),List(7,8,9))println(list3.flatMap(i =&gt; i.map(j=&gt;j*2)))//List(2, 4, 6, 8, 10, 12, 14, 16, 18) filter 过滤 123val list = List(1,2,3,4,5)val res = list.filter(i =&gt; i%2==0)println(res)//List(2, 4) reduce 1234567val list = List(1,2,3,4,5)val res = list.reduce((a,b)=&gt;a+b)println(res)// ((((1-2)-3)-4)-5) println(list.reduceLeft((a,b)=&gt;a-b))//(1-(2-(3-(4-5))))println(list.reduceRight((a,b)=&gt;a-b)) fold 把上一步的返回值交给下一步，继续参与运算 1234567891011val list = List(1,2,3,4,5) //类似 List(1,1,2,3,4,5).reduceLeft((num1,num2)=&gt;num1-num2)) println(list.fold(1)((num1,num2)=&gt;num1-num2)) //类似List(1,1,2,3,4,5).reduceLeft((num1,num2)=&gt;num1-num2)) println(list.foldLeft(1)((num1,num2)=&gt;num1-num2)) //类似List(1,2,3,4,5,１).reduceLeft((num1,num2)=&gt;num1-num2)) println(list.foldLeft(1)((num1,num2)=&gt;num1-num2)) //foldLeft 的缩写为 /: foldRight 的缩写为 \: println(list./:(1)((num1,num2)=&gt;num1-num2)) println(list.:\(1)((num1,num2)=&gt;num1-num2)) scan 对集合中的所有元素做fold操作，但是会保留每步的操作结果 1234567val list = List(1,2,3,4,5) val res: List[Int] = list.scan(5)((n1, n2)=&gt;n1-n2) println(res)//List(5, 4, 2, -1, -5, -10) val res2 = list.scanLeft(5)((n1, n2)=&gt;n1-n2) println(res2)//List(5, 4, 2, -1, -5, -10) val res3 = list.scanRight(5)((n1, n2)=&gt;n1-n2) println(res3)//List(-2, 3, -1, 4, 0, 5) 拉链操作 12345678910val l1 = List(1,2,3,4)val l2 = List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;)println(l1.zip(l2)) // 会造成数据丢失List((1,a), (2,b), (3,c), (4,d))// 三个参数println(l1.zipAll(l2,&quot;l1N&quot;,&quot;l2N&quot;))//List((1,a), (2,b), (3,c), (4,d), (l1N,e))val res = l1.zipAll(l2,0,&quot;l2N&quot;)println(res)//List((1,a), (2,b), (3,c), (4,d), (0,e))val res2: (List[Int], List[String]) = res.unzipprintln(res2)// (List(1, 2, 3, 4, 0),List(a, b, c, d, e)) java集合与scala集合互相转换 12345678// java集合与scala集合的互相转换 import scala.collection.JavaConverters._ val scalaList = List(1,2,3,4,5,6) val javaList: util.List[Int] = scalaList.asJava val scalaList2 = javaList.asScala val scalaMap = Map((1,&quot;a&quot;),(2,&quot;b&quot;)) val javaMap = scalaMap.asJava val scalaMap2 = javaMap.asScala 七.模式匹配1.基本介绍采用match关键字声明，每个分支采用case关键字声明,需要匹配时从第一个case开始，如果匹配成功，执行相应的逻辑代码。 123456789val oper:Char = &apos;2&apos; val n1:Int = 10 val n2:Int =20 val res = oper match &#123; case &apos;+&apos; =&gt; n1+n2 case &apos;-&apos; =&gt; n1-n2 case _ =&gt; println(&quot;error&quot;) &#125; 注意: 如果所有的case都不匹配则执行case _ ,如果没有case _ 会抛出一个MatchException 如果case _后面有条件判断（守卫），则case _不是默认匹配 case后面跟变量，match前面的值会赋值给变量 12345oper match &#123; case a =&gt; if (a.equals(&apos;+&apos;)) n1+n2 case b =&gt; if (b.equals(&apos;-&apos;)) n1-n2 case _ =&gt; println(&quot;error&quot;)&#125; 2.类型匹配在类型匹配时，编译器会自动判断有没有可能匹配的类型 12345678 //类型匹配 val a:Int = 1 a match &#123; case t1:Int =&gt; &quot;Int&quot;// case t2:String =&gt;&quot;String&quot; case t3:Long =&gt; “long”// case t4:List[Int] =&gt; &quot;list[Int]&quot; &#125; 3.数组匹配12345678910val arrs = Array(Array(0,1),Array(0),Array(0,1,2,3)) for (arr&lt;-arrs)&#123; val res = arr match &#123; case Array(x) =&gt; x case Array(x,y) =&gt; ArrayBuffer(y,x) case Array(0,_*) =&gt; &quot;以0开头&quot; case _ =&gt; &quot;other&quot; &#125; println(res) &#125; 4.列表匹配拥有unapply方法的集合才能够进行匹配 1234567891011val lists = List(List(1),List(1,2),List(0,List(1,2))) for (list&lt;-lists)&#123; val res = list match &#123; case x::Nil =&gt; x case 0::tail=&gt; &quot;...&quot; case x::y::Nil =&gt; x+&quot;&quot;+y case _ =&gt; &quot;some else &quot; &#125; println(res) &#125; 5.对象匹配对象匹配需要通过unapply方法进行匹配，unapply是一个对象提取器，返回some则表示匹配成功，返回None表示匹配失败。 123456789101112131415161718object Nums&#123; def apply(x:Double): Double = &#123; Math.pow(x,2) &#125; def unapply(arg: Double): Option[Double] =&#123; Some(Math.sqrt(arg))// None &#125;&#125;val nums = Nums(6) nums match &#123; // 调用unapply方法，将返回值赋值给n case Nums(n) =&gt; println(n) case _ =&gt; println(&quot;else&quot;)&#125; 6.样例类样例类使用case声明的一种特殊的类，构造器中的每个参数默都是val，它提供了apply方法用于创建对象，提供unapply方法用于进行模式匹配，此外，它还提供了equals ,copy,toString等方法。样例类对应了两个class文件，类名.class,类名$.class. 1234567891011121314object TestCaseClass &#123; def main(args: Array[String]): Unit = &#123; for (i&lt;-(Array(Dollar(1000,&quot;$&quot;),Currency(1000,&quot;RMB&quot;))))&#123; i match &#123; case Dollar(x,y) =&gt; println(x+&quot; &quot;+y) case Currency(x,y) =&gt; println(x+&quot; &quot;+y) &#125; &#125; &#125;&#125;case class Dollar(value:Double,unit:String)case class Currency(value:Double,unit:String) 样例类提供了copy方法，构造新的对象 12val dollar = Dollar(20000,&quot;$&quot;)val ndollar = dollar.copy() 7.sealed样本类的超类被封闭（sealed），封闭类除类定义文件外不能添加子类.模式匹配完成后需要确保所有情况皆被考虑,因此Scala编译器会检测match表达式所遗漏的模式组合 12345678910111213sealed abstract class Expr case class Number( n :Int) extends Expr case class Sum(e1 : Expr , e2 : Expr) extends Expr case class Mul(e1 : Expr , e2 : Expr) extends Expr如何定义存在可能样本遗漏的模式匹配def getType(a:Expr):String = a match&#123; case Number(n) =&gt; “Number“ case Sum(m,n) =&gt; “Sum“&#125;warning : match is not exhaustive 可以添加注解,消除warningdef getType(a:Expr):String = (a: @unchecked) match &#123;...&#125; 8.偏函数被包在花括号内的一组case语句是一个偏函数–一个并非对所有输入值都有定义的函数。它是PartialFuncation[A,B]类的一个实例。(A是参数类型，B是返回类型).Scala中的PartialFunction是一个Trait，其的类型为PartialFunction[A,B]，其中接收一个类型为A的参数，返回一个类型为B的结果 12345678val pf :PartialFunction[Int,String] =&#123; case 1 =&gt; &quot;a&quot; case 2 =&gt; &quot;b&quot; case 3 =&gt; &quot;c&quot; case 4 =&gt; &quot;d&quot; case _ =&gt; &quot;other&quot;&#125;println(pf(1)) 偏函数中常用的方法 isDefinedAt:这个函数的作用是判断传入来的参数是否在这个偏函数所处理的范围内。刚才定义的pf来尝试使用isDefinedAt()，只要是Int类型都是正确的，因为有case _=&gt; “Other”这一句。如果换成其他类型就会报错。 orElse : 将多个偏函数组合起来使用，效果类似case语句。 123456789101112 val onePF:PartialFunction[Int,String] = &#123; case 1=&gt;&quot;One&quot; &#125; val twoPF:PartialFunction[Int,String] = &#123; case 2=&gt;&quot;Two&quot; &#125;val threePF:PartialFunction[Int,String] = &#123; case 3=&gt;&quot;Two&quot; &#125;val otherPF:PartialFunction[Int,String] = &#123; case _=&gt;&quot;other&quot; &#125; //作用与上面的偏函数作用一致val newPF = onePF orElse twoPF orElse threePF orElse otherPF andThen: 相当于方法的连续调用 12345678910111213141516171819202122232425262728293031val pf :PartialFunction[Int,String] =&#123; case 1 =&gt; &quot;a&quot; case 2 =&gt; &quot;b&quot; case 3 =&gt; &quot;c&quot; case 4 =&gt; &quot;d&quot; case _ =&gt; &quot;other&quot;&#125;val pf2:PartialFunction[String,String] = &#123; case a if(a.eq(&quot;a&quot;)) =&gt; &quot;a 1&quot; case b if(b.eq(&quot;b&quot;)) =&gt; &quot;b 2&quot; case c if(c.eq(&quot;c&quot;)) =&gt; &quot;c 3&quot; case d if(d.eq(&quot;d&quot;)) =&gt; &quot;d 4&quot; case _ =&gt; &quot;other&quot;&#125;val newpf = pf.andThen(pf2)println(newpf(1)) //结果为 a 1 applyOrElse:它接收2个参数，第一个是调用的参数，第二个是个回调函数。如果第一个调用的参数匹配，返回匹配的值，否则调用回调函数。 1println(pf.applyOrElse(10,&#123;a:Int =&gt; &quot;ten&quot;&#125;)) // other 8.函数式编程闭包函数闭包函数 返回的函数体引用到外部的参数,参数与函数体形成一个闭包 123def cosure(x:Int)=&#123; (y:Int) =&gt; x*y &#125; 柯里化函数将接受多个参数的函数转化成接受一个参数的函数 1234567 def mul(a:Int,b:Int,c:Int)=a*b*c //def mut1(a:Int)(b:Int) = a*b def mul1(a:Int)=(b:Int) =&gt; a*b //def mut2(a:Int)(b:Int)(c:Int) = a*b*c def mut2(a:Int) = (b:Int) =&gt; (c:Int) =&gt; a*b*c 9.类型系统1.泛型 上界: A&lt;:T；只能传入T的子类和本身 1234567891011121314object TestShangJie &#123; def main(args: Array[String]): Unit = &#123; val compareUtil = new CompareUtil[String] println(compareUtil.greater(&quot;Hello&quot;,&quot;acv&quot;)) &#125;&#125;// 只用实现了Comparable的子类才可以使用该类 Comparable为上界class CompareUtil[T&lt;:Comparable[T]]&#123; def greater(a:T,b:T):T=&#123; if (a.compareTo(b)&gt;0) a else b &#125;&#125; 下界 ：A&gt;:T；只能传入T的父类和本身 1234567891011121314151617181920212223242526272829303132class Earth&#123; def sound(): Unit =&#123; println(&quot;earth sound&quot;) &#125;&#125;class Animal extends Earth&#123; override def sound(): Unit = &#123; println(&quot;animal sound&quot;) &#125;&#125;class Dog extends Animal&#123; override def sound(): Unit = &#123; println(&quot;dog sound&quot;) &#125;&#125; def main(args: Array[String]): Unit = &#123; val compareUtil = new CompareUtil[String] println(compareUtil.greater(&quot;Hello&quot;,&quot;acv&quot;)) val earths = Array[Earth](new Earth,new Earth) val animals = Array[Animal](new Animal,new Animal) val dogs = Array[Dog](new Dog,new Dog) testlLowerBound(earths).map(_.sound()) testlLowerBound(animals).map(_.sound()) // testlLowerBound(dogs).map(_.sound()) 报错 &#125; def testlLowerBound[T&gt;:Animal](array: Array[T]): Array[T] =&#123; array &#125; 视图界定：A&lt;%T;支持上界和隐式转换，在利用Compareble进行比较的时候，由于Int不是Comparable[Int]的子类型，不能直接进行传参比较。而scala中会将Int隐式转化成RichInt,利用视图界定,可以有效的解决Comparable不能直接传Int的问题 1234567// 允许传入Comparable的子类和class CompareUtil[T&lt;% Comparable[T]]&#123; def greater(a:T,b:T):T=&#123; if (a.compareTo(b)&gt;0) a else b &#125;&#125; 上下文界定：T:M ;要求必须存在一个类型为M[T]的”隐式值” 写法一： 1234567891011121314151617181920// 隐式值 implicit val compare = new Ordering[Emp]&#123; override def compare(x: Emp, y: Emp): Int = &#123; x.age-y.age &#125;&#125;// [T:Ordering] =》 (implicit ord:Ordering[T])def comparatorEmp[T:Ordering](t1:T,t2:T)(implicit ord:Ordering[T]): T =&#123; if(ord.compare(t1,t2)&gt;0) t1 else t2&#125;class Emp(var age:Int)&#123;&#125;// 方法调用def main(args: Array[String]): Unit = &#123; val emp1 = new Emp(18) val emp2 = new Emp(20) val res = comparatorEmp(emp1,emp2) println(res)&#125; 写法二: 12345def comparatorEmp[T:Ordering](t1:T,t2:T): T =&#123; def helper(implicit ord:Ordering[T]):T = if(ord.compare(t1,t2)&gt;0) t1 else t2 helper&#125; 2.形变在java中 泛型不存在继承关系，即不存在多态但是在scala中 允许泛型之间有联系，使用型变来描述这种联系 协变：C[+T]：如果A是B的子类，那么C[A]是C[B]的子类。也就是被参数化类型的泛化方向与参数类型的方向是一致的，所以称为协变。 逆变：C[-T]：如果A是B的子类，那么C[B]是C[A]的子类。也就是被参数化类型的泛化方向与参数类型的方向是相反的，所以称为逆变。 不变：C[T]：无论A和B是什么关系，C[A]和C[B]没有从属关系 1234567891011121314class Engineerclass Expert extends Engineerclass Meeting[+T]//可以传入T或T的子类object MeetingTest&#123; def participateMeeting(meeting:Meeting[Expert])= println(&quot;welcome&quot;) def main(args: Array[String]): Unit = &#123; val m = new Meeting[Expert] val m1 = new Meeting[Engineer] participateMeeting(m) participateMeeting(m1) &#125;&#125; 3.类型约束类型约束： 放到隐式关键字后面 ​ A =:= T A是否等于T A &lt;:&lt; T A是否为T的子类 A &lt;%&lt; T A是否可以隐式转化成T 注意：使用类型约束需要使用implicit关键字 12]]></content>
      <categories>
        <category>scala</category>
      </categories>
      <tags>
        <tag>scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[virtualbox虚拟机ubuntu无法上网]]></title>
    <url>%2F2019%2F07%2F21%2Fvirtualbox%E8%99%9A%E6%8B%9F%E6%9C%BAubuntu%E6%97%A0%E6%B3%95%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[今天想要在虚拟机上安装redis，结果尽然发现虚拟机不能上网，根据网上的一顿操作，差点把自己心态搞崩，不过，最后还是集百家智慧，重要搞定了。 打开虚拟机的时候，果断 sudo apt-get install redis-server,结果发现不能上网, ping www.baidu.com ,发现ping不同，顿时感觉不好了，于是去度娘搜了一波，过程极度坎坷，好在最后成功解决。 解决方法,配置双网卡，一个Nat模式，用来连接外网，一个host-only模式，用来虚拟机与主机之间进行交换。 一.配置Nat模式 全局设定－－》网络－－》添加 设置–&gt;网络－－》nat模式（网卡一） 二.设置host-only模式（网卡二） 点击管理–&gt;主机网络管理器–&gt; 设置–&gt;网络－－》host-only 配置文件 sudo vi /etc/network/interfaces 123456789101112131415# The loopback network interfaceauto loiface lo inet loopback# The primary network interface 网卡一auto enp0s3iface enp0s3 inet dhcpup route add default gw 10.0.2.2 dev enp0s3 #网卡一默认走网关10.0.2.2 # The primary network interface2 网卡二auto enp0s8iface enp0s8 inet staticaddress 192.168.56.16gateway 192.168.56.1netmask 255.255.255.0broadcast 192.168.56.255 最后都可以ping通 ping www.baidu.com ping主机]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>ubuntu linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop]]></title>
    <url>%2F2019%2F07%2F13%2Fhadoop%2F</url>
    <content type="text"><![CDATA[1.hadoop的优势 高可靠：维护多个副本，数据不易丢失 高扩展：集群方便添加节点 高效：MapReduce并行处理程序 高容错：提交任务失败会再次提交 2.hadoop的组成 MapReduce:负责计算 Map阶段并行处理输入的数据 Reduce阶段对Map结果进行汇总 Yarn:资源调度 Yarn主要由ResourceManager,NodeManager,ApplicationMaster和Container等组件构成。 ResourceManager：接受客户端的请求，监控NodeManager,整个集群的资源调度和分配 NodeManager:管理单个节点的资源，处理来自ReourceManager和ApplicationMaster的命令 ApplicationMaster：负责数据的切分，为应用程序申请并且分配资源 Container:封装资源，如cpu,内存 Hdfs:数据存储 hdfs主要由NameNode,DataNode,SecondaryNameNode组成 NameNode：负责处理客户端的请求，储存元数据 DataNode：负责在本地文件系统中存储文件数据块 SecondaryNameNode:保存NameNode元数据镜像fsimage，定时到NameNode获取edit logs,并合并到fsimage,一旦有了新的fsimage，SecondaryNameNode将其拷贝到NameNode,NameNode在下次重新启动的过程中会加载这个fsimage,减少重启时间。 Commons:辅助工具 三.Hdfs3.1hdfs的读写流程 HDFS写文件流程 ​ １．客户端向NameNode发送写数据的请求 ２．NameNode响应客户端的请求 ３．客户端向NameNode请求发送第一个block的数据，NameNode返回可以写数据的DataNode。 ４．客户端根据就近原则，选择DataNode建立连接，第一个DataNode建立连接后，会与其它的DataNode建立连接，建立一个成功的数据传输通道 ５．客户端开始向第一个DataNode上传Block，以packet为单位，第一个DataNode收到packet后，会依次传递给后面的DataNode. ６．第一个block上传完毕后，会上传第二个。按照这个步骤依次上传 HDFS读文件流程 客户端发送读文件的请求 NameNode返回目标文件的元信息（目标文件的块，块所在的位置） 客户端向最近的DataNode发送读数据的请求 DataNode开始向客户端传输数据，客户端以Packet为单位接受 NameNode工作机制：启动时加载fsimage和Editlogs到内存，当客户端请求对元数据进行增删改的时候，NameNode将操作记录到Editlogs中，然后在内存中对元数据进行增删改 Secondary NameNode：会定时的向NameNode发送CheckPoint请求，将Editlogs和fsimage拷贝到Secondary NameNode进行合并，并且将新的fsimage返回给NameNode。 DataNode工作机制：一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。DataNode会与Name进行心跳检测，每三秒一次，心跳返回结果带有NameNode给DataNode命令，如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。 3.2 副本节点选择 假设复制因为３ 第一个副本：存储在客户端所在的节点(如果客户端不在集群中，则随机选择一个节点） 第二个副本：存储在与客户端相同机架的不同节点 第三个副本：存储在不同的机架上 3.3.数据完整性hdfs写入数据的时候会计算校验和，读数据的时候也会计算校验和，客户端读数据的时候也会计算校验和除了读写之外，datanode还跑着一个后台进程（DataBlockScanner）来定期校验存在在它上面的block，因为除了读写过程中会产生数据错误以外，硬件本身也会产生数据错误。 如果客户端发现有block坏掉，会恢复这个块 1. 客户端在跑出ChecksumException之前，会向NameNode报告错误的block以及它所在的DataNode, 2. NameNode将这个block标记已损坏，NameNode就不会再指向这个block, 3.namenode会把一个好的block复制到datanode 4.NameNode删除坏的block 四.MapReduce4.1.MapReduce进程 一个完整的MapReduce程序在分布式运行时有三类实例进程 1)MrAppMaster:负责整个程序运行过程的资源调度 2)MapTask:负责Map阶段的整个数据处理流程 3)ReduceTask:负责Reduce阶段的整个数据处理流程 4.2.文件切片​ １）一个job的MapTask的个数，取决于文件切片的数量 ２）一个切片需要一个MapTask ３）默认情况下，切片大小为blockSize计公式（Math.max(minSize, Math.min(maxSize, blockSize)); ４）切片时不考虑整体数据，针对每一个文件进行切割 默认的TextInputFormat对于每个文件进行切片，不管文件多小。 CombineTextInputFormat：用于小文件过多的场景，它可以从逻辑上将小文件划分到一个切片中 1).需要设置虚拟存储切片的最大值：CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m 2).切片机制 先按照虚拟存储进行逻辑分块，小于虚拟存储分成一块，大于虚拟存储小于虚拟存储×２的，分两块，对半分。切片的时候小于虚拟存储块的与下一块合并，大于虚拟存储的单独作为一个块 测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为： 1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M） 最终会形成3个切片，大小分别为： （1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M KeyValueTextInputFormat:也是按行读取数据，键值对&lt;Text,Text&gt;,默认按tab键分割，tab键前面的为key，后面的为value。可以通过conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPARATOR,”\t”)设置 NLineInputFormat:不在按block进行分片，而是按照指定的行数进行分片,NLineInputFormat.setNumLinesPerSplit(job,num); 4.3.Shuffle机制Shuffle实在map方法之后，reduce方法之前。如图: 1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中 2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件 3）多个溢出文件会被合并成大的溢出文件 4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序 5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据 6）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序） 7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法） 4.4.Partition 分区将统计结果按照条件输出到不同的文件 默认分区 123public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125; 自定义 Partitioner 步骤 自定义类继承 Partitioner， 重写 getPartition()方法 12345678910111213141516171819public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123;@Overridepublic int getPartition(Text key, FlowBean value, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); int partition = 4; // 2 判断是哪个省 if (&quot;136&quot;.equals(preNum)) &#123; partition = 0; &#125;else if (&quot;137&quot;.equals(preNum)) &#123; partition = 1; &#125;else if (&quot;138&quot;.equals(preNum)) &#123; partition = 2; &#125;else if (&quot;139&quot;.equals(preNum)) &#123; partition = 3; &#125; return partition; &#125;&#125; 在 job 驱动中，设置自定义 partitioner： 1job.setPartitionerClass(CustomPartitioner.class); 自定义 partition 后，要根据自定义 partitioner 的逻辑设置相应数量的 reduce task 1job.setNumReduceTasks(5); 注意: 如果reduceTask的个数大于分区数，则会生成几个空文件 如果reduceTask的个数小于分区数并且大于一，则会抛出IOException 如果reduceTask的个数等于１，生成一个文件 分区号必须从０开始 4.5排序 部分排序 MapReduce 根据输入记录的键对数据集排序。 保证输出的每个文件内部排序。 全排序 何用 Hadoop 产生一个全局排序的文件？最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低， 因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce 所提供的并行架构 辅助排序( GroupingComparator 分组)： 在Reduce端对键进行排序。应用于：在接受的key为bean对象时，想让一个或几个字段相同的key进入到同一个reduce方法时，可以采用分组排序。 4.6 Combiner 合并combiner 是 MR 程序中 Mapper 和 Reducer 之外的一种组件。combiner 组件的父类就是 Reducer。combiner 和 reducer 的区别在于运行的位置：Combiner 是在每一个 maptask 所在的节点运行;Reducer 是接收全局所有 Mapper 的输出结果；Combiner 的意义就是对每一个 maptask 的输出进行局部汇总，以减小网络传输量。 combiner 能够应用的前提是不能影响最终的业务逻辑，而且， combiner 的输出 kv 应该跟 reducer 的输入 kv 类型要对应起来。 比如：文件中有两行数据 ３ ５ ７ ２ ６ 要对他们求平均值，如果使用 combiner进行局部汇总，最终结果为 Combiner :(3+5+7)/2=5 ,(2+6)/2=4 reducer:(5+4)/2 不使用combiner，结果为 (3+5+7+2+6)/5=23/5 可以看到结果不相等，所有如果要使用combiner,前提是不影响业务逻辑 自定义combiner: （ 1）自定义一个 combiner 继承 Reducer，重写 reduce 方法 1234567891011121314public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text,IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; // 1 汇总操作 int count = 0; for(IntWritable v :values)&#123; count = v.get(); &#125; // 2 写出 context.write(key, new IntWritable(count)); &#125;&#125; （ 2）在 job 驱动类中设置 1job.setCombinerClass(WordcountCombiner.class); 4.7.MapTask,ReduceTask工作机制MapTask: １.reader阶段，通过RecoderReader从InputSplit中读取key/value, 2.map阶段，将读取的key/value经过map处理形成新的key/value, 3.Collector收集阶段：将形成的新的key/value输出到环形缓冲区,进行分区和排序 4.溢写：当环形缓冲区的数据到达一定程度后，会将数据输出到本地磁盘，形成一个临时文件，在溢写的过程中也会进行分区和排序，当溢写的文件数量到达一定程度，会对这些文件进行合并。(sortAndSpill()) 5.合并：当所有数据处理完成后，MapTask会将所有生成的临时文件进行合并，合并过程中是以分区为单位进行，以确保每个分区内部有序。 this.mergeParts(); ReduceTask: ​ 1.Copy阶段：ReduceTask会从不同的MapTask中拷贝对应分区的数据，放在内存中，如果数据大小超过阈值，会将数据溢写到磁盘。 2.Merge：ReduceTask将拷贝来的数据进行合并排序，防止内存中的文件过多 3.Sort:基于key的排序，确保将相同的key的数据聚集在一起 4.分组排序：将相同的key的数输入到reduce中 5.reduce：reduce函数，进行数据的处理，处理后的数据写到hdfs中（调用outputformat) 4.8.join操作 Map端的join 将小表置于内存中， 对于大表的一个纪录我们在内存中查找即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class MapJoin &#123; public static class MapJoinMapper extends Mapper&lt;Object, Text, Text, Text&gt; &#123; public Map&lt;String, String&gt; joinData = new HashMap(); //执行连接操作 public void map(Object key, Text value, Context context) throws IOException, InterruptedException&#123; String[] values = value.toString().split(&quot;\t&quot;); context.write(new Text(joinData.get(values[0])), value); &#125; //加载小表 public void setup(Context context) throws IOException, InterruptedException&#123; Path[] path = DistributedCache.getLocalCacheFiles(context.getConfiguration()); BufferedReader reader = new BufferedReader(new FileReader(path[0].toString())); String str = null; while((str = reader.readLine()) != null) &#123; String[] s = str.split(&quot;\t&quot;); joinData.put(s[0], s[1]); &#125; &#125; &#125; public static class MapJoinReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123; public void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException&#123; int ci = 0; double total = 0.0; for(Text val : values) &#123; ci ++; String[] v = val.toString().split(&quot;\t&quot;); total += Float.parseFloat(v[1]); &#125; String str = String.format(&quot;%d\t%f&quot;, ci, total); context.write(key, new Text(str)); &#125; &#125; public static void main(String[] args) throws Exception&#123; Configuration conf = new Configuration(); DistributedCache.addCacheFile(new Path(args[1]).toUri(), conf); Job job = new Job(conf, &quot;MapJoin&quot;); //设置相关类 job.setJarByClass(MapJoin.class); job.setMapperClass(MapJoinMapper.class); job.setReducerClass(MapJoinReducer.class); //设置map输出格式 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //设置输入输出文件 FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[2])); //等待作业执行完毕 System.exit(job.waitForCompletion(true)?0:1); &#125;&#125; Reduce端的join 将Map端输出的key相同即可reduce端进行连接，在Shuffle需要处理的数据太多，会导致效率很低、 五.YarnYarn主要由ResourceManager,NodeManager,ApplicationMaster和Container等组件构成。ReourceManager:处理客户端请求，监控NodeManager,启动或监控ApplicationMaster,资源的调度和分配NodeManager:管理单个节点的资源，处理来自ReourceManager命令,处理来自ApplicationMaster的命令ApplicationMaster:负责数据的切分，为应用程序申请并且分配资源Container:封装资源，如cpu,内存 yarn运行机制 1).Client调用job.waitForCompletion方法，向整个集群提交MR作业 2).Client向RM请求job 3).RM返回job资源提交的路径和id 4).Client提交资源(文件分片信息，配置文件信息) 5).资源提交完毕后，向RM申请MRAppMaster 6).RM收到请求，初始化Task,将job添加到容量调度器（任务调度器） 7).空闲的NM领取到该job 8).NM创建一个Container,并产生MRAppMaster,下载client提交的资源 9).MRAppMaster向RM申请多个MapTask. 10).RM将MapTask分配给其它NM 11).MapTask开始运行，进行分区排序等操作 12).MRAppMaster在所有的MapTask任务完成后，向RM申请ReduceTask 13)ReduceTask获取MapTask分区的数据 14)程序运行完毕，MR会自动向RM申请注销自己]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[交换排序]]></title>
    <url>%2F2019%2F07%2F10%2F%E4%BA%A4%E6%8D%A2%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[交换排序的基本思想根据序列中两个元素的比较结果来对换这两个记录在序列中的位置，也就是说，将键值较大的记录向序列的尾部移动，键值较小的记录向序列的前部移动。 冒泡排序:比较两个相邻的数，如果前面的数大于后面的数，则将这两个数交换位置。第一次遍历后，最大的数会被放到数组的最后位置，即array[length - 1]。第二次遍历时跳过最后一个元素，因为该元素通过第一次遍历已经确定是最大值。持续每次对越来越少的元素重复上面的步骤，直到没有任何一对数字需要较。 1234567891011public void bubleSort(int[] arr) &#123; for(int i=0;i&lt;arr.length-1;i++) &#123; for(int j=0;j&lt;arr.length-1-i;j++) &#123; if(arr[j]&gt;arr[j+1]) &#123; int tmp = arr[j]; arr[j] = arr[j+1]; arr[j+1] = tmp; &#125; &#125; &#125;&#125; 快速排序基本思想： 先从数列中取出一个数作为基准数 分区过程，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边 再对左右区间重复第二步，直到各区间只有一个数 123456789101112131415161718192021222324public void quickSort(int arr[],int left,int right) &#123; if(right&gt;left) &#123; int index = getIndex(arr,left,right); quickSort(arr,left,index-1); quickSort(arr,index+1,right); &#125; &#125; public int getIndex(int[] arr,int left,int right) &#123; int data = arr[left]; while(left&lt;right) &#123; while(left&lt;right&amp;&amp;arr[left]&lt;data) &#123; left++; &#125; arr[right] = arr[left]; while(left&lt;right&amp;&amp;arr[right]&gt;data) &#123; right--; &#125; arr[left] = arr[right]; &#125; arr[left] = data; return left; &#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive]]></title>
    <url>%2F2019%2F07%2F10%2Fhive%2F</url>
    <content type="text"><![CDATA[1.hive的基本概念Hive是基于hadoop的一个数据仓库工具，可以将结构化数据文件映射成一张表，并提供类sql查询功能 本质：将sql语句转化成MapReduce程序 hive处理的数据存储在hdfs hive分析数据底层的实现是MapReduce 执行程序运行在yarn上 2.hive的架构 1）用户接口： Client CLI（ hive shell）、 JDBC/ODBC(java 访问 hive) 2）元数据： Metastore 元数据包括：表名、表所属的数据库（默认是 default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； ３）驱动器： Driver （ 1）解析器（ SQL Parser）：将 SQL 字符串转换成抽象语法树 AST，这一步一般都用第三方工具库完成，比如 antlr；对 AST 进行语法分析，比如表是否存在、字段是否存在、 SQL 语义是否有误。 （ 2）编译器（ Physical Plan）：将 AST 编译生成逻辑执行计划 ​ （ 3）优化器（ Query Optimizer）：对逻辑执行计划进行优化。 ​ （ 4）执行器（ Execution）：把逻辑执行计划转换成可以运行的物理计划。对于 Hive 来说，就是 MR/Spark。 3.hive安装1.derby形式首先将压缩包解压，并且配置环境变量。 建立一个文件加夹用来存储元数据，然后初始化元数据库 1234mkdir /home/master/derby_datacd /home/master/derby_data//执行初始化操作schematool -dbType derby -initSchema 2.mysql形式 首先将压缩包解压，并且配置环境变量。 进入到conf/下，修改配置文件 12345678910111213141516171819202122232425vi hive-site.xml&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;!--指定mysql--&gt; &lt;value&gt;jdbc:mysql://192.168.43.22:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 将mysql驱动包复制到hive/lib下 在mysql中创建用户hive,密码为hive(与配置文件一致)，授予该用户权限，并且打开mysql的远程访问权限 12345create user &apos;hive&apos;@&apos;192.168.43.22&apos; identified by &apos;hive&apos;;grant all privileges on hive.* to &apos;hive&apos;@&apos;%&apos; identified by &apos;hive&apos;;将/etc/mysql/my.conf 下面一句注释# bindaddress localhost 执行初始化命令 1schematool -dbType mysql -initSchema 4.检查mysql中是否存在hive元数据 ４.hive的数据类型基本数据类型： 集合数据类型： Array(集合)： 1234567891011create table tab_array (a array&lt;int&gt;,b array&lt;string&gt;)row format delimitedfields terminated by &apos;\t&apos;collection items terminated by &apos;,&apos;;vi data_array# tab_array的内容1,2,3 jack,Tom,lucy//将数据加载到tab_array中load data local inpath &apos;/homehyy/master/data_array&apos; into table tab_array;//查询select a[2],b[1] from tab_array; Map(键值对): 12345678910create table tab_map (name string,info map&lt;string,string&gt;)row format delimitedfields terminated by &apos;\t&apos;collection items terminated by &apos;,&apos;map keys terminated by &apos;:&apos;;vi data_mapzhangsan name:zhangsan,age:18,gender:maleload data local inpath &apos;/home/hdfs/data_map&apos; into table tab_map;select info[&apos;name&apos;] from tab_map; struct(类似java的对象) 123456789create table tab_struct(name string,info struct&lt;age:int,tel:string,salary:double&gt;)row format delimitedfields terminated by &apos;\t&apos;collection items terminated by &apos;,&apos;;vi data_structzhangsan 18,189,22.3load data local inpath &apos;/home/hdfs/data_struct&apos; into table tab_struct;select info.age,info.tel from tab_struct; 综合案例: 123456789101112create table test(name string,friends array&lt;string&gt;,children map&lt;string, int&gt;,address struct&lt;street:string, city:string&gt;)row format delimited fields terminated by &apos;,&apos; --指定列分隔符为逗号collection items terminated by &apos;_&apos; --指定集合之间的分隔符为 _map keys terminated by &apos;:&apos; --指定map中key与value的分割符# 将下面的数据导入到表中jack,friend1_friend2,son1:18_son2:20,nanc_qingshanhulucy,friend3_friend4,son1:18_son2:20,kunsan_bacheng 5.DDL数据库定义语言数据库的操作创建数据库： 1234create database hsj_test;create database if not exists hsj_test;# 创建数据库并指定在HDFS上的位置create database hsj_test3 location &apos;/hsj_test3&apos; 修改数据库 123alter database hsj_test3 set dbproperties(&apos;createtime&apos;=&apos;20190710&apos;);# 查看修改desc database extended hsj_test3; 查询数据库 12show databases;show databases like &apos;hsj_*&apos;; 显示数据库信息 123desc database hsj_test3;# 显示详细信息desc database extended hsj_test3 删除数据库： 数据库为空的时候才能删除否则报错,Database hsj_test3 is not empty. One or more tables exist 1drop database hsj_test3; 数据库为空的时候才能删除 表操作创建表 内部表:将数据存储在hive.metastore.warehouse.dir(例如， /user/hive/warehouse)的子目录下，删除表时，会将hdfs上的数据一同删除 1234567891011create table inner_table(id int,name string)row format delimited fields terminated by &apos;,&apos; stored as textfile; location &apos;/hdfspath&apos; 指定加载数据的路径＃字段说明row format delimited fields terminated by &apos;,&apos; // 指定列之间以,分割stored as textfile; 指定文件存储形式常用文件类型:SEQUENCEFILE（二进制序列文件）TEXTFILE（文本）RCFILE（列式存储格式文件） 外部表:删除表并不会删除hdfs上的数据 12create external table if not exist ext_table(id int,name string)row format delimited fields terminated by &apos;,&apos;; 分区表:根据业务编码、日期、其他类型等维度创建分区表，在一个表对应的目录下，一个分区对应一个目录. 12345678910# 创建一个以月份为分区的表create table partition_table(id int,name string)partitioned by (month string)row format delimited fields terminated by &apos;,&apos;;load data local inpath &apos;/home/hdfs/order_data&apos; into table partition_tablepartition(month=&apos;8&apos;);load data local inpath &apos;/home/hdfs/order_data&apos; into table partition_tablepartition(month=&apos;７&apos;);去hdfs中查看，发现在该表对应的目录下，每个月份对应了一个目录 分区表的查询 123456＃ 单分区查询select * from t_order where month=&apos;7&apos;;＃ 多分区联合查询select * from t_order where month=&apos;7&apos;union select * from t_order where month=&apos;8&apos;; 增加分区 12alter table t_order add partition(month=&apos;10&apos;);alter table t_order add partition(month=&apos;12&apos;) partition(month=&apos;11&apos;); 删除分区 12alter table t_order drop partition(month=&apos;10&apos;);alter table t_order drop partition(month=&apos;11&apos;),partition(month=&apos;12&apos;); 查看分区表有多少分区 1show partitions t_order; 查看分区表结构 1desc formatted t_order; 创建二级分区表 1234567create table t_order_2(id int,name string,cost double)partitioned by(month string,day string)row format delimited fields terminated by &apos;,&apos;;load data local inpath &apos;/home/master/phone_data&apos; into table t_order_2 partition(month=&apos;1&apos;,day=&apos;20&apos;);load data local inpath &apos;/home/master/phone_data&apos; into table t_order_2 partition(month=&apos;2&apos;,day=&apos;20&apos;);load data local inpath &apos;/home/master/phone_data&apos; into table t_order_2 partition(month=&apos;1&apos;,day=&apos;15&apos;); 将数据直接上传到分区目录上，让分区表和数据产生关联的两种方式 方式一：先上传后修复 1234567上传数据hdfs dfs -mkdir -p /user/hive/warehouse/hsj_hve.db/t_order_2/month=1/day=2hdfs dfs -put phone_data /user/hive/warehouse/hsj_hve.db/t_order_2/month=1/day=2/select * from t_order_2 where month=&apos;1&apos; and day=&apos;2&apos;; 发现查询不到修复数据再查询 msck repair table t_order_2; 方式二:上传数据后添加分区 123hdfs dfs -mkdir -p /user/hive/warehouse/hsj_hve.db/t_order_2/month=2/day=2hdfs dfs -put phone_data /user/hive/warehouse/hsj_hve.db/t_order_2/month=2/day=2/alter table add partition(month=&apos;2&apos;,day=&apos;2&apos;); 桶表：将大表进行哈希散列抽样存储，方便做数据和代码验证。在表对应的目录下，将源文件拆分成N个小文件。桶表中的数据，只能从其他表中用子查询进行插入 123456789101112create table t_phone_bucket(id int,name string ,price string)clustered by(id) into 3 bucketsrow format delimited fields terminated by &apos;,&apos;;set hive.enforce.bucketing=true;insert into table t_phone_bucket select * from t_phone;# 桶抽样查询select * from t_phone_bucket tablesample(bucket 3 out of 3 on id);语法： TABLESAMPLE(BUCKET x OUT OF y) 数据库抽样查询 1select * from stu tablesample(0.1 percent) ; 修改表重命名表:语法 ALTER TABLE table_name RENAME TO new_table_name 1alter table student rename to stu; 修改列 123456# 增加列alter table stu add columns(age int);# 替换列 膝盖表中所有字段 注意类型要匹配alter table stu replace columns(name string,gender string,age int);# 修改列alter table stu change column age age string ## 六.DML数据操作 数据加载： 1load data [local] inpath &apos;/opt/module/datas/student.txt&apos; [overwrite] into table student[partition (partcol1=val1,…)]; 数据插入 1234567插入选择的数据insert overwrite table t_phone select * from t_phone_back;基本插入数据insert into table p_order partition(month=&apos;10&apos;) values(1,&apos;wangwu&apos;,900.1);基本模式插入数据insert overwrite table p_order partition(month=&apos;11&apos;)select id, name from p_order where month=&apos;7&apos;; 查询语句中创建并加载数据 1create table if not exists stu as select id ,name from student; 将数据导出到本地 123insert overwrite local directory &apos;/home/master/hive_export&apos;row format delimited fields terminated by &apos;\t&apos;select * from t_order; 将数据导出到hdfs 123insert overwrite directory &apos;/home/master/hive_export&apos;row format delimited fields terminated by &apos;\t&apos;select * from t_order; 七查询group by语句：GROUP BY 语句通常会和聚合函数一起使用，按照一个或者多个列队结果进行分组，然后对每个组执行聚合操作 计算 emp 表每个部门的平均工资 1select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno; 计算 emp 每个部门中每个岗位的最高薪水 12select t.deptno, t.job, max(t.sal) max_sal from emp t group by t.deptno,t.job; having:（ 1） where 针对表中的列发挥作用，查询数据； having 针对查询结果中的列发挥作用，筛选数据。（ 2） where 后面不能写分组函数，而 having 后面可以使用分组函数。（ 3） having 只用于 group by 分组统计语句 求每个部门的平均薪水大于 2000 的部门 12 select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal &gt;2000 join等值连接根据员工表和部门表中的部门编号相等，查询员工编号、 员工名称和部门编号； 12select e.empno, e.ename, d.deptno, d.dname from emp e join dept d one.deptno = d.deptno; 内连接:只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来 12select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno =d.deptno; 左外连接:JOIN 操作符左边表中符合 WHERE 子句的所有记录将会被返回 12 select e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno =d.deptno; 右外连接JOIN 操作符右边表中符合 WHERE 子句的所有记录将会被返回 12select e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno =d.deptno; 满外连接将会返回所有表中符合 WHERE 语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用 NULL 值替代 12select e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno =d.deptno; 排序全局排序 order byASC ：升序（默认) DESC:降序 查询员工信息按工资升序排列 1select * from emp order by sal; 多个列排序 按照部门和工资升序排序 1select ename, deptno, sal from emp order by deptno, sal ; 内排序 sort by1234567设置 reduce 个数set mapreduce.job.reduces=3;根据部门编号降序查看员工信息select * from emp sort by empno desc;将查询结果导入到文件中（按照部门编号降序排序insert overwrite local directory &apos;/opt/module/datas/sortby-result&apos; select *from emp sort by deptno desc; 分区排序 Distribute By类似 MR 中 partition，进行分区，结合 sort by 使用 123set mapreduce.job.reduces=3;insert overwrite local directory &apos;/opt/module/datas/distribute-result&apos; select *from emp distribute by deptno sort by empno desc; Cluster By:当 distribute by 和 sorts by 字段相同时，可以使用 cluster by 方式。 cluster by 除了具有 distribute by 的功能外还兼具 sort by 的功能。但是排序只能是倒序排序， 不能指定排序规则为 ASC 或者 DESC。 123以下两种写法等价select * emp cluster by deptno;select * from emp distribute by deptno sort by deptno; 行转列123456789101112131415161718192021＃ 数据大海 射手座 A宋宋 白羊座 B猪八戒 白羊座 A凤姐 射手座 A孙悟空 白羊座 A# 表create table persion_info(name string,constellation string,blood_type string)row format delimited fields terminated by &apos;\t&apos;;# 转成格式射手座,A 大海|凤姐白羊座,A 猪八戒|孙悟空白羊座,B 宋宋语句select t.base,concat_ws(&apos;|&apos;,collect_set(t.name)) from (select concat(constellation,&apos;,&apos;,blood_type) base,name from persion_info)t group by t.base; 函数说明： concat : 连接函数，用于将多个列或者字符串连接起来 concat(col1,col2,) concat_ws:一种特殊的concat,有多个参数，第一个参数指定为连接符concat(splitor,col1,col2)，只能用来连接字符串类型或者字符串数组的列 collect_set:只接受基本类型数据，主要是将某字段的值去重汇总，产生array类型字段 列转行1234567891011121314151617181920212223242526# 数据《疑犯追踪》 悬疑,动作,科幻,剧情《 Lie to me》 悬疑,警匪,动作,心理,剧情《战狼 2》 战争,动作,灾难# 表create table movie_info( movie string, category array&lt;string&gt; ) row format delimited fields terminated by &quot;\t&quot; collection items terminated by &quot;,&quot;;转换格式：《疑犯追踪》 悬疑《疑犯追踪》 动作《疑犯追踪》 科幻《疑犯追踪》 剧情《 Lie to me》 悬疑《 Lie to me》 警匪《 Lie to me》 动作《 Lie to me》 心理《 Lie to me》 剧情《战狼 2》 战争《战狼 2》 动作《战狼 2》 灾难＃ 查询语句select movie ,category_name from movie_info lateral view explode(category) table_tmp as category_name; 函数说明： LATERAL VIEW: 用法:LATERAL VIEW udtf(expression) tableAlias AS columnAlias解释:用于和 split, explode 等 UDTF 一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。 八.窗口函数OVER():指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化CURRENT ROW:当前行n PRECEDING :往前 n 行数据n FOLLOWING :往后 n 行数据UNBOUNDED:起点， UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDEDFOLLOWING 表示到后面的终点 LAG(col,n):往前第 n 行数据LEAD(col,n):往后第 n 行数据NTILE(n):把有序分区中的行分发到指定数据的组中，各个组有编号，编号从 1 开始，对于每一行， NTILE 返回此行所属的组的编号。 注意： n 必须为 int 类型。 数据： 1234567891011121314151617181920jack,2017-01-01,10tony,2017-01-02,15jack,2017-02-03,23tony,2017-01-04,29jack,2017-01-05,46jack,2017-04-06,42tony,2017-01-07,50jack,2017-01-08,55mart,2017-04-08,62mart,2017-04-09,68neil,2017-05-10,12mart,2017-04-11,75neil,2017-06-12,80mart,2017-04-13,94# 表 create table business( name string, orderdate string, cost int) row format delimited fields terminated by &apos;,&apos;; 1.查询在 2017 年 4 月份购买过的顾客及总人数 1select name,count(*) over() from business where substring(orderdate,0,7)=&apos;2017-04&apos; group by name; 2.查询顾客的购买明细及月购买总额 1select *,sum(cost) over(distribute by month(orderdate); 3.查询顾客的购买明细,将 cost 按照日期进行累加 123select *,sum(cost) over(partition by name order by orderdate) from business;# 分区限定了窗口的界限 排序限定了窗口的大小 4.查看顾客上次的购买时间 1select * ,lag(orderdate,1) over(distribute by name order by orderdate) from business; 5.查询前 20%时间的订单信息 123select * from(select name,orderdate,cost,ntile(5) over(order by orderdate) id from business)twhere t.id = 1; Rank函数需要配合窗口函数over()使用 Rank() : 排序，如果有相同会重复，总数不会减少 ​ 假设 小明考试考了100,小刚考试考了100,小红考试考了99,那小明小刚并列第一（重复），小红第三（总数不会减少） dense_Rank():排序，如果有相同会重复，总数会减少. ​ 还是上面那个例子，明小刚并列第一（重复），小红第二（总数会减少） row_number():按顺序排序 九.函数系统内置函数 123456查看系统内置函数show functions;显示内置函数用法desc function upper;详细显示内置函数用法desc function extended upped 自定义函数 UDF（user-defined function）：一进一出 UDAF（ User-Defined Aggregation Function):聚集函数 多进一出 UDTF（ User-Defined Table-Generating Functions）：一进多出 实现步骤 继承org.apache.hadoop.hive.ql.UDF,实现evaluate方法 123456789101112131415public class MyUdf extends UDF &#123; public String evaluate(String phone)&#123; switch (phone.substring(0,3))&#123; case &quot;187&quot;: return &quot;上海&quot;; case &quot;159&quot;: return &quot;北京&quot;; case &quot;138&quot;: return &quot;南昌&quot;; default: return &quot;未知&quot;; &#125; &#125;&#125; 打成jar包，上传到hive所在的机器 在hive中创建一个函数，和jar中的自定义类建立 12add jar /home/master/xxxx.jarcreate [temporary] function getArea(col) as com.hsj.udf.MyUDF 使用 1select id,name tel,getArea(tel) from t_student; 十.设置本地模式1234567set hive.exec.mode.local.auto=true; //设置 local mr 的最大输入数据量，当输入数据量小于这个值时采用 local mr 的方式，默认为 134217728，即 128Mset hive.exec.mode.local.auto.inputbytes.max=50000000;//设置 local mr 的最大输入文件个数，当输入文件个数小于这个值时采用 local mr 的方式，默认为 4set hive.exec.mode.local.auto.input.files.max=10; 十一jdbc连接hive 修改配置文件 hdfs-site.xml core-site.xml 1234567891011121314#hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;#core-site.xml&lt;property&gt; &lt;name&gt;hadoop.proxyuser.master.hosts&lt;/name&gt;&lt;!--允许所有主机以master登录--&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.master.groups&lt;/name&gt;&lt;!--允许任意组master登录--&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 2.jdbc api(注意先将驱动包导入) 1234567891011121314151617181920212223242526public class HiveJdbcTest &#123; private static String driver = &quot;org.apache.hive.jdbc.HiveDriver&quot;; private static String url = &quot;jdbc:hive2://192.168.43.58:10000/hsj_test&quot;; private static String user =&quot;master&quot;; private static String passwd=&quot;master&quot;; public static void main(String[] args) throws Exception &#123; //注册驱动 匹配到正确的驱动 Class.forName(driver); //获取连接 Connection connection = DriverManager.getConnection(url, user, passwd); System.out.println(connection); Statement statement = connection.createStatement(); String sql = &quot;select * from business&quot;; ResultSet resultSet = statement.executeQuery(sql); while (resultSet.next())&#123; String per = resultSet.getString(&quot;name&quot;); String info = resultSet.getString(&quot;orderdate&quot;); int extrainfo = resultSet.getInt(&quot;cost&quot;); System.out.println(per+info+extrainfo); &#125; &#125;&#125; 十二.hive结合hbase hbase 中创建表 12345 create &apos;person&apos;,&#123;NAME =&gt; &apos;f1&apos;,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; &apos;f2&apos;,VERSIONS =&gt; 1&#125;,&#123;NAME =&gt; &apos;f3&apos;,VERSIONS =&gt; 1&#125;put &apos;person&apos;,&apos;1001&apos;,&apos;f1:name&apos;,&apos;jack&apos;put &apos;person&apos;,&apos;1001&apos;,&apos;f2:age&apos;,&apos;18&apos;put &apos;person&apos;,&apos;1002&apos;,&apos;f1:name&apos;,&apos;jack&apos;put &apos;person&apos;,&apos;1003&apos;,&apos;f3:position&apos;,&apos;ceo 打开hive 1234567891011SET hbase.zookeeper.quorum=master:2181;SET zookeeper.znode.parent=/hbase;CREATE EXTERNAL TABLE person (rowkey string,f1 map&lt;STRING,STRING&gt;,f2 map&lt;STRING,STRING&gt;,f3 map&lt;STRING,STRING&gt;) STORED BY &apos;org.apache.hadoop.hive.hbase.HBaseStorageHandler&apos;WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,f1:,f2:,f3:&quot;)TBLPROPERTIES (&quot;hbase.table.name&quot; = &quot;person&quot;);]]></content>
      <categories>
        <category>hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[插入排序]]></title>
    <url>%2F2019%2F07%2F10%2F%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[一.直接插入排序算法思想：当插入第i个元素的时候，前面的V[0],…,V[i-1]等i-1个 元素已经有序。这时，将第i个元素与前i-1个元素V[i-1]，…，V[0]依次比较，找到插入位置即将V[i]插入，同时原来位置上的元素向后顺移。在这里，插入位置的查找是顺序查找。直接插入排序是一种稳定的排序算法。 1234567891011121314public void directInsertSort(int[] arr) &#123; if(arr.length == 0 || arr == null) &#123; return; &#125; for(int i=1;i&lt;arr.length;i++) &#123; for(int j=i-1;j&gt;=0;j--) &#123; if(arr[j]&gt;arr[j+1]) &#123; int tmp = arr[j+1]; arr[j+1] = arr[j]; arr[j] = tmp; &#125; &#125; &#125; &#125; 二.希尔排序算法思想：希尔排序也是一种插入排序，只不过希尔排序是根据步长进行比较，不是与前面一个一个元素进行比较。确定步长step，将每个步长为step的元素进行比较,使用插入排序，让该序列有序，之后步长递减，将每个步长为step的元素进行比较，使用插入排序，让该序列有序，直到步长为１． 123456789101112131415public void shellSort(int[] arr) &#123; //确定每次比较的步长 for(int step = arr.length/2;step&gt;0;step/=2) &#123; // 每个步长序列进行直接插入排序 for(int i=step;i&lt;arr.length;i++) &#123; for(int j=i-step;j&gt;=0;j-=step) &#123; if(arr[j+step]&lt;arr[j]) &#123; int tmp = arr[j+step]; arr[j+step]=arr[j]; arr[j] = tmp; &#125; &#125; &#125; &#125; &#125; 三.折半插入算法思想：当插入第i(i&gt;=1)个元素时，前面的V[0],…,V[i-1]等i-1个 元素已经有序，跟直接插入不同的是，折半插入不会与前面的元素一个个比较，而是利用二分搜索的思想，搜索到位置，进行插入 123456789101112131415161718192021222324public void binarySerachInsert(int[] arr) &#123; for(int i=1;i&lt;arr.length;i++) &#123; int right = i-1; int left = 0; int tmp = arr[i]; // 第i个元素的值小于有序序列的最大值 if(tmp&lt;arr[right]) &#123; while(left&lt;=right) &#123; int mid = (right+left)/2; if(tmp&gt;arr[mid]) &#123; left = mid+1; &#125;else if(tmp&lt;arr[mid]) &#123; right = mid-1; &#125;else &#123; left = left+1; &#125; &#125; for(int j=i;j&gt;left;j--) &#123; arr[j] = arr[j-1]; &#125; arr[left] = tmp; &#125; &#125; &#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java类加载器]]></title>
    <url>%2F2019%2F07%2F09%2Fjava%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[一.java类加载器的种类 启动类加载器(Bootstrap):加载 java_home/jre/lib下面的类库 扩展类加载器(Extension):加载 java_home/jre/lib/ext下面的类库 系统类加载器(System/App):加载classpath下的类 二. java类加载体系之双亲委托机制BootStrapClassLoader ： 启 动 类 加 载 器 ， 该 ClassLoader 是 jvm 在 启 动 时 创 建 的 ， 用 于 加载 $JAVA_HOME$/jre/lib 下面的类库 ExtClassLoader：扩展类加载器,加载 $JAVA_HOME/jre/lib/ext 下的类 库 AppClassLoader：应用程序类加载器,AppClassLoader 会加载 java 环境变量CLASSPATH 所 指 定 的 路 径 下 的 类 库 ,可 以 通 过System.getProperty(“java.class.path”)获取. 双亲委派体系是这样的： ​ 1）当 AppClassLoader 加载一个 class 时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器 ExtClassLoader 去完成。 2）当 ExtClassLoader 加载一个 class 时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader 去完成。 3）如果 BootStrapClassLoader 加载失败（例如在$JAVA_HOME$/jre/lib 里未查找到该 class），会使用ExtClassLoader 来尝试加载； 4）若 ExtClassLoader 也加载失败，则会使用 AppClassLoader 来加载，如果 AppClassLoader 也加载失败，则会报出异常 ClassNotFoundException。 为什么说双亲委派机制是安全的？ ClassLoader 加载的 class 文件来源很多，比如编译器编译生成的 class、或者网络下载的字节码。 有一些class文件是不可靠的，比如我可以自定义一个 java.lang.Integer 类来覆盖 jdk 中默认的 Integer类 12345public class Integer &#123; public Integer(int value) &#123; System.exit(0); &#125;&#125; 如果没有双亲委派机制，当我们使用Integer a = new Integer(１);的时候，会加载我们使用的这个类， 那么程序就会退出。如果使用双亲委派机制，类加载器加载的是jdk的Integer。 三.类加载过程 加载：将类的字节码加载到内存，生成代表这个类的java.lang.class对象 验证：验证这个字节码是否符合虚拟机的规范 准备：为类变量分配内存 解析：将符号引用解析为直接引用 初始化：对静态变量和静态代码块进行初始化操作 触发类初始化的条件 创建类的实例，new 一个对象 访问某个类的静态方法或这静态变量 使用反射创建一个类对象 初始化一个类的子类（会先初始化子类的父类) 总结：类的初始化步骤 ​ １. 如果该类还没有被加载和链接，首先执行类的加载和链接 ​ ２．加入这个类存在直接父类，先出初始化直接父类（在一个类加载器中，类只能初始化一次）； ​ ３．初始化静态变量以及静态代码块 ​]]></content>
      <categories>
        <category>jvm java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GC-垃圾回收机制]]></title>
    <url>%2F2019%2F07%2F04%2FGC-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一.垃圾回收机制干嘛的当我们运行一个程序时，创建的对象和一些变量是存在内存中，如果我们创建的对象和变量过多，它会占用大量的内存，在程序运行时，有一些对象和变量可能是无用的，我们没必要浪费内存去存储，垃圾回收机制就是帮我们回收这些无用的垃圾。 二.垃圾回收机制主要针对哪些内存JVM运行时内存区域主要包括五大部分，分别是程序计数器，本地方法栈，java虚拟栈，方法区，堆，由于，程序计数器，本地方法栈，java虚拟栈是每个线程私有的，当线程运行完毕时，会自动回收这些内存区域，而对于堆和方法区是线程共享的，这部分内存的分配和回收都是动态的，所以方法区和堆是GC主要针对的区域。堆中需要回收的主要是对象，方法区回收的主要是一些无用的常量和类，无用的常量可以通过引用计数可以判断是否是废弃的常量。无用的类是指该类的实例都已经被回收，加载该类的ClassLoader已经被回收，该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类 三.GC搜索算法引用计数器：​ 每一个对象都拥有一个对象引用计数器，当增加一个对该对象的引用时，引用计数器就会加一，减少一个对象引用，引用计数器就会减一，当该对象的引用计数器为0时，则认为该对象没有被引用是可以进行回收的，但是引用计数器有一个缺点，就是无法解决循环引用，比如A对象引用了B对象,B对象引用了A对象,但是这两个对象没有被任何的其它对象引用，这两个对象就无法回收了 GC Roots可达性分析从一些GC ROOTS对象作为起点，向下搜索，搜索通过的路径为引用链，当一个对象没有被该引用链连接时，则认为该对象是无用的。 GC Roots对象包括虚拟机栈中的引用的对象，方法区域中的类静态属性引用的对象，方法区域中常量引用的对象。 四.对象的引用是什么无论是引用计数器还是可达性分析，判断对象是否有用都与引用有关，那如何定义对象的引用。 Java中对象的引用分为四种级别，由高到低分别为：强引用，软引用，弱引用，虚引用 强引用（Strong Reference):强引用在我们每天写代码的时候都会用到，比如Object obj = new Object();如果一个对象被强引用引用，那么这个对象是不会被垃圾回收器回收的。 软引用(SoftReference):如果一个对象具有软引用，当JVM内存空间充足的情况下，垃圾回收器不会回收它 12Object obj = new Object();SoftReference&lt;Object&gt; str = new SoftReference&lt;Object&gt;(obj); 可以用来实现内存敏感的高速缓存.在jvm报告内存不足时，立刻清空所有软引用 gc回收软引用的过程 首先将SoftReference引用的obj置空 标记new Object()为finalize 回收内存，并添加到RefererceQueue.(如果有的话) 弱引用被弱引用引用的对象可有可无，只要被GC扫描到，随时都会被清除。 12Object obj = new Object();WeakReference&lt;Object&gt; str = new WeakReference&lt;Object&gt;(obj); 虚引用如果一个对象持有虚引用，就跟没有一样，随时都可以被回收 虚引用与软引用和弱引用不同。 五.GC回收算法 标记清除法：标记清除法从GC ROOT出发，进行扫描，对存活的对象进行标记，标记完后，再扫描整个空间未被标记的对象，进行回收。标记清除法容易产生大量不连续的空间。 标记整理法：采用与标记清除法一样的方式对对象进行标记，它会将存活的对象移到一端，然后将边界的对象清除，解决了内存碎片的问题。 复制算法：把堆分成大小相同的两块，每次使用一块，将存活的对象移动到空闲的一块，然后将另一块的内存清空。由于需要分配空闲的内存，所以内存利用率不高 分代收集算法：大部分jvm目前采用的算法，它将根据对象存活的生命周期将内存分为若干不同的区域。一般分为老年代，新生代，在堆区之外还分配一个永久代。老年代的特点是每次垃圾回收只有少量的对象需要被回收，一般采用标记整理法，新生代由于每次都有大量的对象需要被回收，一般采用复制算法。 新生代内存按照8:1:1的比例分为一个eden区，和两个survivor区（survivor0,survivor1）,所有新生成的对象首先会被放在年轻代，回收时先将一个eden区的存活对象复制到一个survivor0区，然后清空eden区，当survivor0被放满了，会将eden区和survivor0区的对象复制到survivor1，然后清空eden和这个survivor0区，此时survivor0区是空的，然后将survivor0与survivor1交换，保证survivor1是空的，如此循环往复。但是当survivor1不足以存放eden区和survivor0区的对象时，就将存活对象存放到老年代，若是老年代也存满了则触发Full GC,就是新生代老年代都进行回收。当新对象在Eden区申请空间失败时，会触发Eden区的垃圾回收。（Scavenge GC) 在年轻代存活了N次垃圾回收后任然存活的对象，就会被存放到老年代，当老年代内存满的时候会触发Full GC.(对整个堆进行垃圾回收)]]></content>
      <categories>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工厂模式]]></title>
    <url>%2F2019%2F07%2F04%2F%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[java设计模式之工厂模式什么是工厂模式：实现了对同一接口下一些类的实例的创建，定义了一个产生实例对象的接口，让其子类决定实例化哪个工厂类 工厂方法模式：看栗子： 假设我们有一个游戏(Game)接口，它有两个实现类，一个是手机游戏，一个是电脑游戏，游戏都是由一个工厂生产的。 1234public interface Game &#123; public void downLoad();&#125; 12345678public class PhoneGame implements Game&#123; @Override public void downLoad() &#123; System.out.println(&quot;下载手机游戏&quot;); &#125;&#125; 12345678public class ComputerGame implements Game&#123; @Override public void downLoad() &#123; System.out.println(&quot;下载电脑游戏&quot;); &#125;&#125; 12345678910//游戏工厂 负责生产游戏public class GameFactory &#123; public Game getPhoneGame() &#123; return new PhoneGame(); &#125; public Game getComputerGame() &#123; return new ComputerGame(); &#125;&#125; 抽象工厂模式简单工厂模式对象的创建依赖于工厂，当我们新增一个对象时，需要更改工厂类 比如当增加一个新的电视游戏类时，我们需要在工厂类添加一个方法获取电视游戏对象 所以，为了避免更改工厂类，采用抽象工厂模式。 抽象工厂模式采用增加工厂类的方法来避免修改代码 还是上面的例子： 1234public interface Game &#123; public void downLoad();&#125; 12345678public class PhoneGame implements Game&#123; @Override public void downLoad() &#123; System.out.println(&quot;下载手机游戏&quot;); &#125;&#125; 12345678public class ComputerGame implements Game&#123; @Override public void downLoad() &#123; System.out.println(&quot;下载电脑游戏&quot;); &#125;&#125; 提供一个抽象工厂负责创建游戏对象 1234public interface Factory &#123; public Game getGame();&#125; 真正创建对象的工厂 123456789public class ComputerFactory implements Factory&#123; @Override public Game getGame() &#123; return new ComputerGame(); &#125;&#125; 1234567public class PhoneFactory implements Factory&#123; @Override public Game getGame() &#123; return new PhoneGame(); &#125;&#125; 当我们增加一个新的电视游戏类的时候，再创建一个电视游戏工厂就可以了。 说了这么多，那工厂模式有什么用呢？ 首先工厂模式是为了帮我们创建对象的，那为什么要使用工厂模式呢， 工厂模式帮我们降低了耦合性，将对象的使用和创建分离开 降低代码重复使用，如果一个对象的创建需要很多初始化参数，我们不需要在每个需要它的类中去创建，在工厂类中创建一次就行。工厂类创建封装了创建对象的细节]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代理模式]]></title>
    <url>%2F2019%2F07%2F03%2F%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[java的静态代理和动态代理概述代理模式是一种常见的设计模式，分为静态代理，动态代理。代理提供了一种让我们间接访问目标对象的方法，以便我们对目标对象实现基本的功能外，还可以增加一些额外的功能。 静态代理代理类通过实现与目标类相同的接口，并在类中维护一个代理对象，通过构造方法传入目标对象，并赋值给代理对象。通过代理对象执行实现的接口方法，并实现增加功能的需求。 栗子：小明和小刚都待在宿舍，到了中午了，要去食堂吃饭，小明不想去，就叫小刚带饭，小刚答应了，带饭回来的时候捡到十块钱，于是小刚还帮小明多带了瓶饮料。小明吃完饭后，由于游戏开了，不想去丢饭盒，于是小刚又去帮他收拾。 1234567891011121314151617181920212223242526272829//接口public interface Lunch &#123; void eat(); &#125;//小明public class XiaoMing implements Lunch&#123; @Override public void eat() &#123; System.out.println(&quot;小明吃午饭&quot;); &#125;&#125;//代理人小刚public class ProxyXiaoGang implements Lunch&#123; //代理对象 通过构造方法传入实际对象，并赋值给它 private Lunch lunch; public ProxyXiaoGang(Lunch lunch) &#123; this.lunch = lunch; &#125; @Override public void eat() &#123; System.out.println(&quot;我是带饭人小刚，帮小明带饭,顺便带了瓶饮料&quot;); lunch.eat(); System.out.println(&quot;我是带饭人小刚，小明吃完了饭，我帮他收拾一下&quot;); &#125;&#125; 动态代理由于静态代理每个代理类只能为一个接口服务，当接口过多时，会产生大量的的代理类，而动态代理可以解决这一点，它可以为一个类实现的所有接口服务 123456789101112131415161718192021List&lt;String&gt; list = new ArrayList&lt;&gt;(); //param1:类加载器 param2:类实现的所有接口 param3:InvocationHander处理器 List&lt;String&gt; listProxy= (List&lt;String&gt;)Proxy.newProxyInstance(list.getClass().getClassLoader(), list.getClass().getInterfaces(), new InvocationHandler() &#123; // proxy 代理对象的一个实例 method 当前要执行的方法 args:方法执行需要的参数 @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; Object invoke = null; //如果是add方法，则对它增强 if(method.getName().equals(&quot;add&quot;)) &#123; System.out.println(&quot;执行前&quot;); invoke = method.invoke(list, args[0]); System.out.println(&quot;执行后&quot;); &#125; return method.invoke(list, args[0]); &#125; &#125;); listProxy.add(&quot;hello&quot;); System.out.println(listProxy.get(0)); 总结：静态代理一个代理类只能为一个接口提供服务 动态代理一个代理类可以为该类实现的所有接口服务]]></content>
      <tags>
        <tag>java设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java类与类之间的关系]]></title>
    <url>%2F2019%2F07%2F03%2Fjava%E7%B1%BB%E4%B8%8E%E7%B1%BB%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[java类之间的关系在java的面向对象的设计模式中，类与类之间的关系主要有６种。分别是依赖，关联，聚合，组合，继承，实现。其中，关联，聚合，组合仅仅在语义上有区别，在编程环境中的语法是一样的。 依赖(Dependence)它是一种use-a关系，表示一个类A用到了类B,也就是类A依赖类B。依赖关系在JAVA中体现为局部变量、方法的形参，或者对静态方法的调用。 123456789class A&#123; public void t(B b)&#123; &#125;&#125;class B&#123; &#125; 关联关系（Association)一种连接关系，它使一个类知道另一个的属性和方法。在java中体现为成员变量。关联关系可以有单向关联和双向关联。 12345678910class A&#123; B b; public void t()&#123; &#125;&#125;class B&#123; &#125; 聚合关系(Aggregation)一种has-a关系，关联关系的一种，是整体和个体之间的关系，聚合关系的整体和个体是可分的，他们具有各自的生命周期。在java中表现形式与关联关系一致，只是语义上有区别。 123456789class A&#123; B b; //整体（Ａ）和部分（Ｂ） public void t()&#123; &#125;&#125;class B&#123; &#125; 组合关系(Composition)一种contain-a关系，也是关联关系的一种。它表示的也是一种整体和部分的关系，但是整体和部分是不可分割的，具有相同的生命周期。在java中表现形式与关联关系一致，只是语义上有区别。 继承(Inheritance)“is-a”关系，用于表示一般和特殊之间的关系，在java中，表示类与类之间的父子关系，用extends关键字表示。 123456class A&#123; &#125;class B extends B&#123; &#125; 实现(Implementation)表示一个类实现一个或多个接口,由实现类去完成接口的具体操作，在java中用关键字implementsbia表示。 123456interface A&#123; &#125;class B implements A&#123; &#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase的基本操作]]></title>
    <url>%2F2019%2F07%2F02%2Fhbase%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[一.hbase环境搭建在操作hbase之前，肯定得先搭好环境。hbase的环境搭建其实也挺简单的，但前提是你配置好了hadoop和Zookeeper. 1.解压hbase的压缩包 1tar -zxvf hbase-x-x.tar.gz -C 指定目录 2.建立软连接，方便配置环境变量 1234567ln -s hbase-2.2.0/ hbasevi ~/.bashrc# 添加如下代码 自己的路径export HBASE_HOME=/home/master/software/hbase＃追加pathexport PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin:$HBASE_HOME/bin 进入hbase/conf/ 修改两个配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142vi hbase-env.sh# 添加export JAVA_HOME=/home/master/software/jdk＃ 指定不用内置的zookeeperexport HBASE_MANAGES_ZK=false# 指定存放数据的文件 需要创建export HBASE_LOG_DIR=/home/master/software/data/hbase/logsexport HBASE_PID_DIR=/home/master/software/data/hbase/pidsvi hbase-site.xml # 添加&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop01:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;!-- 指定自己配置的zk存放数据的位置-&gt; &lt;value&gt;/home/master/software/data/zk&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--默认HMaster HTTP访问端口--&gt; &lt;property&gt; &lt;name&gt;hbase.master.info.port&lt;/name&gt; &lt;value&gt;16010&lt;/value&gt; &lt;/property&gt; &lt;!--默认HRegionServer HTTP访问端口--&gt; &lt;property&gt; &lt;name&gt;hbase.regionserver.info.port&lt;/name&gt; &lt;value&gt;16030&lt;/value&gt; &lt;/property&gt; &lt;!--不使用默认内置的，配置独立的ZK集群地址，除了master，自己配了几台zookeeper，此处就配几台--&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop01:2181,hadoop02:2181,hadoop03:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 创建配置文件指定的目录 123mkdir -p ~/software/data/hbase/logsmkdir -p ~/software/data/hbase/pidsmkdir -p ~/software/data/hbase/logs 修改conf下的regionservers文件加入regoinserver的ip 12345vi regionservers# 添加hadoop01hadoop02hadoop03 同步时间 1sudo date -s &quot;2019-7-2 14:10:10&quot; 二.hbase的shell命令操作命名空间的操作 123create_namespace &apos;hsj&apos; //创建一个命名空间list_namespace //查看namesoacedrop_namespace &apos;hsj&apos; //删除namespace 删除的namespace一定要为空 表的操作 ddl 12345678//创建表create &apos;hsj:emp&apos;,&#123;NAME =&gt; &apos;info&apos;,NAME =&gt; &apos;extrainfo&apos;,VERSIONS =&gt; 3&#125; //指定版本数create &apos;hsj:student&apos;,&apos;info&apos;//修改表alter &apos;hsj:student&apos;,&#123;NAME =&gt;&apos;info&apos;,VERSIONS =&gt;3&#125;//删除表disable &apos;hsj:student&apos;drop &apos;hsj:studen&apos; dml 123456789101112131415//插入数据或修改put &apos;hsj:student&apos;,&apos;1001&apos;,&apos;info:name&apos;,&apos;zs&apos;put &apos;hsj:student&apos;,&apos;1002&apos;,&apos;info:name&apos;,&apos;li&apos;put &apos;hsj:student&apos;,&apos;1002&apos;,&apos;info:age&apos;,18//查询数据scan &apos;hsj:student&apos; //扫描全表get &apos;hsj:student&apos;,&apos;1001&apos;get &apos;hsj:student&apos;,&apos;1001&apos;,&apos;info:name&apos;//删除数据delete &apos;hsj:student&apos;,&apos;1001&apos;,&apos;info:name&apos;deleteall &apos;hsj:student&apos;,&apos;1002&apos;//查询表中数据有多少行count &apos;hsj:student&apos;//删除表中所有数据truncate &apos;hsj:student&apos; 预分区 12345678910111213//手动设置预分区create &apos;staff&apos;,&apos;info&apos;,&apos;partition1&apos;,SPLITS =&gt; [&apos;1000&apos;,&apos;2000&apos;,&apos;3000&apos;,&apos;4000&apos;]//生产16进行序列预分区create &apos;staff2&apos;,&apos;info&apos;,&apos;partition2&apos;,&#123;NUMREGIONS =&gt; 15, SPLITALGO =&gt; &apos;HexStringSplit&apos;&#125;//按照文件设置的规则预分区vi split.txt# 添加内容aaaaabbbbbcccccdddddcreate &apos;staff3&apos;,&apos;partition3&apos;,SPLITS_FILE =&gt; &apos;splits.txt&apos; 三. 读写数据的流程 读数据的流程 Client向zk集群发送获取元数据所在的RegionServer zk返回Meta所在的RegionServer client读取RegionServer上的Meta，找到对应的读请求的RegionServer client读取RegionServer上的Regizhogon数据，先读MemoStore中的数据，如果不存在,读取blockCache,读取StoreFile 写数据的流程 Client向zk集群发送获取元数据所在的RegionServer zk返回Meta所在的RegionServer client读取RegionServer上的Meta，找到对应的写请求的RegionServer 现将写操作写入RegionServer的Hlog,在将数据写入MemoStore 四.java api123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135public class HbaseOperator &#123; private static Admin admin = null; private static Connection conn = null; private static Configuration conf = null; static &#123; //1 .获取配置 conf = HBaseConfiguration.create(); conf.set(&quot;hbase.zookeeper.quorum&quot;,&quot;hadoop01,hadoop02,hadoop03&quot;); //2.创建连接 try &#123; conn = ConnectionFactory.createConnection(conf); admin = conn.getAdmin(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //判断表是否存在 public static boolean isExistTable(String name) throws Exception &#123; boolean b = admin.tableExists(TableName.valueOf(name)); return b; &#125; //创建表 public static void createTable(String tableName,String cf) throws Exception &#123; ColumnFamilyDescriptor cfd = ColumnFamilyDescriptorBuilder.newBuilder(cf.getBytes()).build(); TableDescriptor td = TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)).setColumnFamily(cfd).build(); admin.createTable(td); System.out.println(&quot;增加成功&quot;); &#125; //异步创建 public static void createTableAsync(String tableName,String cf) throws Exception &#123; ColumnFamilyDescriptorBuilder cfdb = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(cf)); cfdb.setMaxVersions(3); ColumnFamilyDescriptor cfd = cfdb.build(); TableDescriptor tdb = TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)).setColumnFamily(cfd).build(); byte[][] splite = new byte[][]&#123;Bytes.toBytes(&quot;1000&quot;),Bytes.toBytes(&quot;2000&quot;)&#125;; Future&lt;Void&gt; tableAsync = admin.createTableAsync(tdb,splite); tableAsync.get(1000, TimeUnit.MILLISECONDS); System.out.println(&quot;创建表成功&quot;); &#125; //插入数据 public static void put(String tableName,String rowkey,String cf,String column,String data) throws Exception &#123; Put p = new Put(rowkey.getBytes()); p.addColumn(Bytes.toBytes(cf),Bytes.toBytes(column),Bytes.toBytes(data)); Table table = conn.getTable(TableName.valueOf(tableName.getBytes())); table.put(p); table.close(); &#125; public static void insert(String tableName,String rowkey,String cf,String column,String data) throws Exception&#123; &#125; //查找数据 public static void get(String tableName,String rowkey) throws IOException &#123; Get g = new Get(Bytes.toBytes(rowkey)); Table table = conn.getTable(TableName.valueOf(tableName.getBytes())); Result result = table.get(g); showView(result); &#125; //查找数据 public static void get(String tableName,String rowkey,String cf,String column) throws IOException &#123; Get g = new Get(Bytes.toBytes(rowkey)); g.addColumn(cf.getBytes(),column.getBytes()); Table table = conn.getTable(TableName.valueOf(tableName.getBytes())); Result result = table.get(g); showView(result); &#125; //查找表 public static void scan(String tableName) throws IOException &#123; Scan scan = new Scan(); Table table = conn.getTable(TableName.valueOf(tableName.getBytes())); ResultScanner scanner = table.getScanner(scan); for(Result result:scanner)&#123; showView(result); &#125; &#125; //删除表 public static void deleteTable(String tableName) throws Exception &#123; admin.disableTable(TableName.valueOf(tableName)); admin.deleteTable(TableName.valueOf(tableName)); System.out.println(&quot;删除成功&quot;); &#125; /* 过滤器: 行健过滤器 列族过滤器 列名过滤器 值过滤器 */ public static void scanAndFilter(String tableName) throws Exception &#123; Scan scan = new Scan();// PrefixFilter filter = new PrefixFilter(Bytes.toBytes(&quot;l&quot;)); RowFilter filter = new RowFilter(CompareOperator.GREATER_OR_EQUAL, new BinaryComparator(Bytes.toBytes(&quot;3000&quot;))); scan.setFilter(filter); Table table = conn.getTable(TableName.valueOf(tableName.getBytes())); ResultScanner scanner = table.getScanner(scan); for(Result result:scanner)&#123; showView(result); &#125; &#125; public static void showView(Result result) throws IOException &#123; Cell[] cells = result.rawCells(); for(Cell cell:cells)&#123; System.out.println(Bytes.toString(CellUtil.copyRow(cell))+&quot;,&quot;+Bytes.toString(CellUtil.cloneFamily(cell))+ &quot;,&quot;+Bytes.toString(CellUtil.cloneQualifier(cell))+&quot;,&quot;+Bytes.toString(CellUtil.cloneValue(cell))); &#125; &#125; public static void close() throws IOException &#123; admin.close(); conn.close(); &#125; public static void main(String[] args) throws Exception &#123;// System.out.println(HbaseOperator.isExistTable(&quot;hsj:emp&quot;));// createTable(&quot;javaapi1:student&quot;,&quot;indo&quot;,&quot;extrainfo&quot;);// deleteTable(&quot;javaapi1:student&quot;); put(&quot;javaapi1:student&quot;,&quot;1001&quot;,&quot;info&quot;,&quot;age&quot;,&quot;18&quot;); put(&quot;javaapi1:student&quot;,&quot;1001&quot;,&quot;info&quot;,&quot;gender&quot;,&quot;male&quot;); get(&quot;javaapi1:student&quot;,&quot;1001&quot;); get(&quot;javaapi1:student&quot;,&quot;1001&quot;,&quot;info&quot;,&quot;age&quot;);// scan(&quot;javaapi1:student&quot;);// scanAndFilter(&quot;javaapi1:student&quot;); &#125;&#125; 五.hbase架构 Client: 负责请求数据的接口 Zookeeper:负责Hbase集群的高可用，存储了元数据的地址 Hmaser:监控RegionServer，处理RegionServer的故障转移以及元数据的变更，处理Region的分配或转移，通过Zookeeper发送自己的位置给客户端 RegionServer:负责存储Hbase的实际数据，处理分配给他的Region，刷新缓存到Hdfs,维护Hlog以及负责StoreFile的合并 Write-Ahed logs (Hlogs): 数据存储在内存容易丢失，Write-Ahed logs存储了对数据的更新操 Store:一个Store对应一个列族 Region:Hbase表的分片,达到256M会进行分裂 MemoStore:内存存储，保存当前的数据操作,当MemoStore数据达到阈值(默认为128M),将数据刷到磁盘。 HFile:StoreFile以HFile的形式保存在Hdfs上 六.flush,compact flush:当 MemStore 达到阈值，将 Memstore 中的数据 Flush 进 Storefile 12345## memStore的默认大小 128M&lt;property&gt; &lt;name&gt;hbase.hregion.memstore.flush.size&lt;/name&gt; &lt;value&gt;134217728&lt;/value&gt; &lt;/property&gt; compaction:每当memstore的数据flush到磁盘后，就形成一个storefile，当storefile的数量越来越大时,影响hbase的读性能，将多个storeFile进行合并操作，这个过程就称之为compact。compact 的主要作用是为了合并，清除过期和多余版本的数据，提高读写数据的效率。 compaction有两种实现方式 minor compaction：只用来做部分文件的合并操作以及对多余版本和过期数据的清理 major compaction：对Region下的Hstore下的所有StoreFIle执行合并操作，最终的结果是整理合并出一个文件。]]></content>
      <categories>
        <category>hbase</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[简单了解一下zookeeper]]></title>
    <url>%2F2019%2F07%2F02%2F%E7%AE%80%E5%8D%95%E4%BA%86%E8%A7%A3%E4%B8%80%E4%B8%8Bzookeeper%2F</url>
    <content type="text"><![CDATA[一.Zookeeper是什么Zookeeper是一个开源的分布式服务管理框架。它负责管理数据，接受观察者的注册，一旦数据的状态发生变化，Zookeeper负责通知观察者做出相应的反应。 二.Zookeeper的特点 一个领导者（leader)，多个跟随者（Follower) 全局数据一一致，每个Server保存一份相同的数据副本 更新请求按顺序执行，来自同一个Client的更新请求按其发送顺序依次执行 数据更新原子性 实时性，在一定时间范围内，Client能读到最新数据 三.Zookeeper全分布式的搭建 1. 将包解压缩到指定路径 1tar -zxvf Zookeeper.x.x.tar.gz -C 指定路径 建立软连接，方便配置环境变量 1ln -s Zookeeper.x.x zookeeper 配置环境变量 12345vi .bashrcexport ZK_HOME=/home/master/software/zookeeper# 在path后面追加export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$ZK_HOME/bin: 修改文件 123456789101112131415cd zookeeper/conf/vi zoo.cfg tickTime=2000# 指定zk数据存放的位置dataDir=/home/master/software/data/zk/data# 指定zk日志存放的位置dataLogDir=/home/master/software/data/zk/dataLogclientPort=2181initLimit=5syncLimit=2# server.num后面的数字是myid中的值 myid后面要创建 hadoop0x是主机名server.1=hadoop01:2888:3888server.2=hadoop02:2888:3888server.3=hadoop03:2888:3888 在dataDir目录下新建myid文件,里面填写一个数字就行，要和上面的server匹配 比如server.1,我就需要在dataDir下面myid填一个１ 有没有发现zookeeper不需要指定master，是的，因为它自己会通过选举机制选出leader ## 四.选举机制 选举机制比较复杂，这里只说一个不够恰当的例子,稍微理解一下 选举机制其实是半数机制相关，一旦有超过半数的机器启动，zk就会选举出一个leader 选举机制说明 假设有五台Zookeeper组成的集群，id总1-5，假设者五台机器依序启动， (1)服务器１启动，只有一台服务器，发出去的报文没有任何响应，所以它的选举状态一直是LOCKING状态 (2)服务器２启动，与服务器１通信，交换自己的选举结果，这时候服务器１选择id较大服务器 ２，但是由于没 有到达半数以上，２也处于LOCKING (3)服务器３启动，１，２选择３，到达半数以上，３成为leader (4)服务器4启动，但是由于已经确定了３为leader，所以4是Follwer (5)服务器５启动，与４一样 五.节点类型Zookeeper分为两种节点 持久：客户端和服务器断开连接后，创建的节点不删除。持久化节点有持久化编号目录节点，和没有编号的目录节点两种 1create -s /node &quot;node&quot; 带编号的吃句话节点 ，-s 带编号 短暂:客户端和服务器断开连接后，创建的节点删除，临时节点有临时顺序编号目录节点 ，和没有编号的目录节点两种。 1create -e -s /node2 &quot;node2&quot; , -e 临时节点 -s :带编号 六.Zookeeper监听事件监听器原理：事件监听有三要素，分为事件源 事件对象（描述事件源产生的行为）监听器 事件对象 事件源 －－－－－－》监听器（监听关注的事件）－－－－－》触发某个行为 zookeeper监听器原理： zookeeper有两个线程，一个connect(网络连接),一个lisener，.connect会将注册的监听事件发送给zookeeper，zk收到后，会将注册的监听事件放进放进监听事件列表，当有数据或者路径发生变化时，zookeeper会将这个消息发送给listener线程，listener内部会调用process()方法。 zk写数据流程： 1. client向一个server发送写数据的请求，如果这个server不是leader，他会把请求转发给leader，leader会将写请求转发给每个server，server写成功后会通知leader 2. 当半数以上的serverxie写成功后，leader就会告诉那个接受请求的server写数据成功了。 3. server进而告诉客户端写数据成功]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解volatile]]></title>
    <url>%2F2019%2F07%2F01%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3volatile%2F</url>
    <content type="text"><![CDATA[一.java内存模型在说volatile之前，需要了解java的内存模型。java采用的是共享内存模型，线程之间的共享变量存储在主内存中，每个线程有一个私有的本地本地内存，本地内存存储了该线程读写共享变量的副本，如图 举个栗子：a = 100 一个线程需要修改a的值，必须先将a读到自己的本地内存，对a执行更新操作，再将a的值写回到主存中 二.并发编程中的三个概念 原子性：一个操作或多个操作，要么全执行，要么全不执行。最典型的例子就是银行转账问题， 假设A向B转账1000块钱，包含两个操作，A减去1000块钱,B加上1000块钱，假设这个操作不包含原子性，会发生一件很恐怖的事情，在A减去1000块钱后，操作突然中断，这导致A少了1000块钱，但是B没有增加，这1000块钱凭空消失，所有这两个操作必须具备原子性。 在java中，对变量的简单读取和赋值（将数值变量)才具备原子性 123int a = 10 ;//原子性int b = a;//非原子性int count=a+1;//原子性 可见性: 当多个线程共享一个变量时，如果一个线程修改了该变量的值，其它线程会立刻知道 java中通过volatile实现可见性，被volatile修饰的变量，当它被修改时，它会保证修改的值会立即被写回主存，当有其它线程需要的时候，拿到的是最新值 有序性：程序执行的顺序会按照代码的顺序执行。在java中，编译器和处理器会对我们编写的代码优化，进行指令重排，指令重排不会影响单线程执行的结果，但是会影响多线程执行的结果。java中使用volatile可以禁止指令重排序。 123456789//线程一执行的代码int a = 10;boolean start = true;//线程二执行的代码while(!start)&#123; &#125;对a的操作 由于线程一的两行代码没有依赖关系，所以可能打算指令重排，线程一先执行boolean start = true;当还没有执行int a = 10;，线程二开始执行，执行对a的操作，但是由于a的值不是10，不是a的操作需要的结果，程序会出错,使用volatile可以避免这个问题 三.volatile案例一： 1234567891011121314151617181920212223242526272829303132333435363738public class TestVolatile &#123; private /* volatile */ List&lt;String&gt; list = new ArrayList&lt;&gt;(); public void t() &#123; while(true) &#123; if(list.size()==2) &#123; break; &#125; &#125; &#125; public void t2() &#123; for(int i=0;i&lt;3;i++) &#123; System.out.println(&quot;添加了第&quot;+i+&quot;个元素&quot;); list.add(&quot;a&quot;); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public static void main(String[] args) &#123; TestVolatile t = new TestVolatile(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.t2(); &#125; &#125;).start();//线程一 new Thread(new Runnable() &#123; @Override public void run() &#123; t.t(); &#125; &#125;).start();//线程二 &#125;&#125; 如果不使用volatile关键字修饰，程序会进入到死循环，因为当线程一执行list.add()方法时，线程二不能马上获得线程一添加元素后list.size()的大小，当线程一将程序执行完，list.size()=3,那线程二永远也不可能中断。 使用volatile,当线程一执行list.add(),方法时，会改变list.size()的大小，线程二会立刻获取到修改后的新值，当list.size()=2时，线程二就会结束。 案例二： 123456789101112131415161718192021222324252627282930public class TestVolatile2 &#123; boolean flag = true; public void t1() &#123; while(true) &#123;// System.out.println(&quot;111&quot;); // 使用这句化程序会正常结束 １// synchronized(this) &#123; &#125;//使用这句化程序会正常结束 ２ int a = 10; //使用这句化程序不会正常结束 ３ if(!flag) &#123; throw new IllegalArgumentException(Thread.currentThread().getName()+&quot;停止&quot;); &#125; &#125; &#125; public static void main(String[] args) &#123; TestVolatile2 t = new TestVolatile2(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.t1(); &#125; &#125;).start(); try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; t.flag = false; System.out.println(Thread.currentThread().getName()+&quot;执行完毕&quot;); &#125;&#125; 使用上面的１,2语句都可以让程序正常结束，但是使用3不行。为什么呢，因为synchronized也保证了可见性，但是为什么1可以呢？看下面println源码，是不是恍然大悟 123456public void println(String x) &#123; synchronized (this) &#123; print(x); newLine(); &#125; &#125; 四.volatile有没有保证原子性12345678910111213141516171819202122232425262728public class SynchronizedTest1 &#123; private volatile int count = 0; public void synchronizedTest1() &#123; for(int i=0;i&lt;1000;i++) &#123; count++; &#125; &#125; public static void main(String[] args) &#123; SynchronizedTest1 t1 = new SynchronizedTest1(); for(int i=0;i&lt;10;i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; t1.synchronizedTest1(); &#125; &#125;).start(); &#125; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(t1.count); &#125;&#125; 10个线程每个线程累加1000，如果volatile可以保证原子性的话，那么最后的结果就是10000,但是最后的运行结果是小于10000,也就是说volatile没有保证原子性 分析：假设现在count=10;线程一将count=10读到自己的本地内存，线程一阻塞，线程二运行，线程二将count=10读到自己的本地内存，对count进行加一操作(count=11)，但是还没写回到主存，线程二就被阻塞了，线程一开始执行，对count进行加一操作(count=11),将count=11写回到主存中，线程一执行完毕，线程二开始执行，但是由于线程二只剩写操作，不需要重新从主存读数据，直接将count=11写回到主存，线程二将线程一的值覆盖了。这就是最后的结果小于10000的原因。 总结：volatile保证了共享变量的可见性和有序性，但是没有保证原子性]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈synchronized]]></title>
    <url>%2F2019%2F07%2F01%2F%E8%B0%88%E8%B0%88synchronized%2F</url>
    <content type="text"><![CDATA[一.线程安全问题​ 在单线程的情况下不会出现线程安全问题，而在多线程中，有可能会出现同时访问一个共享可变资源的情况。共享意味着该资源可被多个线程共享，可变意味着资源可被多个线程修改，这样很可能出现线程安全问题。 看一段线程安全的代码: 123456789101112131415161718192021222324public class SynchronizedTest1 &#123; private int count = 0; public void synchronizedTest1() &#123; for(int i=0;i&lt;1000;i++) &#123; count++; &#125; &#125; public static void main(String[] args) &#123; SynchronizedTest1 t1 = new SynchronizedTest1(); for(int i=0;i&lt;10;i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; t1.synchronizedTest1(); &#125; &#125;).start(); &#125; try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(t1.count); &#125;&#125; 在这里我开启了10个线程，每个线程我都将它们累加1000，结果应该是10*1000=10000,但是运行程序之后发现，结果count是小于10000,这是为什么呢？ 这是由于count++不是原子操作，它由三步组成，首先要获取count的值，再对它进行加１，最后更新count.假设有两个线程，线程一先读到count=10;线程二这时候也读到count=10;线程二对count加１， count=11,这时候线程二被阻塞，线程一开始执行，进行加１操作，并成功更新值，count=11,线程一执行完成后，线程二开始执行，由于线程二在之前已经进行了加１操作，这时候只需要将count写回到主存，最后count=11,两个线程在执行自增操作的时候，只加了１，这就是共享变量带来的并发问题。 二.synchronized解决线程不安全要解决上述的线程不安全问题，可以使用synchronized解决,我们只需要在方法上添加synchronized 12345public synchronized void synchronizedTest1() &#123; for(int i=0;i&lt;1000;i++) &#123; count++; &#125; &#125; synchronized是一种对象锁，用它来对某个方法或者代码块进行标记后，当某个线程对这个方法或者对象进行访问的时候，这个线程便获得了这把锁，别的线程只能等待该线程将锁释放，才能访问这个方法或者代码块。 三.synchronized的实现机制编写一个简单的同步代码 12345678910public class SynchronizedTest2 &#123; public void test2() &#123; synchronized (this) &#123; &#125; &#125; public static void main(String[] args) &#123; &#125;&#125; 通过javap -v SynchronizedTest2.class,获取字节码 123456789101112131415Code: stack=2, locals=3, args_size=1 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: aload_1 5: monitorexit 6: goto 14 9: astore_2 10: aload_1 11: monitorexit 12: aload_2 13: athrow 14: return 可以看到,使用synchronized进行同步操作，关键是需要对对象的monitor 进行获取，monitorenter让对象的锁计数器加１，monitorexit会让对象的锁计数器减一。 ## 四.synchronized实现同步的方式 同步方法添加在非静态方法的同步:获取的是实例对象的锁 123public synchronized void synchronizedTest1() &#123;&#125; 添加在静态方法的同步：获取的是类对象的锁 123public synchronized static void synchronizedTest2() &#123; &#125; 同步代码块非静态方法中的代码块 12345public void synchronizedTest4() &#123; synchronized(this) &#123; //实例对象的锁 &#125;&#125; 静态方法中的代码块 12345public static void synchronizedTest3() &#123; synchronized(SynchronizedTest1.class) &#123; //类对象的锁 &#125;&#125; 5.使用synchronized需要注意的地方 对于 synchronized方法 或者 synchronized代码块，当出现异常时，JVM会自动释放当前线程占用的锁，因此不会由于异常导致出现死锁现象。 1234567891011121314151617181920212223242526272829303132public class TestException &#123; public synchronized void t1() &#123; while(true) &#123; int i = 10/0; // java.lang.ArithmeticException: / by zero 释放锁 &#125; &#125; public synchronized void t2() &#123; System.out.println(&quot;我是t2，我正在执行&quot;); &#125; public static void main(String[] args) &#123; TestException t = new TestException(); new Thread(new Thread() &#123; @Override public void run() &#123; t.t1(); &#125; &#125;).start(); try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Thread() &#123; @Override public void run() &#123; t.t2(); &#125; &#125;).start(); &#125;&#125; 当一个对象的synchronized的方法正在被一个线程访问，其它线程不能访问该对象的其它synchronized方法，但是可以访问其它非synchronized的方法。这应该很好理解，一个对象就一把锁，被一个线程占用后，其它线程必须等带该锁被释放。 synchronized锁住的是对象 123456789101112131415161718192021222324252627282930313233343536public class TestObject &#123; private Object obj = new Object(); public void t1() &#123; synchronized(obj) &#123; System.out.println(Thread.currentThread().getName()); obj = new Object(); //改变obj,指向另外一个对象 try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName()); &#125; &#125; public static void main(String[] args) &#123; TestObject t = new TestObject(); new Thread(new Runnable() &#123; @Override public void run() &#123; t.t1(); &#125; &#125;).start(); try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; new Thread(new Runnable() &#123; @Override public void run() &#123; t.t1(); &#125; &#125;).start(); &#125;&#125; 线程１先执行，将obj = new Object(); 指向另外一个对象，这时候阻塞两秒，线程２执行，获取锁，由于obj是一个新对象所以获取锁成功,线程１和线程２交替执行 运行结果 1234Thread-0Thread-1Thread-0Thread-1]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java IO]]></title>
    <url>%2F2019%2F06%2F30%2Fjava-IO%2F</url>
    <content type="text"><![CDATA[java中的流的类型 按照流的方向：可以分为输入流(InpuStream)和输出流(OutputStream) 按照功能分类：节点流（从一个特定的地方读写数据）和处理流(对一个已经存在的流的封装) 按照数据的处理单位: 字节流和字符流 流的分类字节流：节点流 FileInputStream:从文件系统获得输入的文件。 构造方法： ​ FileInputStream(File file) 连接到一个实际的文件 ​ FileInputStream(String name) 根据路径连接到一个实际的文件 方法： ​ public int read(byte[] b) throws IOException 将源文件的字节数据读到缓冲区 b，根据返回值判断文件数据是否已经读完，返回-1，代表文件数据已读完 ​ public int read(byte[] b, int off, int len) throws IOException,将源文件的字节数 b,从数组的偏移量off位置开始存放长度为len的字节数，如果len&gt;b.length-off,将会抛出 IndexOutOfBoundsException .FileOutputStream： 构造方法 ​ FileOutputStream(File file) 创建一个文件输出流写入指定的 File对象 FileOutputStream(File file, boolean append)创建一个文件输出流写入指定的 File对象，append表示拼接，不覆盖ByteArrayInputStream： 把字节数组转换为输入流 构造方法： ​ ByteArrayInputStream(byte[] buf) :使用一个字节数组当中所有的数据做为数据源，程序 可以像输入流方式一样读取字节，可以看做一个虚拟的文件，用文件的方式去读取它里面的数据。 ​ ByteArrayInputStream(byte[] b,int offset,int length):从数组当中的第offset开始，一直取出length个这个字节做为数据源 ByteArrayOutputStream ： 把字节数组转换为输出流 构造方法： ​ ByteArrayOutputStream()创建一个新的字节数组输出流 ​ ByteArrayOutputStream(int size)创建一个新的字节数组输出流，具有指定大小的缓冲容量， 以字节为单位的。 处理流 BufferedInputStream: 缓冲输入流 构造方法：BufferedInputStream(InputStream in) ​ BufferedInputStream(InputStream in, int size) 指定缓冲区的大小 BufferedOutputStream：缓冲输出流 构造方法： ​ BufferedOutputStream(OutputStream out)创建一个新的缓冲输出流，将数据入到 指定的基本输出流中 ​ BufferedOutputStream(OutputStream out, int size)创建一个新的缓冲输出流，用指定的缓冲区大小写数据到指定的基本输出流中。 DataInputStream:数据输入流，读时提供更多的类型 构造方法：DataInputStream(InputStream in) 方法： ​ readByte()——从输入流中读取1个字节，指它转换为byte类型的数据； readLong()——从输入流中读取8个字节，指它转换为long类型的数据； readFloat()——从输入流中读取4个字节，指它转换为float类型的数据； readUTF()—— 从输入流中读取1到3个字节，指它转换为UTF-8字符编码的字符串； DataOutputStream：数据输出流，读时提供更多的类型 构造方法： ​ DataOutputStream(OutputStream out)创建一个新的数据输出流，将数据写入到指定的基本输出流中。 方法： ​ writeByte(int v)——将byte作为字节流输出 writeLong(long v)——将long作为输出流输出 writeFloat(float v)——将float作为输出流输出 writeUTF(String str)—— 将String作为输出流输出 ObjectInputStream:对象输入流，主要用于对象序列化和反序列化 构造方法： ​ ObjectInputStream( )创建一个对象输入流 ​ ObjectInputStream(InputStream in)创建一个对象输入流读取指定的输入流。 ObjectOutputStream:对象输出流 构造方法： ​ ObjectOutputStream(OutputStream out)创建对象输出到指定的输出流 字符流： 节点流 CharArrayReader ：把字符数组转换为Reader，从字符数组中读取字符； 构造方法： ​ CharArrayReader(char[] buf)创建从指定的字符数组的一个chararrayreader。 CharArrayReader(char[] buf, int offset, int length创建从指定的字符数组的一个 chararrayreader。 CharArrayWriter：将字符数组有Writer写出 构造方法： ​ CharArrayWriter() StringReader : 把字符串转换为Reader，从字符串中读取字符； 构造方法： ​ StringReader(String s) StringWriter :将字符串有Writer写出 FileReader : 从文件中读取字符； 构造方法： ​ FileReader(File file) FileWriter:将字符写入到指定文件 构造方法： ​ FileWriter(File file) ​ FileWriter(File file, boolean append) ​ FileWriter(String fileName) ​ FileWriter(String fileName, boolean append) 处理流 InputStreamReader : 过滤器，把InputStream转换为Reader，可以指定字符编码； 构造方法： ​ InputStreamReader(InputStream in)创建一个inputstreamreader使用默认字符集。 ​ InputStreamReader(InputStream in, Charset cs)创建一个inputstreamreader使用给定的字符集。 ​ InputStreamReader(InputStream in, CharsetDecoder dec)创建一个inputstreamreader使用给定的字符集解码。 InputStreamReader(InputStream in, String charsetName)创建一个inputstreamreader使用指定的字符集。 OutputStreamWriter:将字符流转为字节流输出，可以指定字符编码； 构造方法： ​ OutputStreamWriter(OutputStream out)创建一个outputstreamwriter使用默认的字符编码。 OutputStreamWriter(OutputStream out, Charset cs)创建一个outputstreamwriter使用给定的字符集。 OutputStreamWriter(OutputStream out, CharsetEncoder enc)创建一个outputstreamwriter使用给定的字符集编码。 OutputStreamWriter(OutputStream out, String charsetName)创建一个outputstreamwriter使用指定的字符集。 BufferedReader: 过滤器，为其他Reader提供读缓冲区，此外，它的readLine()方法能够读入一行字符串； 构造方法： ​ BufferedReader(Reader in)创建一个使用默认大小输入缓冲区的缓冲字符输入流。 BufferedReader(Reader in, int sz)创建一个使用指定大小的输入缓冲区的缓冲字符输入流。 BufferedWriter: 构造方法： ​ BufferedWriter(Writer out)创建一个使用默认大小输出缓冲区的缓冲字符输出流 ​ BufferedWriter(Writer out, int sz)创建一个新的缓冲字符输出流，该流使用给定大小的输出缓冲区。 序列化与反序列化序列化：将对象写到输出流中，便于在网络中进行传输或者存储到文件中 反序列化:将输入流中的数据转化为对象，将从网络或文件中读取的字节数据转化为对象]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exception]]></title>
    <url>%2F2019%2F06%2F29%2Fjava%E7%9A%84exception%2F</url>
    <content type="text"><![CDATA[java中的异常分为哪几类 看图： 在java中，所有异常都都有一个共同的祖先Throwable,Throwable有两个重要的子类，Exception和error error:是程序无法处理的错误，表示程序有严重的错误，大多数错误都是jvm出现的问题，如StackOverError,OutOfMemoryError. Exception:程序本身可以处理的异常。它有一个子类RuntimeException,这一类异常有java虚拟机抛出，如NullException(要访问的变量没有引用任何对象)，ArithmeticException(算术异常，整数除以０时)，ArrayIndexOutOfBoundsException(下标越界) 异常处理 try:用于捕获异常，后面可以接多个catch,如果后面没有catch,则必须接finally catch:用于处理try捕获的异常 finally:无论是否捕获或者处理异常，finally块中的语句都会被执行，当tyr或catch块中遇到return语句时，finally语句块将在方法返回之前执行 throw和throws的区别 throw 用在方法体中，表示抛出异常，由方法体内的语句处理 throw是抛出异常的一个动作，抛出的是一个异常实例，执行throw一定抛出了某种异常 throws 用在方法声明后面，表示如果抛出异常，由该方法的调用者处理 throws主要是用来声明这个方法可能会抛出某种异常，让它的使用者知道需要捕获的异常类型 final,finally,finalize的区别 final :用来声明属性，方法和类，表示属性不可变，方法不可覆盖，类不可继承 finally:异常处理语句结构的一部分，表示总是执行 finalize:Object的一个方法，在垃圾回收器执行的时候会调用被回收对象的此方法，当该方法被调用则代表该对象即将死亡。当我们主动去调用该方法并不会导致该对象的死亡，这是一个回调方法，不需要我们调用]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[equals]]></title>
    <url>%2F2019%2F06%2F29%2Fequals%2F</url>
    <content type="text"><![CDATA[equals()方法与==的区别 ==:对于基本类型比较的是值，对于引用类型比较的是地址 123456789101112public static void main(String[] args) &#123; int a = 10; int b = 10; System.out.println(a == b);//true String str1 = &quot;hello&quot;; String str2 = &quot;hello&quot;; System.out.println(str1 == str2);//true String str3 = new String(&quot;hello&quot;); String str4 = new String(&quot;hello&quot;); System.out.println(str3 == str4);//false &#125; 在这里解释一下String类型在jvm是怎样存储的 如果使用“”创建String类型，如str1,str2,它会去常量池查找有没有当前我要创建的值，如果有直接将常量池的引用复制给该变量。第一次使用String str1 = “hello”，由于常量池中没有“hello”，它会在常量池中创建，在使用String str2 = “hello”，由于常量池中存在”hello”，str2它会直接引用这个值，所以str1 == str2为true 使用new创建String对象的时候，它回到常量池去查找有没有我要创建的值，如果有则拷贝一份到堆中，将该副本的引用赋值给变量。如果没有，则实例化该对象放到常量池，并且拷贝副本到堆中，将副本的引用复制给变量 如：String str3 = new String(“hello”)，常量池创建，拷贝一份到堆中，并副本赋值给str3,String str4 = new String(“hello”)，从常量池拷贝一份到堆中，并将副本引用赋值给str4.所有str3 == str4为false equals:Object中的方法，在Object中比较的也是两个对象的地址,但是一般情况下，都要重写equals方法，来指定相等的规则。比如String类，重写equsls方法，比较的是String的值，而不是地址。 案例：一个Student类，重写equals方法，如果它的name相等就认为他们是同一个学生。 1234567891011121314151617181920212223242526272829303132public class Student &#123; private String name; private int age; public Student(String name, int age) &#123; this.name = name; this.age = age; &#125; @Override public boolean equals(Object obj) &#123; //两个对象的地址相等，一定是同一个对象 if(this==obj) &#123; return true; &#125; if(obj instanceof Student) &#123; Student s = (Student) obj; if(this.name.equals(s.name)) &#123; return true; &#125; &#125; return false; &#125; public static void main(String[] args) &#123; Student s1 = new Student(&quot;zs&quot;, 18); Student s2 = new Student(&quot;zs&quot;, 20); System.out.println(s1.equals(s2));//true System.out.println(s1 == s2);//false &#125;&#125; eausle的特性 自反性：a.equals(a)一定为true 对称性: 如果a.equals(b)为true，那么b.equals(a)一定也为true 传递性: a.equals(b)为true,b.equals(c)为true,那么a.equals(c)也为true equals与hashcode重写equals是否需要重写hashcode? 在api中是建议在重写equals时，我们有必要重写hashcode. 在一些用到hashcode的数据结构存储数据的时候，如hashset,是一定要重写的hashcode的 hashset存放元素的时候存放的是不重复的元素，它存数据的时候会根据元素的hash值和equals方法来判断是否添加，如果两个元素的hash值相同和equals方法返回为true,则认为元素相同，hashset不将它添加进去。这样就会产生一个问题，假设有 student1 = new Student(“zs”,18),student2 = new Student(“zs”,18),hashset在添加这两个对象的时候，只会将一个对象添加进去，但是如果不重写hashcode,hastset会认为这是两个不同的元素并将它添加进去，当我们在从hashset中取出数据的时候会发现取出了两个相同对象，这与hashset的的规则不符合，所以重写equals方需要重写hashcode. hashset添加元素的判断 12if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) 案例：一个Student类，重写equals,不重写hashcode,创建两个属性相等的对象，存放到hashset 1234567891011121314@Override public boolean equals(Object obj) &#123; //两个对象的地址相等，一定是同一个对象 if(this==obj) &#123; return true; &#125; if(obj instanceof Student) &#123; Student s = (Student) obj; if(this.name.equals(s.name)&amp;&amp;this.age == s.age) &#123; return true; &#125; &#125; return false; &#125; 存放到hashset,并输出到控制台 1234567891011121314public static void main(String[] args) &#123; Student s1 = new Student(&quot;zs&quot;, 18); Student s2 = new Student(&quot;zs&quot;, 18); HashSet&lt;Student&gt; set = new HashSet&lt;&gt;(); set.add(s1); set.add(s2); Iterator&lt;Student&gt; iterator = set.iterator(); while(iterator.hasNext()) &#123; Student next = iterator.next(); System.out.println(next); &#125;&#125; 输出结果]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[clone() 浅拷贝与深拷贝]]></title>
    <url>%2F2019%2F06%2F29%2Fclone-%E6%B5%85%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B7%B1%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[clone的用法用来复制一个对象的副本，产生一个新的对象，新对象的属性与原对象的属性一致，而且原对象的改变不影响新对象 克隆有浅拷贝与深拷贝，用的时候一定要注意 使用对象的clone()方法时，需要实现Cloneable接口,这是一个标志接口，不提供任何抽象方法 12public interface Cloneable &#123;&#125; 浅拷贝先来看一段代码： 123456789101112131415161718192021222324public class Persion implements Cloneable&#123; private int age; private String name; public Persion(int age, String name) &#123; this.age = age; this.name = name; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; return super.clone(); &#125; public static void main(String[] args) throws CloneNotSupportedException &#123; Persion p = new Persion(18,&quot;zｈang&quot;); Persion p2 = (Persion) p.clone(); System.out.println(p.name == p2.name); &#125;&#125; 它的打印结果是什么呢？ true 没看错，就是这个结果，有没有困惑的地方？这就是由于浅拷贝导致的。看张图： 由于name是String类型，拷贝的只是它的引用,所以他们的name的地址值是相等的 那怎样将它改为深拷贝呢？ 如果是引用类型，我们需要重新在原对象的基础上重新创建出一个对象。 例如，将上面改为深拷贝，只需要修改一下clone方法 12345678@Override protected Object clone() throws CloneNotSupportedException &#123; String newName = new String(this.name);//拷贝原对象的name值 Persion p = (Persion)super.clone(); p.name = newName; return p; &#125; 深拷贝再来看一个深拷贝的例子： A要实现深拷贝，必须要求其属性中含有的引用类型也必须进行深拷贝，也就是要求B要进行深拷贝 123456789101112131415161718192021222324252627282930313233343536373839class A implements Cloneable&#123; int id; B b; public A(int id, B b) &#123; this.id = id; this.b = b; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; //深拷贝 B nb = (B) b.clone(); A na = (A) super.clone(); na.b = nb; return na; &#125;&#125;class B implements Cloneable &#123; String name; B(String name) &#123; this.name = name; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; B b = (B) super.clone(); String sname = new String(name); b.name = sname; return b; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flume]]></title>
    <url>%2F2019%2F06%2F27%2FFlume%2F</url>
    <content type="text"><![CDATA[一.Flume是什么Flume是一个分布式的，可靠的，高可用的海量日志采集系统，以Event为单位进行传输 二.Flume的三大组件source数据输入端的常见类型 spooling directory :文件中的数据 exec :执行linux的命令，监控文件数据 syslog:文件日志 avro:序列化框架 netcat:监听端口 channel缓冲区 位于Source和Sink,flume自带两种缓冲区Memory Channel,File Channel Memoey Channel :基于内存缓存，在不关心数据丢失的情况下使用 File Channel : 持久化Channel,不易丢失数据 sink数据输出端常见的目的地包括Hdfs,Kafka,logger,avro,file 三.Flume的操作1.监控指定端口，并采集数据，输出到控制台agent: source使用netcat,监控指定的端口 先检测要监听的端口是否已经被占用 1sudo netstat -tunlp | grep 44444 123sudo netstat -tunlp | grep 44444参数说明： -t:tcp -u:udp -n:网络连接 -l:listener p:进程 channel使用memory sink使用logger 创一个配置文件:vi netcat.flm 12345678910111213141516171819# Name the components on this agenta1.sources = r1 // a1:agent的名称 r1:source的名称a1.sinks = k1 // k1:sink的名称a1.channels = c1 //c1:channel的名称# Describe/configure the sourcea1.sources.r1.type = netcat a1.sources.r1.bind = localhost //绑定本机a1.sources.r1.port = 44444 //监听对应端口# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memory# Bind the source and sink to the channela1.sources.r1.channels = c1 // source输出到指定channela1.sinks.k1.channel = c1 //channel到指定的sink 执行命令，启动agent： 123flume-ng agent -c flume/conf/ -f netcat.flm -n a1 -Dflume.root.logger=INFO,console//-c:指定flume的配置文件 －f:指定要执行的配置文件 -n:agent的名字，要与配置中一致 使用telnet测试 1telnet local host message 2.实时采集数据并输出到控制台agent: ​ source使用exec ​ 配置文件 vi exec.flm 12345678910111213141516171819202122# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1# Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /home/briup/log/test.log //要监控的文件# 命令从-c后的字符串读取a1.sources.r1.shell = /bin/sh -c# Describe the sinka1.sinks.k1.type = logger# Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 运行agent 1flume-ng agent -c flume/conf/ -f exec.flm -n a1 -Dflume.root.logger=INFO,console 3.Spool监测配置的目录下新增的文件agent: spooldir 1mkdir spool //先创建一个目录，指定这个目录为要监听的目录 配置文件：vi spool.flm 1234567891011121314a1.sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = spooldira1.sources.r1.spoolDir =/home/master/spoola1.sources.r1.fileHeader = truea1.sinks.k1.type = loggera1.channels.c1.type = memorya1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动agent 1flume-ng agent -c flume/conf/ -f spool.flm -n a1 -Dflume.root.logger=INFO,console 4.Syslogtcp监听TCP的端口做为数据源agent:syslogtcp vi syslogtcp.flm //配置文件 12345678910111213141516sources = r1a1.sinks = k1a1.channels = c1a1.sources.r1.type = syslogtcpa1.sources.r1.port = 5140a1.sources.r1.host = localhosta1.sinks.k1.type = loggera1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动agent 1flume-ng agent -c flume/conf/ -f syslogtcp.flm -n a1 -Dflume.root.logger=INFO,console 测试 1echo &quot;hello world&quot; | nc localhost 5140 三.agent执行原理 source接受事件 channel处理器处理事件后，被拦截器拦截 拦截器处理后，进入channel选择器，根据channel选择器的选择结果，将事件写入对应的channel Sink选择器选择其中一个Sink去获取Channel数据，并将数据写入到下一个阶段 channel选择器：1. Replicating Channel Selector:将source发过来的events发往所有channel 2. Multiplexing channel Selector:可以将事件配置发往哪些Channel五.flume的高可用1.故障转移利用备份agent，当原来的agent挂掉后，切换到备份agent,需要使用到一个前置agent，或者是一个java程序来读取原来的数据，通过sink组(processor)输出到不同的agent. 配置实现 首先配置前置的agent，vi pre_flm 123456789101112131415161718192021222324252627282930313233343536373839a1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2#这个是配置failover的关键，需要有一个sink groupa1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2#处理的类型是failovera1.sinkgroups.g1.processor.type = failover#优先级，数字越大优先级越高，每个sink的优先级必须不相同a1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 10#设置为10秒，当然可以根据你的实际状况更改成更快或者很慢a1.sinkgroups.g1.processor.maxpenalty = 10000 # Describe/configure the sourcea1.sources.r1.type = syslogtcpa1.sources.r1.port = 5140a1.sources.r1.channels = c1 c2a1.sources.r1.selector.type = replicating# Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.channel = c1a1.sinks.k1.hostname = localhosta1.sinks.k1.port = 5555 a1.sinks.k2.type = avroa1.sinks.k2.channel = c2a1.sinks.k2.hostname = localhosta1.sinks.k2.port = 6666 # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 a1.channels.c2.type = memorya1.channels.c2.capacity = 1000a1.channels.c2.transactionCapacity = 100 第二配置agent与back_agent vi agent.flm vi back_agent.flm 12345678910111213141516171819202122232425262728293031323334353637#agent.flma1.sources = r1a1.sinks = k1a1.channels = c1Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 6666 Describe the sinka1.sinks.k1.type = loggerUse a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1#back_agent.flma1.sources = r1a1.sinks = k1a1.channels = c1Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 5555Describe the sinka1.sinks.k1.type = loggerUse a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 然后就可以使用命令启动这些配置了，先启动agent和back_agent，再启动pre_flm 然后向pre_flm发送数据，发现agent会接受到数据，使用crtl c关闭agent，会发现back_agent可以接受到数据。 12345flume-ng agent -c flume/conf/ -f pre.flm -n a1 -Dflume.root.logger=INFO,consoleflume-ng agent -c flume/conf/ -f agent.flm -n a1 -Dflume.root.logger=INFO,consoleflume-ng agent -c flume/conf/ –f agent_back.flm -n a1 -Dflume.root.logger=INFO,console 2.负载均衡利用前置的agent，通过processor向多个agent轮询的发送数据 首先配置前置的agent，vi pre_flm 12345678910111213141516171819202122232425262728293031323334353637a1.sources = r1a1.sinks = k1 k2a1.channels = c1 c2 #这个是配置Load balancing的关键，需要有一个sink groupa1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = load_balance#是否是开启退避功能a1.sinkgroups.g1.processor.backoff = true＃轮询a1.sinkgroups.g1.processor.selector = round_robin # Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 5140a1.sources.r1.channels = c1 # Describe the sinka1.sinks.k1.type = avroa1.sinks.k1.channel = c1a1.sinks.k1.hostname = localhosta1.sinks.k1.port = 5555 a1.sinks.k2.type = avroa1.sinks.k2.channel = c2a1.sinks.k2.hostname = localhosta1.sinks.k2.port = 6666 # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100a1.channels.c2.type = memory 第二配置agent a与agnet b vi a.flm vi b.flm 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#a.flma1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 5555 # Describe the sinka1.sinks.k1.type = logger # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1#b.flma1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the sourcea1.sources.r1.type = avroa1.sources.r1.channels = c1a1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 5555 # Describe the sinka1.sinks.k1.type = logger # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1]]></content>
      <categories>
        <category>flume</category>
      </categories>
      <tags>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java设计模式(单例模式)]]></title>
    <url>%2F2019%2F06%2F26%2Fjava%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[设计模式之单例模式一.什么是单例模式​ 确保类只有一个实例，不会出现多个 二.单例模式解决了什么问题​ 保证类在内存的对象唯一 三.单例模式的两种形式饿汉式public class Single{ private static Single instance = new Single(); private Single(){ } public static Single getInstance(){ return instance; } }１．为什么叫这种模式为饿汉式？ ​ 类在加载的时候就创建好了这个对象 ２．为什么构造方法是私有的，方法是静态的，属性也是静态的？ ​ 首先单例模式只创建一个实例，是不能通过new去创建实例的，所以构造方法是私有的 ​ 第二不能通过实例去调用方法，只能通过类名去访问，所以方法是静态的 由于静态方法只能访 问静态的属性所以属性也是静态的。 恶汉式线程安全的，由于类加载是按需加载，且只加载一次，上述单利模式中，在类加载的时候就已经创建好了对象，并且在在整个声明周期中只创建一次 懒汉式线程不安全1234567891011121314151617public class Single｛ private static Single single = null; public Single&#123; &#125; public static Single getInstance()&#123; if(single == null)&#123; single = new Single(); &#125; return single; &#125; &#125; 线程安全1234567891011121314151617public class Single&#123; private volatile static Single single = null; public static Single getInstance()&#123; if(single == null)&#123; //如果single已经实例化，则不在去获取锁，提高效率 synchronized(Single.class)&#123; if(single == null)&#123; single = new Single(); &#125; &#125; &#125; return single; &#125;&#125; 注意： volatile 关键是一定要加的 原因：single = new Single(); 操作不是原子性的，它分为三步 1. 分配内存空间 2. 初始化对象 3. 将初始化化的对象赋值给single如果不加volatile，它可能会进行指令重排，执行步骤可能变为： 分配内存空间 将初始化的对象赋值给single 初始化对象 假设两个线程，线程一和线程二都执行到synchronized代码块那一行，这时候线程一先抢到锁，进入同步代码块中，执行 single = new Single()，在single = new Single()还未执行完成时，也就是执行了指令重排后的第二步，这时候single不为空了，线程二开始执行，返回single,这个single是不完整的 ​]]></content>
      <categories>
        <category>java设计模式　java</category>
      </categories>
      <tags>
        <tag>java设计模式</tag>
      </tags>
  </entry>
</search>
